{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8222fd-8e11-49eb-8772-859ae12314bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamil/Desktop/git/meteoapp-data/venv/lib/python3.11/site-packages/gribapi/__init__.py:23: UserWarning: ecCodes 2.31.0 or higher is recommended. You are running version 2.30.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from baselines.gnn.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e96222-4498-45e5-ab0b-dab20c2fcf2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(architecture='cgcn', subset=1, hidden_dim=1024)\n",
    "trainer.train(num_epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f238ab1f-9b44-441a-98c9-bc680cc9f0af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.evaluate(\"train\")\n",
    "# trainer.evaluate(\"test\")\n",
    "trainer.plot_predictions(data_type='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a7d96-8bc8-4e01-8c9a-c4cc407b23f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [A3TGCN]\n",
    "\n",
    "## Best overfit:\n",
    " \n",
    "---------\n",
    "Epochs = 200; hidden = 1024\n",
    "\n",
    "RMSE for f0: 1.1007113849840253\n",
    "\n",
    "RMSE for f1: 6.182032024036881\n",
    "\n",
    "RMSE for f2: 0.11709922635573465\n",
    "\n",
    "RMSE for f3: 1.0938558029230945\n",
    "\n",
    "RMSE for f4: 1.135363085849578\n",
    "\n",
    "RMSE for f5: 5.304731379016946e-05\n",
    "\n",
    "* predicitons of mini-batches looks similar in terms of accurayc but it doesn't look like it is possible to overfit for batch_size > 1\n",
    "* [with only one MLP] after SE $\\approx$ 21; loss decreases very slowly\n",
    "* [with more MLP] it looks a little better after SE $\\approx$ 17 loss decreases slowly; after 1k epochs: SE-11; hidden_dim / 2 in each linear layer; for linear layers with same size similar resutls\n",
    "\n",
    "---------\n",
    "\n",
    "# [CGCN]\n",
    "\n",
    "## Best overfit - only first mini batch is nice:\n",
    "\n",
    "---------\n",
    "\n",
    "### for batch_size = 1:\n",
    "\n",
    "N = 5; Epochs = 3000; hidden = 1024\n",
    "\n",
    "RMSE for f0: 0.46053606271743774; MAE for f0: 0.34814876317977905;\n",
    "\n",
    "RMSE for f1: 0.5324509739875793; MAE for f1: 0.3963014781475067;\n",
    "\n",
    "RMSE for f2: 0.06111479550600052; MAE for f2: 0.04281201213598251;\n",
    "\n",
    "RMSE for f3: 0.5062028765678406; MAE for f3: 0.3906308114528656;\n",
    "\n",
    "RMSE for f4: 0.511387050151825; MAE for f4: 0.4053451120853424;\n",
    "\n",
    "RMSE for f5: 4.359076774562709e-05; MAE for f5: 2.7624157155514695e-05;\n",
    "\n",
    "\n",
    "\n",
    "* loss convergence to ~0 is much slower than in GCN; model struggles to achieve SE less than **10** - loss decrease is extremely little each epoch after that; after 3k epochs it is SE $\\approx$ 4.9\n",
    "\n",
    "* overfit process looks kinda worse than in GCN I guess, the val_loss is lower though\n",
    "\n",
    "* prediction plots looks smoother than those from GCN\n",
    "<br>\n",
    "\n",
    "After using basic edge_attr:\n",
    "\n",
    "* convergence to ~0 is a little faster I guess; after 2k epoch SE $\\approx$ 5.1\n",
    "\n",
    "RMSE for f0: 0.5378605969502436; MAE for f0: 0.4116863712567267;\n",
    "\n",
    "RMSE for f1: 0.5801239272308413; MAE for f1: 0.43263243272569446;\n",
    "\n",
    "RMSE for f2: 0.06308332136000509; MAE for f2: 0.04308025844891866;\n",
    "\n",
    "RMSE for f3: 0.5010276104107196; MAE for f3: 0.3898159819923393;\n",
    "\n",
    "RMSE for f4: 0.5142384579445146; MAE for f4: 0.40151560864514774;\n",
    "\n",
    "RMSE for f5: 5.695505041428303e-05; MAE for f5: 3.787352560382513e-05;\n",
    "\n",
    "<br>\n",
    "After using no edge_attr nor edge_weights - same results :((( \n",
    "\n",
    "* after 2k epochs SE $\\approx$ 4.9\n",
    "\n",
    "RMSE for f0: 0.5154228210449219; MAE for f0: 0.3955957591533661;\n",
    "\n",
    "RMSE for f1: 0.616793692111969; MAE for f1: 0.4610046148300171;\n",
    "\n",
    "RMSE for f2: 0.06191185861825943; MAE for f2: 0.042094070464372635;\n",
    "\n",
    "RMSE for f3: 0.5192311406135559; MAE for f3: 0.40097156167030334;\n",
    "\n",
    "RMSE for f4: 0.47820839285850525; MAE for f4: 0.37651485204696655;\n",
    "\n",
    "RMSE for f5: 4.282597728888504e-05; MAE for f5: 2.9758122764178552e-05;\n",
    "\n",
    "<br>\n",
    "\n",
    "With updated edge_attr maaaaaybe slightly better idk?\n",
    "\n",
    "* after 2k epochs SE $\\approx$ 4.1\n",
    "\n",
    "RMSE for f0: 0.487577348947525; MAE for f0: 0.3760671019554138;\n",
    "\n",
    "RMSE for f1: 0.6514126658439636; MAE for f1: 0.4907068610191345;\n",
    "\n",
    "RMSE for f2: 0.05654036998748779; MAE for f2: 0.03860820084810257;\n",
    "\n",
    "RMSE for f3: 0.4825141429901123; MAE for f3: 0.3743925094604492;\n",
    "\n",
    "RMSE for f4: 0.43773719668388367; MAE for f4: 0.3455958366394043;\n",
    "\n",
    "RMSE for f5: 4.442723002284765e-05; MAE for f5: 2.835924351529684e-05;\n",
    "\n",
    "---------\n",
    "\n",
    "# [GCN]\n",
    "\n",
    "## Best overfit: \n",
    "\n",
    "1st mini-batch looks noisy on plots, \n",
    "\n",
    "Not sure if inverse transform is fine\n",
    "\n",
    "t2m in mini-batches > 1 are in weird scale - with 0 mean? std ~ 1\n",
    "\n",
    "---------\n",
    "\n",
    "### for batch_size = 1:\n",
    "\n",
    "N = 5; Epochs = 1794; hidden = 1024\n",
    "\n",
    "RMSE for f0: 0.2709898352622986; MAE for f0: 0.17411679029464722;\n",
    "\n",
    "RMSE for f1: 0.9162903428077698; MAE for f1: 0.5823767185211182;\n",
    "\n",
    "RMSE for f2: 0.007963583804666996; MAE for f2: 0.004284282214939594;\n",
    "\n",
    "RMSE for f3: 0.19165192544460297; MAE for f3: 0.12410622835159302;\n",
    "\n",
    "RMSE for f4: 0.21218980848789215; MAE for f4: 0.12774522602558136;\n",
    "\n",
    "RMSE for f5: 2.9874003303120844e-05; MAE for f5: 1.6733891243347898e-05;\n",
    "\n",
    "\n",
    "* it is definetaly the fastest training process; convergence looks good, after 1.7k epoch SE $\\approx$ 0.2; other experiment 3k epochs SE $\\approx$ 0.6\n",
    "* decrasing hidden_dim to 256 -> epoch 4k. SE $\\approx$ 1.1; RMSE f0 - 0.55, RMSE f1 -> 2.\n",
    "\n",
    "### for batch_size = 4:\n",
    "\n",
    "RMSE for f0: 0.5673630833625793; MAE for f0: 0.3919491171836853;\n",
    "\n",
    "RMSE for f1: 1.640638828277588; MAE for f1: 1.1147913932800293;\n",
    "\n",
    "RMSE for f2: 0.014014026150107384; MAE for f2: 0.008001776412129402;\n",
    "\n",
    "RMSE for f3: 0.4537373185157776; MAE for f3: 0.301805704832077;\n",
    "\n",
    "RMSE for f4: 0.41247645020484924; MAE for f4: 0.2598435878753662;\n",
    "\n",
    "RMSE for f5: 6.930914969416335e-05; MAE for f5: 3.580433622119017e-05;\n",
    "\n",
    "After 3k epoch SE $\\approx$ 0.8; hidden=1024\n",
    "\n",
    "### for batch_size = 32 (plots are kinda noisy but it still overfit pretty ok?) :\n",
    "\n",
    "RMSE for f0: 1.00359737230392; MAE for f0: 0.7348482976609693;\n",
    "\n",
    "RMSE for f1: 3.519246861894878; MAE for f1: 2.5813824818929034;\n",
    "\n",
    "RMSE for f2: 0.03600320123506689; MAE for f2: 0.01963227628564669;\n",
    "\n",
    "RMSE for f3: 0.9141614341014068; MAE for f3: 0.6554838256781391;\n",
    "\n",
    "RMSE for f4: 0.8640779164793619; MAE for f4: 0.6287803415469534;\n",
    "\n",
    "RMSE for f5: 0.00015852658515357014; MAE for f5: 9.89001418981821e-05;\n",
    "\n",
    "* After 2k epochs SE $\\approx$ 4; hidden=1024\n",
    "\n",
    "\n",
    "### Full training:\n",
    "\n",
    "N = 5; hidden = 256\n",
    "\n",
    "RMSE for f0: 6.057778648517553; MAE for f0: 4.787944748312894;\n",
    "\n",
    "RMSE for f1: 8.216086426384388; MAE for f1: 6.418473183528259;\n",
    "\n",
    "RMSE for f2: 0.26353396146013697; MAE for f2: 0.19950958294953372;\n",
    "\n",
    "RMSE for f3: 2.0419449013364606; MAE for f3: 1.5629899934557656;\n",
    "\n",
    "RMSE for f4: 2.105134228203406; MAE for f4: 1.6288272409697975;\n",
    "\n",
    "RMSE for f5: 0.0003035589159654516; MAE for f5: 0.00011708228228673238;\n",
    "\n",
    "* it is very bad and noisy\n",
    "\n",
    "Epoch 161/1000, Train Loss: 100.1107, lr: 3.125e-05\n",
    "Val Loss: 131.3151\n",
    "\n",
    "Early stopping ....\n",
    "1821.1647243499756 [s]\n",
    "\n",
    "---------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
