{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8222fd-8e11-49eb-8772-859ae12314bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kamil/Desktop/git/meteoapp-data/venv/lib/python3.11/site-packages/gribapi/__init__.py:23: UserWarning: ecCodes 2.31.0 or higher is recommended. You are running version 2.30.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from baselines.gnn.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0232d943-35e6-4eeb-bc32-74ae22cf50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size = 4 worse val_loss better rmse and other metrics ?\n",
    "# batch_size = 32 better val_loss worse rmse and other metrics ?\n",
    "# why ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e96222-4498-45e5-ab0b-dab20c2fcf2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(architecture='cgcn', hidden_dim=32, lr=1e-3)\n",
    "# trainer.load_model('cgcn_aw_h32_b32.pt')\n",
    "trainer.train(num_epochs=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7ab0e2f-6950-4ea7-aae5-3ac46758a123",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for t2m: 2.3938751269677825; MAE for t2m: 1.822873919890962;\n",
      "RMSE for tcc: 1.7928591901672486; MAE for tcc: 1.323529751108289;\n",
      "RMSE for u10: 0.2732322504230127; MAE for u10: 0.21589336265193956;\n",
      "RMSE for v10: 1.35257825360288; MAE for v10: 1.009126482300822;\n",
      "RMSE for tp: 1.3507401889076487; MAE for tp: 0.9962495362800119;\n",
      "RMSE for sp: 0.0002785220389248491; MAE for sp: 0.00010348816286302983;\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07b0f57d-cef3-44e0-b44c-39c5e124e99b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for t2m: 2.478773593165839; MAE for t2m: 1.9161613328296017;\n",
      "RMSE for tcc: 1.9557973406183256; MAE for tcc: 1.435367676933997;\n",
      "RMSE for u10: 0.27970377140309594; MAE for u10: 0.222125477575149;\n",
      "RMSE for v10: 1.4170224306825128; MAE for v10: 1.056216633587702;\n",
      "RMSE for tp: 1.4330866463290028; MAE for tp: 1.0559691220468526;\n",
      "RMSE for sp: 0.0002639260373372121; MAE for sp: 0.00010320724673989641;\n"
     ]
    }
   ],
   "source": [
    "trainer.evaluate(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a7d96-8bc8-4e01-8c9a-c4cc407b23f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [CGCN]\n",
    "\n",
    "### Full training:\n",
    "\n",
    "N = 5; hidden = 32; batch=4 TEST SET:\n",
    " \n",
    "RMSE for f0: 2.2464043976548047; MAE for f0: 1.6999678436793948;\n",
    "\n",
    "RMSE for f1: 1.9364765893388307; MAE for f1: 1.408220916940946;\n",
    "\n",
    "RMSE for f2: 0.28386787466464225; MAE for f2: 0.21952327282997064;\n",
    "\n",
    "RMSE for f3: 1.411270602820433; MAE for f3: 1.0502013356032074;\n",
    "\n",
    "RMSE for f4: 1.412410520285922; MAE for f4: 1.0404767270553594;\n",
    "\n",
    "RMSE for f5: 0.0002722878701922671; MAE for f5: 0.0001054861260521275;\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "N = 5; hidden = 64; batch=4; no significant difference TEST SET:\n",
    "\n",
    "RMSE for f0: 2.205407197194004; MAE for f0: 1.6519353043910658;\n",
    "\n",
    "RMSE for f1: 2.029980887836009; MAE for f1: 1.4714761668475365;\n",
    "\n",
    "RMSE for f2: 0.28731255755261337; MAE for f2: 0.22192368753909236;\n",
    "\n",
    "RMSE for f3: 1.4478491874482888; MAE for f3: 1.0733782789693203;\n",
    "\n",
    "RMSE for f4: 1.4493260989133352; MAE for f4: 1.058615083231145;\n",
    "\n",
    "RMSE for f5: 0.0002749573712955729; MAE for f5: 0.00010411299434883103;\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "With bigger batch_size training is more stable (almost no overfitting) but it does not improve performance\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "N = 10; hidden = 32; batch=32; no significant difference, much longer training; even worse performance\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "N = 10; hidden = 32; batch=32; mlp - 3 layers for encoder and decoder each - totally failed to learn sufficient representation; loss after 100 epochs is 2x bigger\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "With gradient clip=100; h=32; b=32; n=5 -> loss plot is more smooth\n",
    "\n",
    "RMSE for t2m: 2.3807000767333983; MAE for t2m: 1.8284217557244822;\n",
    "\n",
    "RMSE for tcc: 2.2638646906319004; MAE for tcc: 1.6580551376662336;\n",
    "\n",
    "RMSE for u10: 0.28466073801596425; MAE for u10: 0.22292598680924447;\n",
    "\n",
    "RMSE for v10: 1.4152165344262562; MAE for v10: 1.056918092717271;\n",
    "\n",
    "RMSE for tp: 1.4210972798532857; MAE for tp: 1.0426973776264234;\n",
    "\n",
    "RMSE for sp: 0.0002774151951318293; MAE for sp: 0.00010699005800520397;\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "Using learning constants advised by authors of graphcast does not improve performance (it is actually worse) but training is super stable - no overfittin at all:\n",
    "clip=32; self.optimizer = torch.optim.AdamW(self.model.parameters(), betas=(0.9, 0.95), weight_decay=0.1); h=32; b=32\n",
    "\n",
    "RMSE for t2m: 2.6790109251385346; MAE for t2m: 2.064865770450839;\n",
    "\n",
    "RMSE for tcc: 2.2133519471112115; MAE for tcc: 1.6241865286434534;\n",
    "\n",
    "RMSE for u10: 0.2823515758338797; MAE for u10: 0.22227485714086204;\n",
    "\n",
    "RMSE for v10: 1.4398015960157504; MAE for v10: 1.0776576370734994;\n",
    "\n",
    "RMSE for tp: 1.459266796233724; MAE for tp: 1.0795614604177897;\n",
    "\n",
    "RMSE for sp: 0.00026684837551226877; MAE for sp: 0.00011326350266132657;\n",
    "\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "Test AdamW for subset 30:\n",
    "\n",
    "\n",
    "\n",
    "Epoch 300/300, Train Loss: 2126.0813, lr: 0.0005\n",
    "Val Loss: 2170.4064\n",
    "\n",
    "RMSE for t2m: 2.542392155132118; MAE for t2m: 1.9760580701717938;\n",
    "\n",
    "RMSE for tcc: 2.2345008124975436; MAE for tcc: 1.641610119153863;\n",
    "\n",
    "RMSE for u10: 0.2798438730903883; MAE for u10: 0.22311758102642487;\n",
    "\n",
    "RMSE for v10: 1.4142314190690894; MAE for v10: 1.0572955320246566;\n",
    "\n",
    "RMSE for tp: 1.4419994581694888; MAE for tp: 1.0626909864744751;\n",
    "\n",
    "RMSE for sp: 0.00026517082409864415; MAE for sp: 0.00011171030055055118;\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Epoch 111/300, Train Loss: 1892.6492, lr: 1.953125e-05\n",
    "Val Loss: 2278.2073\n",
    "\n",
    "Regular Adam overfits faster!\n",
    "\n",
    "RMSE for t2m: 2.462725161219301; MAE for t2m: 1.8802260621920581;\n",
    "\n",
    "RMSE for tcc: 2.050671564700832; MAE for tcc: 1.496878075945819;\n",
    "\n",
    "RMSE for u10: 0.29108879830409606; MAE for u10: 0.22735035309318213;\n",
    "\n",
    "RMSE for v10: 1.45013345457189; MAE for v10: 1.0824693106168237;\n",
    "\n",
    "RMSE for tp: 1.4534132896562777; MAE for tp: 1.0730399367480314;\n",
    "\n",
    "RMSE for sp: 0.00026861947818754685; MAE for sp: 0.00010743896842065517;\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "32 batch_size\n",
    "\n",
    "Test AdamW for 650 epochs\n",
    "\n",
    "epoch 450 -> 630: val_loss 2156 -> 2154.35 ... veeery slow\n",
    "\n",
    "Epoch 635/3000, Train Loss: 2007.6319, lr: 6.25e-05\n",
    "Val Loss: 2154.3627\n",
    "\n",
    "RMSE for t2m: 2.374920093281214; MAE for t2m: 1.825446942889366;\n",
    "\n",
    "RMSE for tcc: 1.934781080630695; MAE for tcc: 1.440275953202643;\n",
    "\n",
    "RMSE for u10: 0.27891942856927693; MAE for u10: 0.21959464355241812;\n",
    "\n",
    "RMSE for v10: 1.4032331119908186; MAE for v10: 1.0472651794472845;\n",
    "\n",
    "RMSE for tp: 1.4270852073400706; MAE for tp: 1.0514319534209646;\n",
    "\n",
    "RMSE for sp: 0.00026807549549976584; MAE for sp: 0.00010374101359034243;\n",
    "\n",
    "And retrain for another ~100 epochs starting with lr=0.001\n",
    "\n",
    "Epoch 117/3000, Train Loss: 1996.2265, lr: 4.8828125e-07\n",
    "Val Loss: 2147.5976\n",
    "\n",
    "RMSE for t2m: 2.362443101215429; MAE for t2m: 1.8129088702135565;\n",
    "\n",
    "RMSE for tcc: 1.9058418101131351; MAE for tcc: 1.4109735495544298;\n",
    "\n",
    "RMSE for u10: 0.278758591001822; MAE for u10: 0.21784827063459586;\n",
    "\n",
    "RMSE for v10: 1.4033993603293886; MAE for v10: 1.0458335290530631;\n",
    "\n",
    "RMSE for tp: 1.4286469497076204; MAE for tp: 1.0510217919543199;\n",
    "\n",
    "RMSE for sp: 0.00026647597382923086; MAE for sp: 0.00010325525085403092;\n",
    "\n",
    "\n",
    "Retrain for another ~200epoch with batch_size=4; start with lr=0.01\n",
    "Epoch 217/3000, Train Loss: 2147.8535, lr: 7.8125e-05\n",
    "Val Loss: 2165.5707\n",
    "\n",
    "RMSE for t2m: 2.2909087951225993; MAE for t2m: 1.7465725081254844;\n",
    "\n",
    "RMSE for tcc: 1.9833718048738613; MAE for tcc: 1.4148737335402626;\n",
    "\n",
    "RMSE for u10: 0.2820898910703319; MAE for u10: 0.21638233899559758;\n",
    "\n",
    "RMSE for v10: 1.4179210015176462; MAE for v10: 1.0595987630294779;\n",
    "\n",
    "RMSE for tp: 1.4344424007598335; MAE for tp: 1.0635660365221824;\n",
    "\n",
    "RMSE for sp: 0.00026667937622278033; MAE for sp: 0.00011324805656348794;\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "AdamW; batch_size=4\n",
    "\n",
    "Loss still goes down but veeery slowly, super stable though, no overfitting at all\n",
    "Epoch 418/3000, Train Loss: 2167.8868, lr: 3.125e-05\n",
    "Val Loss: 2169.5276\n",
    "\n",
    "RMSE for t2m: 2.478773593165839; MAE for t2m: 1.9161613328296017;\n",
    "\n",
    "RMSE for tcc: 1.9557973406183256; MAE for tcc: 1.435367676933997;\n",
    "\n",
    "RMSE for u10: 0.27970377140309594; MAE for u10: 0.222125477575149;\n",
    "\n",
    "RMSE for v10: 1.4170224306825128; MAE for v10: 1.056216633587702;\n",
    "\n",
    "RMSE for tp: 1.4330866463290028; MAE for tp: 1.0559691220468526;\n",
    "\n",
    "RMSE for sp: 0.0002639260373372121; MAE for sp: 0.00010320724673989641;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
