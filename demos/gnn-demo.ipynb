{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a8222fd-8e11-49eb-8772-859ae12314bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "sys.path.append('..')\n",
    "\n",
    "from models.gnn.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3e96222-4498-45e5-ab0b-dab20c2fcf2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 51.63570713996887 [s]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "trainer = Trainer(architecture='trans', hidden_dim=32, lr=1e-3)\n",
    "stop = time.time()\n",
    "print(f'Elapsed time: {stop-start} [s]') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "205432ff-1117-4872-a8ad-103329bdcd18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataBatch(x=[1125, 6, 5], edge_index=[2, 12804], edge_attr=[12804, 3], y=[1125, 6, 1], pos=[1, 1125, 6], time=[4], batch=[1125], ptr=[2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(trainer.train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e84b7463-a62a-43d8-95e3-5e70415dad96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from models.config import config as cfg\n",
    "# cfg.FH = 2\n",
    "# cfg.INPUT_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7b13c46-7859-473d-abda-8f7b66d51aa5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.update_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97b274f7-e172-4e9c-b1f9-283a23a85dea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# next(iter(trainer.train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94606484-d1cd-4955-a8ec-6868d3c8b461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# s = trainer.nn_proc.data_proc.spatial_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a82f6d29-9b58-4831-9694-b6518934a68b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# next(iter(trainer.train_loader)).x.size(0) / 4 / 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcfb8110-87d3-49b3-9042-260db191ee24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 1536, 32]' is invalid for input of size 36000",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/january/meteoapp-data/demos/../models/gnn/trainer.py:155\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m    151\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# batch = batch.to(self.cfg.DEVICE)\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;66;03m# batch = batch.to(torch.device(\"cuda\"))\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43medge_attr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m     batch_y \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39my\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_mapping:\n",
      "File \u001b[0;32m~/anaconda3/envs/engeneeringEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/january/meteoapp-data/demos/../models/gnn/transformer_conv.py:42\u001b[0m, in \u001b[0;36mTransformerGNN.forward\u001b[0;34m(self, x, edge_index, edge_attr, t, s)\u001b[0m\n\u001b[1;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_embedder(x)\n\u001b[0;32m---> 42\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mst_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm_embed(x)\u001b[38;5;241m.\u001b[39mrelu()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transgnn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransgnns:\n",
      "File \u001b[0;32m~/anaconda3/envs/engeneeringEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/january/meteoapp-data/demos/../models/gnn/st_encoder_module.py:17\u001b[0m, in \u001b[0;36mSpatioTemporalEncoder.forward\u001b[0;34m(self, X, t, s)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, t, s):\n\u001b[1;32m     16\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_X_dim\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_embedder(X)\u001b[38;5;241m.\u001b[39mrelu()\n\u001b[1;32m     19\u001b[0m     t \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     20\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_embedder(t\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;241m.\u001b[39mrelu()\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     23\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 1536, 32]' is invalid for input of size 36000"
     ]
    }
   ],
   "source": [
    "trainer.train(num_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdf3215-d13d-4de5-8670-f6b5e61ad345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.plot_predictions(\"train\")\n",
    "# trainer.plot_predictions(\"val\")\n",
    "# trainer.plot_predictions(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab0e2f-6950-4ea7-aae5-3ac46758a123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rmse, mae = trainer.evaluate(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d171b-9058-4e1b-a72c-ebcd31ac9768",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0f57d-cef3-44e0-b44c-39c5e124e99b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482fda92-4602-43ce-b9fe-e3ee6ad5b613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2020-2022.grib\n",
    "\n",
    "# Transformer: Epoch 50/50, Train Loss: 1631.7028, lr: 0.000125 Val Loss: 1995.9098  *10 epoch < 2.1k                    627.07288813591 [s]; 577.6894102096558 [s] - no spatial mapping Val Loss: 2773.0315\n",
    "\n",
    "# GAT:         Epoch 50/50, Train Loss: 1912.3626, lr: 6.25e-05 Val Loss: 2068.4888  *rmse sp ~ 6                        710.3302898406982 [s]; \n",
    "\n",
    "# GEN:         Epoch 50/50, Train Loss: 1738.9288, lr: 0.000125 Val Loss: 2400.2730  *rmse sp ~ 6                        632.7618696689606 [s]\n",
    "#              Epoch 50/50, Train Loss: 1926.7893, lr: 3.125e-05Val Loss: 2358.8036                                      250.7063798904419 [s] with num_layers!!!\n",
    "\n",
    "# CGCN:        Epoch 50/50, Train Loss: 1973.3904, lr: 0.001    Val Loss: 2126.4853                                      580.7204239368439 [s]\n",
    "\n",
    "# PDN:         Epoch 50/50, Train Loss: 1991.0188, lr: 0.000125 Val Loss: 2150.5120 *rmse sp ~ 6                         674.661036491394 [s]\n",
    "# h_chnnls*4   Epoch 50/50, Train Loss: 1984.7307, lr: 0.00025  Val Loss: 2160.6990                                      660.2813837528229 [s]\n",
    "\n",
    "\n",
    "# BEST mapping:\n",
    "\n",
    "# RMSE for t2m: 1.9502057215900213; MAE for t2m: 1.460449989283434;\n",
    "# RMSE for sp: 3.399038812459811; MAE for sp: 2.5950842092613984;\n",
    "# RMSE for tcc: 0.27409164273468917; MAE for tcc: 0.21394984584361584;\n",
    "# RMSE for u10: 1.3402054937222394; MAE for u10: 1.003815006674253;\n",
    "# RMSE for v10: 1.3046558578388197; MAE for v10: 0.9701394707445763;\n",
    "# RMSE for tp: 0.00028378795573663634; MAE for tp: 9.744955463428279e-05;\n",
    "\n",
    "# BEST no mapping:\n",
    "\n",
    "# RMSE for t2m: 1.848757972304198; MAE for t2m: 1.395213300597085;\n",
    "# RMSE for sp: 3.449658892501596; MAE for sp: 2.667022191931498;\n",
    "# RMSE for tcc: 0.27683438137853805; MAE for tcc: 0.21672284132768843;\n",
    "# RMSE for u10: 1.410872957033123; MAE for u10: 1.0491217187220006;\n",
    "# RMSE for v10: 1.3660170921185273; MAE for v10: 1.0087554784574733;\n",
    "# RMSE for tp: 0.00027853536544616465; MAE for tp: 9.733342513856305e-05;\n",
    "\n",
    "# lowest we get with new features; mapping:\n",
    "# Epoch 36/50, Train Loss: 1880.5901, lr: 0.0005\n",
    "# Val Loss: 2332.4124\n",
    "# RMSE for t2m: 1.8650685428773215; MAE for t2m: 1.4327338052306162;\n",
    "# RMSE for sp: 3.125996870921546; MAE for sp: 2.394205469596666;\n",
    "# RMSE for tcc: 0.2769834191870229; MAE for tcc: 0.21654550355782148;\n",
    "# RMSE for u10: 1.3499078509381048; MAE for u10: 1.0095530567394215;\n",
    "# RMSE for v10: 1.3224928259398205; MAE for v10: 0.9903028884270412;\n",
    "# RMSE for tp: 0.00028549145677631216; MAE for tp: 0.0001046686964487334;\n",
    "\n",
    "# loqwest we get with new feature no mapping:\n",
    "# Epoch 36/50, Train Loss: 1950.8919, lr: 0.0005\n",
    "# Val Loss: 2511.0089\n",
    "# ---------\n",
    "# RMSE for t2m: 1.9533919521164234; MAE for t2m: 1.4978515430230384;\n",
    "# RMSE for sp: 2.8703930929801524; MAE for sp: 2.198967421439784;\n",
    "# RMSE for tcc: 0.27739108423280834; MAE for tcc: 0.21839308087625517;\n",
    "# RMSE for u10: 1.3687520654035368; MAE for u10: 1.0232698703248595;\n",
    "# RMSE for v10: 1.327701042680353; MAE for v10: 0.9899769595131284;\n",
    "# RMSE for tp: 0.0002961515820969641; MAE for tp: 0.00010478641882246229;\n",
    "\n",
    "# factor  0.732421875\n",
    "# Epoch 26/50, Train Loss: 2882.6302, lr: 0.001\n",
    "# Val Loss: 3317.5983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974c5753-4ba2-4407-8fd7-69655f18449d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# # [CGCN]\n",
    "\n",
    "# ### Full training:\n",
    "\n",
    "# N = 5; hidden = 32; batch=4; s = 3:\n",
    " \n",
    "# RMSE for f0: 2.2464043976548047; MAE for f0: 1.6999678436793948;\n",
    "\n",
    "# RMSE for f1: 1.9364765893388307; MAE for f1: 1.408220916940946;\n",
    "\n",
    "# RMSE for f2: 0.28386787466464225; MAE for f2: 0.21952327282997064;\n",
    "\n",
    "# RMSE for f3: 1.411270602820433; MAE for f3: 1.0502013356032074;\n",
    "\n",
    "# RMSE for f4: 1.412410520285922; MAE for f4: 1.0404767270553594;\n",
    "\n",
    "# RMSE for f5: 0.0002722878701922671; MAE for f5: 0.0001054861260521275;\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# N = 5; hidden = 64; batch=4; s = 3:\n",
    "\n",
    "# RMSE for f0: 2.205407197194004; MAE for f0: 1.6519353043910658;\n",
    "\n",
    "# RMSE for f1: 2.029980887836009; MAE for f1: 1.4714761668475365;\n",
    "\n",
    "# RMSE for f2: 0.28731255755261337; MAE for f2: 0.22192368753909236;\n",
    "\n",
    "# RMSE for f3: 1.4478491874482888; MAE for f3: 1.0733782789693203;\n",
    "\n",
    "# RMSE for f4: 1.4493260989133352; MAE for f4: 1.058615083231145;\n",
    "\n",
    "# RMSE for f5: 0.0002749573712955729; MAE for f5: 0.00010411299434883103;\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# With bigger batch_size training is more stable (almost no overfitting) but it does not improve performance\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# N = 10; hidden = 32; batch=32; no significant difference, much longer training; even worse performance\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# N = 10; hidden = 32; batch=32; mlp - 3 layers for encoder and decoder each - totally failed to learn sufficient representation; loss after 100 epochs is 2x bigger\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# With gradient clip=100; h=32; b=32; n=5 -> loss plot is more smooth; no performance improvement\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# Using learning constants advised by authors of graphcast does not improve performance (it is actually worse) but training is super stable - no overfittin at all:\n",
    "# clip=32; self.optimizer = torch.optim.AdamW(self.model.parameters(), betas=(0.9, 0.95), weight_decay=0.1); h=32; b=32\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# N = 5; hidden = 32; batch = 4; s = 7\n",
    "\n",
    "# Epoch 100/100, Train Loss: 1848.7699, lr: 3.125e-05 Val Loss: 2230.6389\n",
    "# 735.7001855373383 [s]22666046945865614\n",
    "\n",
    "# RMSE for t2m: 1.8891855143316563; MAE for t2m: 1.4600152796625336;\n",
    "\n",
    "# RMSE for sp: 2.0393454691406316; MAE for sp: 1.5120049358744065;\n",
    "\n",
    "# RMSE for tcc: 0.2860184447921189; MAE for tcc: 0.22666046945865614;\n",
    "\n",
    "# RMSE for u10: 1.4378177755816777; MAE for u10: 1.0736904505733116;\n",
    "\n",
    "# RMSE for v10: 1.4574477223923084; MAE for v10: 1.0663613129750227;\n",
    "\n",
    "# RMSE for tp: 0.0002711004547077864; MAE for tp: 0.00010284140930705948;\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# N = 5; hidden = 32; batch = 4; s = 3\n",
    "\n",
    "# Epoch 99/100, Train Loss: 1955.0575, lr: 0.0005 Val Loss: 2207.3025\n",
    "\n",
    "# 693.315672159195 [s]\n",
    "\n",
    "# RMSE for t2m: 2.3858469221825898; MAE for t2m: 1.8332994478389537;\n",
    "\n",
    "# RMSE for sp: 1.9815898823743083; MAE for sp: 1.4750331930477616;\n",
    "\n",
    "# RMSE for tcc: 0.28416700897460695; MAE for tcc: 0.23001444125440887;\n",
    "\n",
    "# RMSE for u10: 1.4444404739897925; MAE for u10: 1.0740849332227052;\n",
    "\n",
    "# RMSE for v10: 1.4384182183045433; MAE for v10: 1.05619047567965;\n",
    "\n",
    "# RMSE for tp: 0.0002669724249068656; MAE for tp: 0.00010341137409668945;\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# N = 5; hidden = 32; batch = 4; s = 5\n",
    "\n",
    "# Epoch 100/100, Train Loss: 1901.1544, lr: 6.25e-05 Val Loss: 2181.1193\n",
    "\n",
    "# 678.085875749588 [s]\n",
    "\n",
    "# RMSE for t2m: 1.8778959983044772; MAE for t2m: 1.4465936457254398;\n",
    "\n",
    "# RMSE for sp: 2.190103341129505; MAE for sp: 1.6404208756272267;\n",
    "\n",
    "# RMSE for tcc: 0.2848521568307657; MAE for tcc: 0.22361195825242483;\n",
    "\n",
    "# RMSE for u10: 1.437490006965024; MAE for u10: 1.0706755302203412;\n",
    "\n",
    "# RMSE for v10: 1.4364091328286357; MAE for v10: 1.0579831289563082;\n",
    "\n",
    "# RMSE for tp: 0.0002660477946055829; MAE for tp: 0.00010596483422627151;\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# N = 5; hidden = 32; batch = 4; s = 5; r=2\n",
    "\n",
    "# Epoch 100/100, Train Loss: 1882.7291, lr: 1.5625e-05 Val Loss: 2143.8953 (*at some point 2129)\n",
    "\n",
    "# 887.3842961788177 [s]\n",
    "\n",
    "# RMSE for t2m: 1.8323451416138599; MAE for t2m: 1.4136113810162858;\n",
    "\n",
    "# RMSE for sp: 2.0521229454022802; MAE for sp: 1.527130837420577;\n",
    "\n",
    "# RMSE for tcc: 0.2793271584790608; MAE for tcc: 0.21826436014217337;\n",
    "\n",
    "# RMSE for u10: 1.3972067698487758; MAE for u10: 1.0433846660489516;\n",
    "\n",
    "# RMSE for v10: 1.4072224335796066; MAE for v10: 1.0366618030650943;\n",
    "\n",
    "# RMSE for tp: 0.00026956517312684114; MAE for tp: 0.0001043501217544922;\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# N = 5; hidden = 32; batch = 4; s = 5; r = 2; 32x48;\n",
    "\n",
    "# Epoch 100/100, Train Loss: 2461.7144, lr: 6.25e-05 Val Loss: 3011.8880\n",
    "\n",
    "# 1520.1190974712372 [s]\n",
    "\n",
    "# RMSE for t2m: 1.7308520625058639; MAE for t2m: 1.3247728998925457;\n",
    "\n",
    "# RMSE for sp: 2.3992479669737645; MAE for sp: 1.7568102339318754;\n",
    "\n",
    "# RMSE for tcc: 0.2805274129657387; MAE for tcc: 0.21913696437798416;\n",
    "\n",
    "# RMSE for u10: 1.4401554352858472; MAE for u10: 1.0654466357169252;\n",
    "\n",
    "# RMSE for v10: 1.4595404951536077; MAE for v10: 1.0686582104736524;\n",
    "\n",
    "# RMSE for tp: 0.0002717262792747912; MAE for tp: 0.00010331567566263692;\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# N = 5; hidden = 32; batch = 4; s = 5; r = 2; 32x48 -> 25x45;\n",
    "# ~ 1140 s; Train Loss: 1686.4399, lr: 6.25e-05Val Loss: 2182.0108 (2 trainings)\n",
    "\n",
    "# RMSE for t2m: 1.784685209006511; MAE for t2m: 1.3758342778804562;\n",
    "\n",
    "# RMSE for sp: 2.7761703742751234; MAE for sp: 2.0798808012966767;\n",
    "\n",
    "# RMSE for tcc: 0.278589605641175; MAE for tcc: 0.21694147138915904;\n",
    "\n",
    "# RMSE for u10: 1.413366736459174; MAE for u10: 1.0589858767124307;\n",
    "\n",
    "# RMSE for v10: 1.416900322369188; MAE for v10: 1.0449888444043014;\n",
    "\n",
    "# RMSE for tp: 0.00027346005336134616; MAE for tp: 0.00010490509133304818;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
