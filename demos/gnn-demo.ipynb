{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8222fd-8e11-49eb-8772-859ae12314bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from baselines.gnn.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e96222-4498-45e5-ab0b-dab20c2fcf2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(architecture='cgcn', hidden_dim=32, lr=1e-3)\n",
    "trainer.train(num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdf3215-d13d-4de5-8670-f6b5e61ad345",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.plot_predictions(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ab0e2f-6950-4ea7-aae5-3ac46758a123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b0f57d-cef3-44e0-b44c-39c5e124e99b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.evaluate(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a7d96-8bc8-4e01-8c9a-c4cc407b23f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# [CGCN]\n",
    "\n",
    "### Full training:\n",
    "\n",
    "N = 5; hidden = 32; batch=4; s = 3:\n",
    " \n",
    "RMSE for f0: 2.2464043976548047; MAE for f0: 1.6999678436793948;\n",
    "\n",
    "RMSE for f1: 1.9364765893388307; MAE for f1: 1.408220916940946;\n",
    "\n",
    "RMSE for f2: 0.28386787466464225; MAE for f2: 0.21952327282997064;\n",
    "\n",
    "RMSE for f3: 1.411270602820433; MAE for f3: 1.0502013356032074;\n",
    "\n",
    "RMSE for f4: 1.412410520285922; MAE for f4: 1.0404767270553594;\n",
    "\n",
    "RMSE for f5: 0.0002722878701922671; MAE for f5: 0.0001054861260521275;\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "N = 5; hidden = 64; batch=4; s = 3:\n",
    "\n",
    "RMSE for f0: 2.205407197194004; MAE for f0: 1.6519353043910658;\n",
    "\n",
    "RMSE for f1: 2.029980887836009; MAE for f1: 1.4714761668475365;\n",
    "\n",
    "RMSE for f2: 0.28731255755261337; MAE for f2: 0.22192368753909236;\n",
    "\n",
    "RMSE for f3: 1.4478491874482888; MAE for f3: 1.0733782789693203;\n",
    "\n",
    "RMSE for f4: 1.4493260989133352; MAE for f4: 1.058615083231145;\n",
    "\n",
    "RMSE for f5: 0.0002749573712955729; MAE for f5: 0.00010411299434883103;\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "With bigger batch_size training is more stable (almost no overfitting) but it does not improve performance\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "N = 10; hidden = 32; batch=32; no significant difference, much longer training; even worse performance\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "N = 10; hidden = 32; batch=32; mlp - 3 layers for encoder and decoder each - totally failed to learn sufficient representation; loss after 100 epochs is 2x bigger\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "With gradient clip=100; h=32; b=32; n=5 -> loss plot is more smooth; no performance improvement\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "Using learning constants advised by authors of graphcast does not improve performance (it is actually worse) but training is super stable - no overfittin at all:\n",
    "clip=32; self.optimizer = torch.optim.AdamW(self.model.parameters(), betas=(0.9, 0.95), weight_decay=0.1); h=32; b=32\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "N = 5; hidden = 32; batch = 4; s = 7\n",
    "\n",
    "Epoch 100/100, Train Loss: 1848.7699, lr: 3.125e-05 Val Loss: 2230.6389\n",
    "735.7001855373383 [s]22666046945865614\n",
    "\n",
    "RMSE for t2m: 1.8891855143316563; MAE for t2m: 1.4600152796625336;\n",
    "\n",
    "RMSE for sp: 2.0393454691406316; MAE for sp: 1.5120049358744065;\n",
    "\n",
    "RMSE for tcc: 0.2860184447921189; MAE for tcc: 0.22666046945865614;\n",
    "\n",
    "RMSE for u10: 1.4378177755816777; MAE for u10: 1.0736904505733116;\n",
    "\n",
    "RMSE for v10: 1.4574477223923084; MAE for v10: 1.0663613129750227;\n",
    "\n",
    "RMSE for tp: 0.0002711004547077864; MAE for tp: 0.00010284140930705948;\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "N = 5; hidden = 32; batch = 4; s = 3\n",
    "\n",
    "Epoch 99/100, Train Loss: 1955.0575, lr: 0.0005 Val Loss: 2207.3025\n",
    "\n",
    "693.315672159195 [s]\n",
    "\n",
    "RMSE for t2m: 2.3858469221825898; MAE for t2m: 1.8332994478389537;\n",
    "\n",
    "RMSE for sp: 1.9815898823743083; MAE for sp: 1.4750331930477616;\n",
    "\n",
    "RMSE for tcc: 0.28416700897460695; MAE for tcc: 0.23001444125440887;\n",
    "\n",
    "RMSE for u10: 1.4444404739897925; MAE for u10: 1.0740849332227052;\n",
    "\n",
    "RMSE for v10: 1.4384182183045433; MAE for v10: 1.05619047567965;\n",
    "\n",
    "RMSE for tp: 0.0002669724249068656; MAE for tp: 0.00010341137409668945;\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "N = 5; hidden = 32; batch = 4; s = 5\n",
    "\n",
    "Epoch 100/100, Train Loss: 1901.1544, lr: 6.25e-05 Val Loss: 2181.1193\n",
    "\n",
    "678.085875749588 [s]\n",
    "\n",
    "RMSE for t2m: 1.8778959983044772; MAE for t2m: 1.4465936457254398;\n",
    "\n",
    "RMSE for sp: 2.190103341129505; MAE for sp: 1.6404208756272267;\n",
    "\n",
    "RMSE for tcc: 0.2848521568307657; MAE for tcc: 0.22361195825242483;\n",
    "\n",
    "RMSE for u10: 1.437490006965024; MAE for u10: 1.0706755302203412;\n",
    "\n",
    "RMSE for v10: 1.4364091328286357; MAE for v10: 1.0579831289563082;\n",
    "\n",
    "RMSE for tp: 0.0002660477946055829; MAE for tp: 0.00010596483422627151;\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "N = 5; hidden = 32; batch = 4; s = 5; r=2\n",
    "\n",
    "Epoch 100/100, Train Loss: 1882.7291, lr: 1.5625e-05 Val Loss: 2143.8953 (*at some point 2129)\n",
    "\n",
    "887.3842961788177 [s]\n",
    "\n",
    "RMSE for t2m: 1.8323451416138599; MAE for t2m: 1.4136113810162858;\n",
    "\n",
    "RMSE for sp: 2.0521229454022802; MAE for sp: 1.527130837420577;\n",
    "\n",
    "RMSE for tcc: 0.2793271584790608; MAE for tcc: 0.21826436014217337;\n",
    "\n",
    "RMSE for u10: 1.3972067698487758; MAE for u10: 1.0433846660489516;\n",
    "\n",
    "RMSE for v10: 1.4072224335796066; MAE for v10: 1.0366618030650943;\n",
    "\n",
    "RMSE for tp: 0.00026956517312684114; MAE for tp: 0.0001043501217544922;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
