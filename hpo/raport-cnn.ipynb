{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iwama\\Desktop\\Maciej\\Eng\\meteoapp-data\\venv\\lib\\site-packages\\gribapi\\__init__.py:23: UserWarning: ecCodes 2.31.0 or higher is recommended. You are running version 2.27.0\n",
      "  warnings.warn(\n",
      "c:\\Users\\iwama\\Desktop\\Maciej\\Eng\\meteoapp-data\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from hpo import HPO\n",
    "from models.config import config as cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "'baseline_type' : 'cnn',\n",
    "'n_trials' : 10,\n",
    "'sequence_n_trials' : 10,\n",
    "'fh_n_trials' : 10,\n",
    "'num_epochs' : 1000,\n",
    "'sequence_length' : 6,\n",
    "'forcasting_horizon' : 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo = HPO(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 0.06988, lr: 0.001---------------------------| 0.0% Complete\n",
      "Val Loss: 0.05803\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.05186, lr: 0.001\n",
      "Val Loss: 0.05071\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.04931, lr: 0.001\n",
      "Val Loss: 0.04882\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.04840, lr: 0.001\n",
      "Val Loss: 0.04815\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.04748, lr: 0.001\n",
      "Val Loss: 0.04696\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.04555, lr: 0.001\n",
      "Val Loss: 0.04519\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.04470, lr: 0.001\n",
      "Val Loss: 0.04448\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.04421, lr: 0.001\n",
      "Val Loss: 0.04393\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.04388, lr: 0.001\n",
      "Val Loss: 0.04357\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.04364, lr: 0.001\n",
      "Val Loss: 0.04330\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.04343, lr: 0.001\n",
      "Val Loss: 0.04308\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.04328, lr: 0.001\n",
      "Val Loss: 0.04290\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.04316, lr: 0.001\n",
      "Val Loss: 0.04277\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.04305, lr: 0.001\n",
      "Val Loss: 0.04263\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.04296, lr: 0.001\n",
      "Val Loss: 0.04253\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.04291, lr: 0.001\n",
      "Val Loss: 0.04246\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.04282, lr: 0.001\n",
      "Val Loss: 0.04236\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.04276, lr: 0.001\n",
      "Val Loss: 0.04234\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.04269, lr: 0.001\n",
      "Val Loss: 0.04232\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.04263, lr: 0.001\n",
      "Val Loss: 0.04228\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.04258, lr: 0.001\n",
      "Val Loss: 0.04220\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.04253, lr: 0.001\n",
      "Val Loss: 0.04215\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.04248, lr: 0.001\n",
      "Val Loss: 0.04209\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.04244, lr: 0.001\n",
      "Val Loss: 0.04202\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.04239, lr: 0.001\n",
      "Val Loss: 0.04197\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.04235, lr: 0.001\n",
      "Val Loss: 0.04193\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.04229, lr: 0.001\n",
      "Val Loss: 0.04190\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.04224, lr: 0.001\n",
      "Val Loss: 0.04186\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.04220, lr: 0.001\n",
      "Val Loss: 0.04182\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.04217, lr: 0.001\n",
      "Val Loss: 0.04180\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.04213, lr: 0.001\n",
      "Val Loss: 0.04179\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.04210, lr: 0.001\n",
      "Val Loss: 0.04178\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.04207, lr: 0.001\n",
      "Val Loss: 0.04176\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.04204, lr: 0.001\n",
      "Val Loss: 0.04175\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.04201, lr: 0.001\n",
      "Val Loss: 0.04173\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.04198, lr: 0.001\n",
      "Val Loss: 0.04174\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.04196, lr: 0.001\n",
      "Val Loss: 0.04173\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.04193, lr: 0.001\n",
      "Val Loss: 0.04175\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.04190, lr: 0.001\n",
      "Val Loss: 0.04173\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.04187, lr: 0.001\n",
      "Val Loss: 0.04176\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.04185, lr: 0.001\n",
      "Val Loss: 0.04175\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.04181, lr: 0.001\n",
      "Val Loss: 0.04177\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.04179, lr: 0.001\n",
      "Val Loss: 0.04176\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.04176, lr: 0.001\n",
      "Val Loss: 0.04178\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 45/1000, Train Loss: 0.04104, lr: 0.0005\n",
      "Val Loss: 0.04212\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.04102, lr: 0.0005\n",
      "Val Loss: 0.04207\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.04099, lr: 0.0005\n",
      "Val Loss: 0.04204\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.04098, lr: 0.0005\n",
      "Val Loss: 0.04201\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.04096, lr: 0.0005\n",
      "Val Loss: 0.04199\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.04094, lr: 0.0005\n",
      "Val Loss: 0.04196\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.04092, lr: 0.0005\n",
      "Val Loss: 0.04195\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 52/1000, Train Loss: 0.04051, lr: 0.00025\n",
      "Val Loss: 0.04135\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.04048, lr: 0.00025\n",
      "Val Loss: 0.04134\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.04047, lr: 0.00025\n",
      "Val Loss: 0.04133\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.04046, lr: 0.00025\n",
      "Val Loss: 0.04132\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.04045, lr: 0.00025\n",
      "Val Loss: 0.04132\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.04044, lr: 0.00025\n",
      "Val Loss: 0.04132\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.04043, lr: 0.00025\n",
      "Val Loss: 0.04131\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.04042, lr: 0.00025\n",
      "Val Loss: 0.04131\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.04041, lr: 0.00025\n",
      "Val Loss: 0.04131\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.04041, lr: 0.00025\n",
      "Val Loss: 0.04130\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.04040, lr: 0.00025\n",
      "Val Loss: 0.04130\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.04039, lr: 0.00025\n",
      "Val Loss: 0.04130\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.04038, lr: 0.00025\n",
      "Val Loss: 0.04129\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.04037, lr: 0.00025\n",
      "Val Loss: 0.04129\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.04036, lr: 0.00025\n",
      "Val Loss: 0.04128\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.04035, lr: 0.00025\n",
      "Val Loss: 0.04127\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.04035, lr: 0.00025\n",
      "Val Loss: 0.04127\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.04034, lr: 0.00025\n",
      "Val Loss: 0.04126\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.04033, lr: 0.00025\n",
      "Val Loss: 0.04126\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.04032, lr: 0.00025\n",
      "Val Loss: 0.04125\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.04031, lr: 0.00025\n",
      "Val Loss: 0.04125\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.04030, lr: 0.00025\n",
      "Val Loss: 0.04124\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.04030, lr: 0.00025\n",
      "Val Loss: 0.04123\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.04029, lr: 0.00025\n",
      "Val Loss: 0.04123\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.04028, lr: 0.00025\n",
      "Val Loss: 0.04122\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.04027, lr: 0.00025\n",
      "Val Loss: 0.04121\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.04027, lr: 0.00025\n",
      "Val Loss: 0.04121\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.04026, lr: 0.00025\n",
      "Val Loss: 0.04120\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.04025, lr: 0.00025\n",
      "Val Loss: 0.04120\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.04024, lr: 0.00025\n",
      "Val Loss: 0.04119\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.04024, lr: 0.00025\n",
      "Val Loss: 0.04119\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.04023, lr: 0.00025\n",
      "Val Loss: 0.04118\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.04022, lr: 0.00025\n",
      "Val Loss: 0.04118\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.04021, lr: 0.00025\n",
      "Val Loss: 0.04117\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.04021, lr: 0.00025\n",
      "Val Loss: 0.04117\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.04020, lr: 0.00025\n",
      "Val Loss: 0.04116\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.04019, lr: 0.00025\n",
      "Val Loss: 0.04115\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.04018, lr: 0.00025\n",
      "Val Loss: 0.04115\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.04018, lr: 0.00025\n",
      "Val Loss: 0.04114\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.04017, lr: 0.00025\n",
      "Val Loss: 0.04114\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.04016, lr: 0.00025\n",
      "Val Loss: 0.04113\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.04016, lr: 0.00025\n",
      "Val Loss: 0.04113\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.04015, lr: 0.00025\n",
      "Val Loss: 0.04113\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.04014, lr: 0.00025\n",
      "Val Loss: 0.04112\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.04013, lr: 0.00025\n",
      "Val Loss: 0.04112\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.04013, lr: 0.00025\n",
      "Val Loss: 0.04111\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.04012, lr: 0.00025\n",
      "Val Loss: 0.04111\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.04011, lr: 0.00025\n",
      "Val Loss: 0.04111\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.04011, lr: 0.00025\n",
      "Val Loss: 0.04110\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.04010, lr: 0.00025\n",
      "Val Loss: 0.04110\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.04009, lr: 0.00025\n",
      "Val Loss: 0.04109\n",
      "---------\n",
      "Epoch 103/1000, Train Loss: 0.04008, lr: 0.00025\n",
      "Val Loss: 0.04109\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.04008, lr: 0.00025\n",
      "Val Loss: 0.04108\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.04007, lr: 0.00025\n",
      "Val Loss: 0.04107\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.04006, lr: 0.00025\n",
      "Val Loss: 0.04107\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.04006, lr: 0.00025\n",
      "Val Loss: 0.04107\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.04005, lr: 0.00025\n",
      "Val Loss: 0.04106\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.04005, lr: 0.00025\n",
      "Val Loss: 0.04106\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.04004, lr: 0.00025\n",
      "Val Loss: 0.04105\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.04003, lr: 0.00025\n",
      "Val Loss: 0.04105\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.04003, lr: 0.00025\n",
      "Val Loss: 0.04104\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.04002, lr: 0.00025\n",
      "Val Loss: 0.04104\n",
      "---------\n",
      "Epoch 114/1000, Train Loss: 0.04001, lr: 0.00025\n",
      "Val Loss: 0.04104\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.04001, lr: 0.00025\n",
      "Val Loss: 0.04103\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.04000, lr: 0.00025\n",
      "Val Loss: 0.04102\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.04000, lr: 0.00025\n",
      "Val Loss: 0.04102\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.03999, lr: 0.00025\n",
      "Val Loss: 0.04101\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.03998, lr: 0.00025\n",
      "Val Loss: 0.04101\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.03998, lr: 0.00025\n",
      "Val Loss: 0.04100\n",
      "---------\n",
      "Epoch 121/1000, Train Loss: 0.03997, lr: 0.00025\n",
      "Val Loss: 0.04100\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.03997, lr: 0.00025\n",
      "Val Loss: 0.04099\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.03996, lr: 0.00025\n",
      "Val Loss: 0.04099\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.03996, lr: 0.00025\n",
      "Val Loss: 0.04099\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.03995, lr: 0.00025\n",
      "Val Loss: 0.04098\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.03995, lr: 0.00025\n",
      "Val Loss: 0.04098\n",
      "---------\n",
      "Epoch 127/1000, Train Loss: 0.03994, lr: 0.00025\n",
      "Val Loss: 0.04098\n",
      "---------\n",
      "Epoch 128/1000, Train Loss: 0.03994, lr: 0.00025\n",
      "Val Loss: 0.04097\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.03993, lr: 0.00025\n",
      "Val Loss: 0.04097\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.03993, lr: 0.00025\n",
      "Val Loss: 0.04097\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.03992, lr: 0.00025\n",
      "Val Loss: 0.04096\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.03992, lr: 0.00025\n",
      "Val Loss: 0.04096\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.03991, lr: 0.00025\n",
      "Val Loss: 0.04095\n",
      "---------\n",
      "Epoch 134/1000, Train Loss: 0.03991, lr: 0.00025\n",
      "Val Loss: 0.04095\n",
      "---------\n",
      "Epoch 135/1000, Train Loss: 0.03990, lr: 0.00025\n",
      "Val Loss: 0.04095\n",
      "---------\n",
      "Epoch 136/1000, Train Loss: 0.03990, lr: 0.00025\n",
      "Val Loss: 0.04094\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.03989, lr: 0.00025\n",
      "Val Loss: 0.04094\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.03989, lr: 0.00025\n",
      "Val Loss: 0.04094\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.03988, lr: 0.00025\n",
      "Val Loss: 0.04093\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.03988, lr: 0.00025\n",
      "Val Loss: 0.04093\n",
      "---------\n",
      "Epoch 141/1000, Train Loss: 0.03987, lr: 0.00025\n",
      "Val Loss: 0.04093\n",
      "---------\n",
      "Epoch 142/1000, Train Loss: 0.03987, lr: 0.00025\n",
      "Val Loss: 0.04092\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.03986, lr: 0.00025\n",
      "Val Loss: 0.04092\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.03986, lr: 0.00025\n",
      "Val Loss: 0.04092\n",
      "---------\n",
      "Epoch 145/1000, Train Loss: 0.03986, lr: 0.00025\n",
      "Val Loss: 0.04091\n",
      "---------\n",
      "Epoch 146/1000, Train Loss: 0.03985, lr: 0.00025\n",
      "Val Loss: 0.04091\n",
      "---------\n",
      "Epoch 147/1000, Train Loss: 0.03985, lr: 0.00025\n",
      "Val Loss: 0.04091\n",
      "---------\n",
      "Epoch 148/1000, Train Loss: 0.03984, lr: 0.00025\n",
      "Val Loss: 0.04091\n",
      "---------\n",
      "Epoch 149/1000, Train Loss: 0.03984, lr: 0.00025\n",
      "Val Loss: 0.04090\n",
      "---------\n",
      "Epoch 150/1000, Train Loss: 0.03983, lr: 0.00025\n",
      "Val Loss: 0.04090\n",
      "---------\n",
      "Epoch 151/1000, Train Loss: 0.03983, lr: 0.00025\n",
      "Val Loss: 0.04090\n",
      "---------\n",
      "Epoch 152/1000, Train Loss: 0.03982, lr: 0.00025\n",
      "Val Loss: 0.04089\n",
      "---------\n",
      "Epoch 153/1000, Train Loss: 0.03982, lr: 0.00025\n",
      "Val Loss: 0.04089\n",
      "---------\n",
      "Epoch 154/1000, Train Loss: 0.03982, lr: 0.00025\n",
      "Val Loss: 0.04089\n",
      "---------\n",
      "Epoch 155/1000, Train Loss: 0.03981, lr: 0.00025\n",
      "Val Loss: 0.04088\n",
      "---------\n",
      "Epoch 156/1000, Train Loss: 0.03981, lr: 0.00025\n",
      "Val Loss: 0.04088\n",
      "---------\n",
      "Epoch 157/1000, Train Loss: 0.03980, lr: 0.00025\n",
      "Val Loss: 0.04088\n",
      "---------\n",
      "Epoch 158/1000, Train Loss: 0.03980, lr: 0.00025\n",
      "Val Loss: 0.04088\n",
      "---------\n",
      "Epoch 159/1000, Train Loss: 0.03980, lr: 0.00025\n",
      "Val Loss: 0.04087\n",
      "---------\n",
      "Epoch 160/1000, Train Loss: 0.03979, lr: 0.00025\n",
      "Val Loss: 0.04087\n",
      "---------\n",
      "Epoch 161/1000, Train Loss: 0.03979, lr: 0.00025\n",
      "Val Loss: 0.04087\n",
      "---------\n",
      "Epoch 162/1000, Train Loss: 0.03978, lr: 0.00025\n",
      "Val Loss: 0.04086\n",
      "---------\n",
      "Epoch 163/1000, Train Loss: 0.03978, lr: 0.00025\n",
      "Val Loss: 0.04086\n",
      "---------\n",
      "Epoch 164/1000, Train Loss: 0.03978, lr: 0.00025\n",
      "Val Loss: 0.04086\n",
      "---------\n",
      "Epoch 165/1000, Train Loss: 0.03977, lr: 0.00025\n",
      "Val Loss: 0.04086\n",
      "---------\n",
      "Epoch 166/1000, Train Loss: 0.03977, lr: 0.00025\n",
      "Val Loss: 0.04085\n",
      "---------\n",
      "Epoch 167/1000, Train Loss: 0.03976, lr: 0.00025\n",
      "Val Loss: 0.04085\n",
      "---------\n",
      "Epoch 168/1000, Train Loss: 0.03976, lr: 0.00025\n",
      "Val Loss: 0.04084\n",
      "---------\n",
      "Epoch 169/1000, Train Loss: 0.03976, lr: 0.00025\n",
      "Val Loss: 0.04084\n",
      "---------\n",
      "Epoch 170/1000, Train Loss: 0.03975, lr: 0.00025\n",
      "Val Loss: 0.04084\n",
      "---------\n",
      "Epoch 171/1000, Train Loss: 0.03975, lr: 0.00025\n",
      "Val Loss: 0.04084\n",
      "---------\n",
      "Epoch 172/1000, Train Loss: 0.03975, lr: 0.00025\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 173/1000, Train Loss: 0.03974, lr: 0.00025\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 174/1000, Train Loss: 0.03974, lr: 0.00025\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 175/1000, Train Loss: 0.03973, lr: 0.00025\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 176/1000, Train Loss: 0.03973, lr: 0.00025\n",
      "Val Loss: 0.04082\n",
      "---------\n",
      "Epoch 177/1000, Train Loss: 0.03973, lr: 0.00025\n",
      "Val Loss: 0.04082\n",
      "---------\n",
      "Epoch 178/1000, Train Loss: 0.03972, lr: 0.00025\n",
      "Val Loss: 0.04082\n",
      "---------\n",
      "Epoch 179/1000, Train Loss: 0.03972, lr: 0.00025\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 180/1000, Train Loss: 0.03972, lr: 0.00025\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 181/1000, Train Loss: 0.03971, lr: 0.00025\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 182/1000, Train Loss: 0.03971, lr: 0.00025\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 183/1000, Train Loss: 0.03971, lr: 0.00025\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 184/1000, Train Loss: 0.03970, lr: 0.00025\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 185/1000, Train Loss: 0.03970, lr: 0.00025\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 186/1000, Train Loss: 0.03969, lr: 0.00025\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 187/1000, Train Loss: 0.03969, lr: 0.00025\n",
      "Val Loss: 0.04079\n",
      "---------\n",
      "Epoch 188/1000, Train Loss: 0.03969, lr: 0.00025\n",
      "Val Loss: 0.04079\n",
      "---------\n",
      "Epoch 189/1000, Train Loss: 0.03968, lr: 0.00025\n",
      "Val Loss: 0.04079\n",
      "---------\n",
      "Epoch 190/1000, Train Loss: 0.03968, lr: 0.00025\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 191/1000, Train Loss: 0.03968, lr: 0.00025\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 192/1000, Train Loss: 0.03967, lr: 0.00025\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 193/1000, Train Loss: 0.03967, lr: 0.00025\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 194/1000, Train Loss: 0.03967, lr: 0.00025\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 195/1000, Train Loss: 0.03966, lr: 0.00025\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 196/1000, Train Loss: 0.03966, lr: 0.00025\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 197/1000, Train Loss: 0.03966, lr: 0.00025\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 198/1000, Train Loss: 0.03965, lr: 0.00025\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 199/1000, Train Loss: 0.03965, lr: 0.00025\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 200/1000, Train Loss: 0.03965, lr: 0.00025\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 201/1000, Train Loss: 0.03964, lr: 0.00025\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 202/1000, Train Loss: 0.03964, lr: 0.00025\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 203/1000, Train Loss: 0.03964, lr: 0.00025\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 204/1000, Train Loss: 0.03963, lr: 0.00025\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 205/1000, Train Loss: 0.03963, lr: 0.00025\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 206/1000, Train Loss: 0.03963, lr: 0.00025\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 207/1000, Train Loss: 0.03962, lr: 0.00025\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 208/1000, Train Loss: 0.03962, lr: 0.00025\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 209/1000, Train Loss: 0.03962, lr: 0.00025\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 210/1000, Train Loss: 0.03961, lr: 0.00025\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 211/1000, Train Loss: 0.03961, lr: 0.00025\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 212/1000, Train Loss: 0.03961, lr: 0.00025\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 213/1000, Train Loss: 0.03960, lr: 0.00025\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 214/1000, Train Loss: 0.03960, lr: 0.00025\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 215/1000, Train Loss: 0.03960, lr: 0.00025\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 216/1000, Train Loss: 0.03959, lr: 0.00025\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 217/1000, Train Loss: 0.03959, lr: 0.00025\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 218/1000, Train Loss: 0.03959, lr: 0.00025\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 219/1000, Train Loss: 0.03958, lr: 0.00025\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 220/1000, Train Loss: 0.03958, lr: 0.00025\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 221/1000, Train Loss: 0.03958, lr: 0.00025\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 222/1000, Train Loss: 0.03957, lr: 0.00025\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 223/1000, Train Loss: 0.03957, lr: 0.00025\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 224/1000, Train Loss: 0.03957, lr: 0.00025\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 225/1000, Train Loss: 0.03957, lr: 0.00025\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 226/1000, Train Loss: 0.03956, lr: 0.00025\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 227/1000, Train Loss: 0.03956, lr: 0.00025\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 228/1000, Train Loss: 0.03956, lr: 0.00025\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 229/1000, Train Loss: 0.03955, lr: 0.00025\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 230/1000, Train Loss: 0.03955, lr: 0.00025\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 231/1000, Train Loss: 0.03955, lr: 0.00025\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 232/1000, Train Loss: 0.03954, lr: 0.00025\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 233/1000, Train Loss: 0.03954, lr: 0.00025\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 234/1000, Train Loss: 0.03954, lr: 0.00025\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 235/1000, Train Loss: 0.03953, lr: 0.00025\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 236/1000, Train Loss: 0.03953, lr: 0.00025\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 237/1000, Train Loss: 0.03953, lr: 0.00025\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 238/1000, Train Loss: 0.03952, lr: 0.00025\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 239/1000, Train Loss: 0.03952, lr: 0.00025\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 240/1000, Train Loss: 0.03952, lr: 0.00025\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 241/1000, Train Loss: 0.03951, lr: 0.00025\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 242/1000, Train Loss: 0.03951, lr: 0.00025\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 243/1000, Train Loss: 0.03951, lr: 0.00025\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 244/1000, Train Loss: 0.03951, lr: 0.00025\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 245/1000, Train Loss: 0.03950, lr: 0.00025\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 246/1000, Train Loss: 0.03950, lr: 0.00025\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 247/1000, Train Loss: 0.03950, lr: 0.00025\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 248/1000, Train Loss: 0.03949, lr: 0.00025\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 249/1000, Train Loss: 0.03949, lr: 0.00025\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 250/1000, Train Loss: 0.03949, lr: 0.00025\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 251/1000, Train Loss: 0.03949, lr: 0.00025\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 252/1000, Train Loss: 0.03948, lr: 0.00025\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 253/1000, Train Loss: 0.03948, lr: 0.00025\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 254/1000, Train Loss: 0.03948, lr: 0.00025\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 255/1000, Train Loss: 0.03947, lr: 0.00025\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 256/1000, Train Loss: 0.03947, lr: 0.00025\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 257/1000, Train Loss: 0.03947, lr: 0.00025\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 258/1000, Train Loss: 0.03946, lr: 0.00025\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 259/1000, Train Loss: 0.03946, lr: 0.00025\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 260/1000, Train Loss: 0.03946, lr: 0.00025\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 261/1000, Train Loss: 0.03946, lr: 0.00025\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 262/1000, Train Loss: 0.03945, lr: 0.00025\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 263/1000, Train Loss: 0.03945, lr: 0.00025\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 264/1000, Train Loss: 0.03945, lr: 0.00025\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 265/1000, Train Loss: 0.03944, lr: 0.00025\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 266/1000, Train Loss: 0.03944, lr: 0.00025\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 267/1000, Train Loss: 0.03944, lr: 0.00025\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 268/1000, Train Loss: 0.03944, lr: 0.00025\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 269/1000, Train Loss: 0.03943, lr: 0.00025\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 270/1000, Train Loss: 0.03943, lr: 0.00025\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 271/1000, Train Loss: 0.03943, lr: 0.00025\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 272/1000, Train Loss: 0.03943, lr: 0.00025\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 273/1000, Train Loss: 0.03942, lr: 0.00025\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 274/1000, Train Loss: 0.03942, lr: 0.00025\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 275/1000, Train Loss: 0.03942, lr: 0.00025\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 276/1000, Train Loss: 0.03941, lr: 0.00025\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 277/1000, Train Loss: 0.03941, lr: 0.00025\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 278/1000, Train Loss: 0.03941, lr: 0.00025\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 279/1000, Train Loss: 0.03941, lr: 0.00025\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 280/1000, Train Loss: 0.03940, lr: 0.00025\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 281/1000, Train Loss: 0.03940, lr: 0.00025\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 282/1000, Train Loss: 0.03940, lr: 0.00025\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 283/1000, Train Loss: 0.03940, lr: 0.00025\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 284/1000, Train Loss: 0.03939, lr: 0.00025\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 285/1000, Train Loss: 0.03939, lr: 0.00025\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 286/1000, Train Loss: 0.03939, lr: 0.00025\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 287/1000, Train Loss: 0.03939, lr: 0.00025\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 288/1000, Train Loss: 0.03938, lr: 0.00025\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 289/1000, Train Loss: 0.03938, lr: 0.00025\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 290/1000, Train Loss: 0.03938, lr: 0.00025\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 291/1000, Train Loss: 0.03937, lr: 0.00025\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 292/1000, Train Loss: 0.03937, lr: 0.00025\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 293/1000, Train Loss: 0.03937, lr: 0.00025\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 294/1000, Train Loss: 0.03937, lr: 0.00025\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 295/1000, Train Loss: 0.03936, lr: 0.00025\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 296/1000, Train Loss: 0.03936, lr: 0.00025\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 297/1000, Train Loss: 0.03936, lr: 0.00025\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 298/1000, Train Loss: 0.03936, lr: 0.00025\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 299/1000, Train Loss: 0.03936, lr: 0.00025\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 300/1000, Train Loss: 0.03935, lr: 0.00025\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 301/1000, Train Loss: 0.03935, lr: 0.00025\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 302/1000, Train Loss: 0.03935, lr: 0.00025\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 303/1000, Train Loss: 0.03935, lr: 0.00025\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 304/1000, Train Loss: 0.03934, lr: 0.00025\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 305/1000, Train Loss: 0.03934, lr: 0.00025\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 306/1000, Train Loss: 0.03934, lr: 0.00025\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 307/1000, Train Loss: 0.03934, lr: 0.00025\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 308/1000, Train Loss: 0.03933, lr: 0.00025\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 309/1000, Train Loss: 0.03933, lr: 0.00025\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 310/1000, Train Loss: 0.03933, lr: 0.00025\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 311/1000, Train Loss: 0.03933, lr: 0.00025\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 312/1000, Train Loss: 0.03932, lr: 0.00025\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 313/1000, Train Loss: 0.03932, lr: 0.00025\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 314/1000, Train Loss: 0.03932, lr: 0.00025\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 315/1000, Train Loss: 0.03932, lr: 0.00025\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 316/1000, Train Loss: 0.03931, lr: 0.00025\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 317/1000, Train Loss: 0.03931, lr: 0.00025\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 318/1000, Train Loss: 0.03931, lr: 0.00025\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 319/1000, Train Loss: 0.03931, lr: 0.00025\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 320/1000, Train Loss: 0.03930, lr: 0.00025\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 321/1000, Train Loss: 0.03930, lr: 0.00025\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 322/1000, Train Loss: 0.03930, lr: 0.00025\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 323/1000, Train Loss: 0.03930, lr: 0.00025\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 324/1000, Train Loss: 0.03930, lr: 0.00025\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 325/1000, Train Loss: 0.03929, lr: 0.00025\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 326/1000, Train Loss: 0.03929, lr: 0.00025\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 327/1000, Train Loss: 0.03929, lr: 0.00025\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 328/1000, Train Loss: 0.03929, lr: 0.00025\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 329/1000, Train Loss: 0.03928, lr: 0.00025\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 330/1000, Train Loss: 0.03928, lr: 0.00025\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 331/1000, Train Loss: 0.03928, lr: 0.00025\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 332/1000, Train Loss: 0.03928, lr: 0.00025\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 333/1000, Train Loss: 0.03927, lr: 0.00025\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 334/1000, Train Loss: 0.03927, lr: 0.00025\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 335/1000, Train Loss: 0.03927, lr: 0.00025\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 336/1000, Train Loss: 0.03927, lr: 0.00025\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 337/1000, Train Loss: 0.03927, lr: 0.00025\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 338/1000, Train Loss: 0.03926, lr: 0.00025\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 339/1000, Train Loss: 0.03926, lr: 0.00025\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 340/1000, Train Loss: 0.03926, lr: 0.00025\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 341/1000, Train Loss: 0.03926, lr: 0.00025\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 342/1000, Train Loss: 0.03925, lr: 0.00025\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 343/1000, Train Loss: 0.03925, lr: 0.00025\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 344/1000, Train Loss: 0.03925, lr: 0.00025\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 345/1000, Train Loss: 0.03924, lr: 0.00025\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 346/1000, Train Loss: 0.03924, lr: 0.00025\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 347/1000, Train Loss: 0.03924, lr: 0.00025\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 348/1000, Train Loss: 0.03924, lr: 0.00025\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 349/1000, Train Loss: 0.03923, lr: 0.00025\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 350/1000, Train Loss: 0.03923, lr: 0.00025\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 351/1000, Train Loss: 0.03923, lr: 0.00025\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 352/1000, Train Loss: 0.03923, lr: 0.00025\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 353/1000, Train Loss: 0.03922, lr: 0.00025\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 354/1000, Train Loss: 0.03922, lr: 0.00025\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 355/1000, Train Loss: 0.03922, lr: 0.00025\n",
      "Val Loss: 0.04051\n",
      "---------\n",
      "Epoch 356/1000, Train Loss: 0.03922, lr: 0.00025\n",
      "Val Loss: 0.04051\n",
      "---------\n",
      "Epoch 357/1000, Train Loss: 0.03921, lr: 0.00025\n",
      "Val Loss: 0.04051\n",
      "---------\n",
      "Epoch 358/1000, Train Loss: 0.03921, lr: 0.00025\n",
      "Val Loss: 0.04051\n",
      "---------\n",
      "Epoch 359/1000, Train Loss: 0.03921, lr: 0.00025\n",
      "Val Loss: 0.04051\n",
      "---------\n",
      "Epoch 360/1000, Train Loss: 0.03921, lr: 0.00025\n",
      "Val Loss: 0.04051\n",
      "---------\n",
      "Epoch 361/1000, Train Loss: 0.03920, lr: 0.00025\n",
      "Val Loss: 0.04050\n",
      "---------\n",
      "Epoch 362/1000, Train Loss: 0.03920, lr: 0.00025\n",
      "Val Loss: 0.04050\n",
      "---------\n",
      "Epoch 363/1000, Train Loss: 0.03920, lr: 0.00025\n",
      "Val Loss: 0.04050\n",
      "---------\n",
      "Epoch 364/1000, Train Loss: 0.03920, lr: 0.00025\n",
      "Val Loss: 0.04050\n",
      "---------\n",
      "Epoch 365/1000, Train Loss: 0.03920, lr: 0.00025\n",
      "Val Loss: 0.04050\n",
      "---------\n",
      "Epoch 366/1000, Train Loss: 0.03919, lr: 0.00025\n",
      "Val Loss: 0.04050\n",
      "---------\n",
      "Epoch 367/1000, Train Loss: 0.03919, lr: 0.00025\n",
      "Val Loss: 0.04050\n",
      "---------\n",
      "Epoch 368/1000, Train Loss: 0.03919, lr: 0.00025\n",
      "Val Loss: 0.04049\n",
      "---------\n",
      "Epoch 369/1000, Train Loss: 0.03919, lr: 0.00025\n",
      "Val Loss: 0.04049\n",
      "---------\n",
      "Epoch 370/1000, Train Loss: 0.03918, lr: 0.00025\n",
      "Val Loss: 0.04049\n",
      "---------\n",
      "Epoch 371/1000, Train Loss: 0.03918, lr: 0.00025\n",
      "Val Loss: 0.04049\n",
      "---------\n",
      "Epoch 372/1000, Train Loss: 0.03918, lr: 0.00025\n",
      "Val Loss: 0.04049\n",
      "---------\n",
      "Epoch 373/1000, Train Loss: 0.03918, lr: 0.00025\n",
      "Val Loss: 0.04049\n",
      "---------\n",
      "Epoch 374/1000, Train Loss: 0.03917, lr: 0.00025\n",
      "Val Loss: 0.04048\n",
      "---------\n",
      "Epoch 375/1000, Train Loss: 0.03917, lr: 0.00025\n",
      "Val Loss: 0.04048\n",
      "---------\n",
      "Epoch 376/1000, Train Loss: 0.03917, lr: 0.00025\n",
      "Val Loss: 0.04048\n",
      "---------\n",
      "Epoch 377/1000, Train Loss: 0.03917, lr: 0.00025\n",
      "Val Loss: 0.04048\n",
      "---------\n",
      "Epoch 378/1000, Train Loss: 0.03916, lr: 0.00025\n",
      "Val Loss: 0.04047\n",
      "---------\n",
      "Epoch 379/1000, Train Loss: 0.03916, lr: 0.00025\n",
      "Val Loss: 0.04047\n",
      "---------\n",
      "Epoch 380/1000, Train Loss: 0.03915, lr: 0.00025\n",
      "Val Loss: 0.04046\n",
      "---------\n",
      "Epoch 381/1000, Train Loss: 0.03915, lr: 0.00025\n",
      "Val Loss: 0.04046\n",
      "---------\n",
      "Epoch 382/1000, Train Loss: 0.03914, lr: 0.00025\n",
      "Val Loss: 0.04046\n",
      "---------\n",
      "Epoch 383/1000, Train Loss: 0.03914, lr: 0.00025\n",
      "Val Loss: 0.04045\n",
      "---------\n",
      "Epoch 384/1000, Train Loss: 0.03913, lr: 0.00025\n",
      "Val Loss: 0.04045\n",
      "---------\n",
      "Epoch 385/1000, Train Loss: 0.03913, lr: 0.00025\n",
      "Val Loss: 0.04044\n",
      "---------\n",
      "Epoch 386/1000, Train Loss: 0.03912, lr: 0.00025\n",
      "Val Loss: 0.04043\n",
      "---------\n",
      "Epoch 387/1000, Train Loss: 0.03912, lr: 0.00025\n",
      "Val Loss: 0.04043\n",
      "---------\n",
      "Epoch 388/1000, Train Loss: 0.03911, lr: 0.00025\n",
      "Val Loss: 0.04042\n",
      "---------\n",
      "Epoch 389/1000, Train Loss: 0.03911, lr: 0.00025\n",
      "Val Loss: 0.04042\n",
      "---------\n",
      "Epoch 390/1000, Train Loss: 0.03910, lr: 0.00025\n",
      "Val Loss: 0.04042\n",
      "---------\n",
      "Epoch 391/1000, Train Loss: 0.03910, lr: 0.00025\n",
      "Val Loss: 0.04042\n",
      "---------\n",
      "Epoch 392/1000, Train Loss: 0.03909, lr: 0.00025\n",
      "Val Loss: 0.04041\n",
      "---------\n",
      "Epoch 393/1000, Train Loss: 0.03909, lr: 0.00025\n",
      "Val Loss: 0.04041\n",
      "---------\n",
      "Epoch 394/1000, Train Loss: 0.03908, lr: 0.00025\n",
      "Val Loss: 0.04041\n",
      "---------\n",
      "Epoch 395/1000, Train Loss: 0.03908, lr: 0.00025\n",
      "Val Loss: 0.04040\n",
      "---------\n",
      "Epoch 396/1000, Train Loss: 0.03908, lr: 0.00025\n",
      "Val Loss: 0.04040\n",
      "---------\n",
      "Epoch 397/1000, Train Loss: 0.03907, lr: 0.00025\n",
      "Val Loss: 0.04040\n",
      "---------\n",
      "Epoch 398/1000, Train Loss: 0.03907, lr: 0.00025\n",
      "Val Loss: 0.04039\n",
      "---------\n",
      "Epoch 399/1000, Train Loss: 0.03906, lr: 0.00025\n",
      "Val Loss: 0.04039\n",
      "---------\n",
      "Epoch 400/1000, Train Loss: 0.03906, lr: 0.00025\n",
      "Val Loss: 0.04039\n",
      "---------\n",
      "Epoch 401/1000, Train Loss: 0.03906, lr: 0.00025\n",
      "Val Loss: 0.04038\n",
      "---------\n",
      "Epoch 402/1000, Train Loss: 0.03905, lr: 0.00025\n",
      "Val Loss: 0.04038\n",
      "---------\n",
      "Epoch 403/1000, Train Loss: 0.03905, lr: 0.00025\n",
      "Val Loss: 0.04038\n",
      "---------\n",
      "Epoch 404/1000, Train Loss: 0.03905, lr: 0.00025\n",
      "Val Loss: 0.04037\n",
      "---------\n",
      "Epoch 405/1000, Train Loss: 0.03904, lr: 0.00025\n",
      "Val Loss: 0.04037\n",
      "---------\n",
      "Epoch 406/1000, Train Loss: 0.03904, lr: 0.00025\n",
      "Val Loss: 0.04037\n",
      "---------\n",
      "Epoch 407/1000, Train Loss: 0.03904, lr: 0.00025\n",
      "Val Loss: 0.04037\n",
      "---------\n",
      "Epoch 408/1000, Train Loss: 0.03903, lr: 0.00025\n",
      "Val Loss: 0.04037\n",
      "---------\n",
      "Epoch 409/1000, Train Loss: 0.03903, lr: 0.00025\n",
      "Val Loss: 0.04037\n",
      "---------\n",
      "Epoch 410/1000, Train Loss: 0.03903, lr: 0.00025\n",
      "Val Loss: 0.04036\n",
      "---------\n",
      "Epoch 411/1000, Train Loss: 0.03902, lr: 0.00025\n",
      "Val Loss: 0.04036\n",
      "---------\n",
      "Epoch 412/1000, Train Loss: 0.03902, lr: 0.00025\n",
      "Val Loss: 0.04036\n",
      "---------\n",
      "Epoch 413/1000, Train Loss: 0.03902, lr: 0.00025\n",
      "Val Loss: 0.04036\n",
      "---------\n",
      "Epoch 414/1000, Train Loss: 0.03901, lr: 0.00025\n",
      "Val Loss: 0.04035\n",
      "---------\n",
      "Epoch 415/1000, Train Loss: 0.03901, lr: 0.00025\n",
      "Val Loss: 0.04035\n",
      "---------\n",
      "Epoch 416/1000, Train Loss: 0.03901, lr: 0.00025\n",
      "Val Loss: 0.04035\n",
      "---------\n",
      "Epoch 417/1000, Train Loss: 0.03901, lr: 0.00025\n",
      "Val Loss: 0.04035\n",
      "---------\n",
      "Epoch 418/1000, Train Loss: 0.03900, lr: 0.00025\n",
      "Val Loss: 0.04034\n",
      "---------\n",
      "Epoch 419/1000, Train Loss: 0.03900, lr: 0.00025\n",
      "Val Loss: 0.04034\n",
      "---------\n",
      "Epoch 420/1000, Train Loss: 0.03900, lr: 0.00025\n",
      "Val Loss: 0.04034\n",
      "---------\n",
      "Epoch 421/1000, Train Loss: 0.03900, lr: 0.00025\n",
      "Val Loss: 0.04034\n",
      "---------\n",
      "Epoch 422/1000, Train Loss: 0.03899, lr: 0.00025\n",
      "Val Loss: 0.04034\n",
      "---------\n",
      "Epoch 423/1000, Train Loss: 0.03899, lr: 0.00025\n",
      "Val Loss: 0.04034\n",
      "---------\n",
      "Epoch 424/1000, Train Loss: 0.03899, lr: 0.00025\n",
      "Val Loss: 0.04034\n",
      "---------\n",
      "Epoch 425/1000, Train Loss: 0.03898, lr: 0.00025\n",
      "Val Loss: 0.04033\n",
      "---------\n",
      "Epoch 426/1000, Train Loss: 0.03898, lr: 0.00025\n",
      "Val Loss: 0.04033\n",
      "---------\n",
      "Epoch 427/1000, Train Loss: 0.03898, lr: 0.00025\n",
      "Val Loss: 0.04033\n",
      "---------\n",
      "Epoch 428/1000, Train Loss: 0.03898, lr: 0.00025\n",
      "Val Loss: 0.04033\n",
      "---------\n",
      "Epoch 429/1000, Train Loss: 0.03897, lr: 0.00025\n",
      "Val Loss: 0.04032\n",
      "---------\n",
      "Epoch 430/1000, Train Loss: 0.03897, lr: 0.00025\n",
      "Val Loss: 0.04032\n",
      "---------\n",
      "Epoch 431/1000, Train Loss: 0.03897, lr: 0.00025\n",
      "Val Loss: 0.04032\n",
      "---------\n",
      "Epoch 432/1000, Train Loss: 0.03897, lr: 0.00025\n",
      "Val Loss: 0.04032\n",
      "---------\n",
      "Epoch 433/1000, Train Loss: 0.03897, lr: 0.00025\n",
      "Val Loss: 0.04032\n",
      "---------\n",
      "Epoch 434/1000, Train Loss: 0.03896, lr: 0.00025\n",
      "Val Loss: 0.04032\n",
      "---------\n",
      "Epoch 435/1000, Train Loss: 0.03896, lr: 0.00025\n",
      "Val Loss: 0.04032\n",
      "---------\n",
      "Epoch 436/1000, Train Loss: 0.03896, lr: 0.00025\n",
      "Val Loss: 0.04032\n",
      "---------\n",
      "Epoch 437/1000, Train Loss: 0.03896, lr: 0.00025\n",
      "Val Loss: 0.04032\n",
      "---------\n",
      "Epoch 438/1000, Train Loss: 0.03895, lr: 0.00025\n",
      "Val Loss: 0.04031\n",
      "---------\n",
      "Epoch 439/1000, Train Loss: 0.03895, lr: 0.00025\n",
      "Val Loss: 0.04031\n",
      "---------\n",
      "Epoch 440/1000, Train Loss: 0.03895, lr: 0.00025\n",
      "Val Loss: 0.04031\n",
      "---------\n",
      "Epoch 441/1000, Train Loss: 0.03895, lr: 0.00025\n",
      "Val Loss: 0.04031\n",
      "---------\n",
      "Epoch 442/1000, Train Loss: 0.03895, lr: 0.00025\n",
      "Val Loss: 0.04031\n",
      "---------\n",
      "Epoch 443/1000, Train Loss: 0.03894, lr: 0.00025\n",
      "Val Loss: 0.04031\n",
      "---------\n",
      "Epoch 444/1000, Train Loss: 0.03894, lr: 0.00025\n",
      "Val Loss: 0.04031\n",
      "---------\n",
      "Epoch 445/1000, Train Loss: 0.03894, lr: 0.00025\n",
      "Val Loss: 0.04031\n",
      "---------\n",
      "Epoch 446/1000, Train Loss: 0.03894, lr: 0.00025\n",
      "Val Loss: 0.04030\n",
      "---------\n",
      "Epoch 447/1000, Train Loss: 0.03893, lr: 0.00025\n",
      "Val Loss: 0.04030\n",
      "---------\n",
      "Epoch 448/1000, Train Loss: 0.03893, lr: 0.00025\n",
      "Val Loss: 0.04030\n",
      "---------\n",
      "Epoch 449/1000, Train Loss: 0.03893, lr: 0.00025\n",
      "Val Loss: 0.04030\n",
      "---------\n",
      "Epoch 450/1000, Train Loss: 0.03893, lr: 0.00025\n",
      "Val Loss: 0.04030\n",
      "---------\n",
      "Epoch 451/1000, Train Loss: 0.03893, lr: 0.00025\n",
      "Val Loss: 0.04029\n",
      "---------\n",
      "Epoch 452/1000, Train Loss: 0.03892, lr: 0.00025\n",
      "Val Loss: 0.04029\n",
      "---------\n",
      "Epoch 453/1000, Train Loss: 0.03892, lr: 0.00025\n",
      "Val Loss: 0.04029\n",
      "---------\n",
      "Epoch 454/1000, Train Loss: 0.03892, lr: 0.00025\n",
      "Val Loss: 0.04029\n",
      "---------\n",
      "Epoch 455/1000, Train Loss: 0.03892, lr: 0.00025\n",
      "Val Loss: 0.04029\n",
      "---------\n",
      "Epoch 456/1000, Train Loss: 0.03892, lr: 0.00025\n",
      "Val Loss: 0.04029\n",
      "---------\n",
      "Epoch 457/1000, Train Loss: 0.03891, lr: 0.00025\n",
      "Val Loss: 0.04029\n",
      "---------\n",
      "Epoch 458/1000, Train Loss: 0.03891, lr: 0.00025\n",
      "Val Loss: 0.04029\n",
      "---------\n",
      "Epoch 459/1000, Train Loss: 0.03891, lr: 0.00025\n",
      "Val Loss: 0.04029\n",
      "---------\n",
      "Epoch 460/1000, Train Loss: 0.03891, lr: 0.00025\n",
      "Val Loss: 0.04029\n",
      "---------\n",
      "Epoch 461/1000, Train Loss: 0.03891, lr: 0.00025\n",
      "Val Loss: 0.04029\n",
      "---------\n",
      "Epoch 462/1000, Train Loss: 0.03890, lr: 0.00025\n",
      "Val Loss: 0.04029\n",
      "---------\n",
      "Epoch 463/1000, Train Loss: 0.03890, lr: 0.00025\n",
      "Val Loss: 0.04029\n",
      "---------\n",
      "Epoch 464/1000, Train Loss: 0.03890, lr: 0.00025\n",
      "Val Loss: 0.04028\n",
      "---------\n",
      "Epoch 465/1000, Train Loss: 0.03890, lr: 0.00025\n",
      "Val Loss: 0.04028\n",
      "---------\n",
      "Epoch 466/1000, Train Loss: 0.03890, lr: 0.00025\n",
      "Val Loss: 0.04028\n",
      "---------\n",
      "Epoch 467/1000, Train Loss: 0.03889, lr: 0.00025\n",
      "Val Loss: 0.04028\n",
      "---------\n",
      "Epoch 468/1000, Train Loss: 0.03889, lr: 0.00025\n",
      "Val Loss: 0.04028\n",
      "---------\n",
      "Epoch 469/1000, Train Loss: 0.03889, lr: 0.00025\n",
      "Val Loss: 0.04028\n",
      "---------\n",
      "Epoch 470/1000, Train Loss: 0.03889, lr: 0.00025\n",
      "Val Loss: 0.04028\n",
      "---------\n",
      "Epoch 471/1000, Train Loss: 0.03889, lr: 0.00025\n",
      "Val Loss: 0.04027\n",
      "---------\n",
      "Epoch 472/1000, Train Loss: 0.03888, lr: 0.00025\n",
      "Val Loss: 0.04027\n",
      "---------\n",
      "Epoch 473/1000, Train Loss: 0.03888, lr: 0.00025\n",
      "Val Loss: 0.04027\n",
      "---------\n",
      "Epoch 474/1000, Train Loss: 0.03888, lr: 0.00025\n",
      "Val Loss: 0.04027\n",
      "---------\n",
      "Epoch 475/1000, Train Loss: 0.03888, lr: 0.00025\n",
      "Val Loss: 0.04027\n",
      "---------\n",
      "Epoch 476/1000, Train Loss: 0.03888, lr: 0.00025\n",
      "Val Loss: 0.04027\n",
      "---------\n",
      "Epoch 477/1000, Train Loss: 0.03888, lr: 0.00025\n",
      "Val Loss: 0.04027\n",
      "---------\n",
      "Epoch 478/1000, Train Loss: 0.03887, lr: 0.00025\n",
      "Val Loss: 0.04027\n",
      "---------\n",
      "Epoch 479/1000, Train Loss: 0.03887, lr: 0.00025\n",
      "Val Loss: 0.04027\n",
      "---------\n",
      "Epoch 480/1000, Train Loss: 0.03887, lr: 0.00025\n",
      "Val Loss: 0.04027\n",
      "---------\n",
      "Epoch 481/1000, Train Loss: 0.03887, lr: 0.00025\n",
      "Val Loss: 0.04027\n",
      "---------\n",
      "Epoch 482/1000, Train Loss: 0.03887, lr: 0.00025\n",
      "Val Loss: 0.04026\n",
      "---------\n",
      "Epoch 483/1000, Train Loss: 0.03886, lr: 0.00025\n",
      "Val Loss: 0.04026\n",
      "---------\n",
      "Epoch 484/1000, Train Loss: 0.03886, lr: 0.00025\n",
      "Val Loss: 0.04026\n",
      "---------\n",
      "Epoch 485/1000, Train Loss: 0.03886, lr: 0.00025\n",
      "Val Loss: 0.04026\n",
      "---------\n",
      "Epoch 486/1000, Train Loss: 0.03886, lr: 0.00025\n",
      "Val Loss: 0.04026\n",
      "---------\n",
      "Epoch 487/1000, Train Loss: 0.03886, lr: 0.00025\n",
      "Val Loss: 0.04026\n",
      "---------\n",
      "Epoch 488/1000, Train Loss: 0.03885, lr: 0.00025\n",
      "Val Loss: 0.04026\n",
      "---------\n",
      "Epoch 489/1000, Train Loss: 0.03885, lr: 0.00025\n",
      "Val Loss: 0.04026\n",
      "---------\n",
      "Epoch 490/1000, Train Loss: 0.03885, lr: 0.00025\n",
      "Val Loss: 0.04026\n",
      "---------\n",
      "Epoch 491/1000, Train Loss: 0.03885, lr: 0.00025\n",
      "Val Loss: 0.04026\n",
      "---------\n",
      "Epoch 492/1000, Train Loss: 0.03885, lr: 0.00025\n",
      "Val Loss: 0.04025\n",
      "---------\n",
      "Epoch 493/1000, Train Loss: 0.03885, lr: 0.00025\n",
      "Val Loss: 0.04025\n",
      "---------\n",
      "Epoch 494/1000, Train Loss: 0.03884, lr: 0.00025\n",
      "Val Loss: 0.04025\n",
      "---------\n",
      "Epoch 495/1000, Train Loss: 0.03884, lr: 0.00025\n",
      "Val Loss: 0.04025\n",
      "---------\n",
      "Epoch 496/1000, Train Loss: 0.03884, lr: 0.00025\n",
      "Val Loss: 0.04025\n",
      "---------\n",
      "Epoch 497/1000, Train Loss: 0.03884, lr: 0.00025\n",
      "Val Loss: 0.04025\n",
      "---------\n",
      "Epoch 498/1000, Train Loss: 0.03884, lr: 0.00025\n",
      "Val Loss: 0.04025\n",
      "---------\n",
      "Epoch 499/1000, Train Loss: 0.03884, lr: 0.00025\n",
      "Val Loss: 0.04025\n",
      "---------\n",
      "Epoch 500/1000, Train Loss: 0.03883, lr: 0.00025\n",
      "Val Loss: 0.04025\n",
      "---------\n",
      "Epoch 501/1000, Train Loss: 0.03883, lr: 0.00025\n",
      "Val Loss: 0.04025\n",
      "---------\n",
      "Epoch 502/1000, Train Loss: 0.03883, lr: 0.00025\n",
      "Val Loss: 0.04025\n",
      "---------\n",
      "Epoch 503/1000, Train Loss: 0.03883, lr: 0.00025\n",
      "Val Loss: 0.04025\n",
      "---------\n",
      "Epoch 504/1000, Train Loss: 0.03883, lr: 0.00025\n",
      "Val Loss: 0.04025\n",
      "---------\n",
      "Epoch 505/1000, Train Loss: 0.03882, lr: 0.00025\n",
      "Val Loss: 0.04025\n",
      "---------\n",
      "Epoch 506/1000, Train Loss: 0.03882, lr: 0.00025\n",
      "Val Loss: 0.04025\n",
      "---------\n",
      "Epoch 507/1000, Train Loss: 0.03882, lr: 0.00025\n",
      "Val Loss: 0.04024\n",
      "---------\n",
      "Epoch 508/1000, Train Loss: 0.03882, lr: 0.00025\n",
      "Val Loss: 0.04024\n",
      "---------\n",
      "Epoch 509/1000, Train Loss: 0.03882, lr: 0.00025\n",
      "Val Loss: 0.04024\n",
      "---------\n",
      "Epoch 510/1000, Train Loss: 0.03882, lr: 0.00025\n",
      "Val Loss: 0.04024\n",
      "---------\n",
      "Epoch 511/1000, Train Loss: 0.03881, lr: 0.00025\n",
      "Val Loss: 0.04024\n",
      "---------\n",
      "Epoch 512/1000, Train Loss: 0.03881, lr: 0.00025\n",
      "Val Loss: 0.04024\n",
      "---------\n",
      "Epoch 513/1000, Train Loss: 0.03881, lr: 0.00025\n",
      "Val Loss: 0.04024\n",
      "---------\n",
      "Epoch 514/1000, Train Loss: 0.03881, lr: 0.00025\n",
      "Val Loss: 0.04024\n",
      "---------\n",
      "Epoch 515/1000, Train Loss: 0.03881, lr: 0.00025\n",
      "Val Loss: 0.04023\n",
      "---------\n",
      "Epoch 516/1000, Train Loss: 0.03881, lr: 0.00025\n",
      "Val Loss: 0.04023\n",
      "---------\n",
      "Epoch 517/1000, Train Loss: 0.03880, lr: 0.00025\n",
      "Val Loss: 0.04023\n",
      "---------\n",
      "Epoch 518/1000, Train Loss: 0.03880, lr: 0.00025\n",
      "Val Loss: 0.04023\n",
      "---------\n",
      "Epoch 519/1000, Train Loss: 0.03880, lr: 0.00025\n",
      "Val Loss: 0.04023\n",
      "---------\n",
      "Epoch 520/1000, Train Loss: 0.03880, lr: 0.00025\n",
      "Val Loss: 0.04023\n",
      "---------\n",
      "Epoch 521/1000, Train Loss: 0.03880, lr: 0.00025\n",
      "Val Loss: 0.04023\n",
      "---------\n",
      "Epoch 522/1000, Train Loss: 0.03880, lr: 0.00025\n",
      "Val Loss: 0.04022\n",
      "---------\n",
      "Epoch 523/1000, Train Loss: 0.03879, lr: 0.00025\n",
      "Val Loss: 0.04023\n",
      "---------\n",
      "Epoch 524/1000, Train Loss: 0.03879, lr: 0.00025\n",
      "Val Loss: 0.04022\n",
      "---------\n",
      "Epoch 525/1000, Train Loss: 0.03879, lr: 0.00025\n",
      "Val Loss: 0.04022\n",
      "---------\n",
      "Epoch 526/1000, Train Loss: 0.03879, lr: 0.00025\n",
      "Val Loss: 0.04022\n",
      "---------\n",
      "Epoch 527/1000, Train Loss: 0.03879, lr: 0.00025\n",
      "Val Loss: 0.04022\n",
      "---------\n",
      "Epoch 528/1000, Train Loss: 0.03879, lr: 0.00025\n",
      "Val Loss: 0.04022\n",
      "---------\n",
      "Epoch 529/1000, Train Loss: 0.03878, lr: 0.00025\n",
      "Val Loss: 0.04022\n",
      "---------\n",
      "Epoch 530/1000, Train Loss: 0.03878, lr: 0.00025\n",
      "Val Loss: 0.04022\n",
      "---------\n",
      "Epoch 531/1000, Train Loss: 0.03878, lr: 0.00025\n",
      "Val Loss: 0.04022\n",
      "---------\n",
      "Epoch 532/1000, Train Loss: 0.03878, lr: 0.00025\n",
      "Val Loss: 0.04022\n",
      "---------\n",
      "Epoch 533/1000, Train Loss: 0.03878, lr: 0.00025\n",
      "Val Loss: 0.04022\n",
      "---------\n",
      "Epoch 534/1000, Train Loss: 0.03878, lr: 0.00025\n",
      "Val Loss: 0.04022\n",
      "---------\n",
      "Epoch 535/1000, Train Loss: 0.03878, lr: 0.00025\n",
      "Val Loss: 0.04022\n",
      "---------\n",
      "Epoch 536/1000, Train Loss: 0.03877, lr: 0.00025\n",
      "Val Loss: 0.04022\n",
      "---------\n",
      "Epoch 537/1000, Train Loss: 0.03877, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 538/1000, Train Loss: 0.03877, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 539/1000, Train Loss: 0.03877, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 540/1000, Train Loss: 0.03877, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 541/1000, Train Loss: 0.03877, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 542/1000, Train Loss: 0.03876, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 543/1000, Train Loss: 0.03876, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 544/1000, Train Loss: 0.03876, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 545/1000, Train Loss: 0.03876, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 546/1000, Train Loss: 0.03876, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 547/1000, Train Loss: 0.03876, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 548/1000, Train Loss: 0.03875, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 549/1000, Train Loss: 0.03875, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 550/1000, Train Loss: 0.03875, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 551/1000, Train Loss: 0.03875, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 552/1000, Train Loss: 0.03875, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 553/1000, Train Loss: 0.03875, lr: 0.00025\n",
      "Val Loss: 0.04020\n",
      "---------\n",
      "Epoch 554/1000, Train Loss: 0.03875, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 555/1000, Train Loss: 0.03874, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 556/1000, Train Loss: 0.03874, lr: 0.00025\n",
      "Val Loss: 0.04021\n",
      "---------\n",
      "Epoch 557/1000, Train Loss: 0.03874, lr: 0.00025\n",
      "Val Loss: 0.04020\n",
      "---------\n",
      "Epoch 558/1000, Train Loss: 0.03874, lr: 0.00025\n",
      "Val Loss: 0.04020\n",
      "---------\n",
      "Epoch 559/1000, Train Loss: 0.03874, lr: 0.00025\n",
      "Val Loss: 0.04020\n",
      "---------\n",
      "Epoch 560/1000, Train Loss: 0.03874, lr: 0.00025\n",
      "Val Loss: 0.04020\n",
      "---------\n",
      "Epoch 561/1000, Train Loss: 0.03874, lr: 0.00025\n",
      "Val Loss: 0.04020\n",
      "---------\n",
      "Epoch 562/1000, Train Loss: 0.03873, lr: 0.00025\n",
      "Val Loss: 0.04020\n",
      "---------\n",
      "Epoch 563/1000, Train Loss: 0.03873, lr: 0.00025\n",
      "Val Loss: 0.04020\n",
      "---------\n",
      "Epoch 564/1000, Train Loss: 0.03873, lr: 0.00025\n",
      "Val Loss: 0.04020\n",
      "---------\n",
      "Epoch 565/1000, Train Loss: 0.03873, lr: 0.00025\n",
      "Val Loss: 0.04020\n",
      "---------\n",
      "Epoch 566/1000, Train Loss: 0.03873, lr: 0.00025\n",
      "Val Loss: 0.04020\n",
      "---------\n",
      "Epoch 567/1000, Train Loss: 0.03873, lr: 0.00025\n",
      "Val Loss: 0.04020\n",
      "---------\n",
      "Epoch 568/1000, Train Loss: 0.03873, lr: 0.00025\n",
      "Val Loss: 0.04020\n",
      "---------\n",
      "Epoch 569/1000, Train Loss: 0.03873, lr: 0.00025\n",
      "Val Loss: 0.04020\n",
      "---------\n",
      "Epoch 570/1000, Train Loss: 0.03872, lr: 0.00025\n",
      "Val Loss: 0.04019\n",
      "---------\n",
      "Epoch 571/1000, Train Loss: 0.03872, lr: 0.00025\n",
      "Val Loss: 0.04019\n",
      "---------\n",
      "Epoch 572/1000, Train Loss: 0.03872, lr: 0.00025\n",
      "Val Loss: 0.04019\n",
      "---------\n",
      "Epoch 573/1000, Train Loss: 0.03872, lr: 0.00025\n",
      "Val Loss: 0.04019\n",
      "---------\n",
      "Epoch 574/1000, Train Loss: 0.03872, lr: 0.00025\n",
      "Val Loss: 0.04019\n",
      "---------\n",
      "Epoch 575/1000, Train Loss: 0.03872, lr: 0.00025\n",
      "Val Loss: 0.04019\n",
      "---------\n",
      "Epoch 576/1000, Train Loss: 0.03872, lr: 0.00025\n",
      "Val Loss: 0.04019\n",
      "---------\n",
      "Epoch 577/1000, Train Loss: 0.03871, lr: 0.00025\n",
      "Val Loss: 0.04019\n",
      "---------\n",
      "Epoch 578/1000, Train Loss: 0.03871, lr: 0.00025\n",
      "Val Loss: 0.04019\n",
      "---------\n",
      "Epoch 579/1000, Train Loss: 0.03871, lr: 0.00025\n",
      "Val Loss: 0.04019\n",
      "---------\n",
      "Epoch 580/1000, Train Loss: 0.03871, lr: 0.00025\n",
      "Val Loss: 0.04019\n",
      "---------\n",
      "Epoch 581/1000, Train Loss: 0.03871, lr: 0.00025\n",
      "Val Loss: 0.04018\n",
      "---------\n",
      "Epoch 582/1000, Train Loss: 0.03871, lr: 0.00025\n",
      "Val Loss: 0.04018\n",
      "---------\n",
      "Epoch 583/1000, Train Loss: 0.03871, lr: 0.00025\n",
      "Val Loss: 0.04018\n",
      "---------\n",
      "Epoch 584/1000, Train Loss: 0.03870, lr: 0.00025\n",
      "Val Loss: 0.04018\n",
      "---------\n",
      "Epoch 585/1000, Train Loss: 0.03870, lr: 0.00025\n",
      "Val Loss: 0.04018\n",
      "---------\n",
      "Epoch 586/1000, Train Loss: 0.03870, lr: 0.00025\n",
      "Val Loss: 0.04018\n",
      "---------\n",
      "Epoch 587/1000, Train Loss: 0.03870, lr: 0.00025\n",
      "Val Loss: 0.04018\n",
      "---------\n",
      "Epoch 588/1000, Train Loss: 0.03870, lr: 0.00025\n",
      "Val Loss: 0.04018\n",
      "---------\n",
      "Epoch 589/1000, Train Loss: 0.03870, lr: 0.00025\n",
      "Val Loss: 0.04018\n",
      "---------\n",
      "Epoch 590/1000, Train Loss: 0.03870, lr: 0.00025\n",
      "Val Loss: 0.04018\n",
      "---------\n",
      "Epoch 591/1000, Train Loss: 0.03869, lr: 0.00025\n",
      "Val Loss: 0.04018\n",
      "---------\n",
      "Epoch 592/1000, Train Loss: 0.03869, lr: 0.00025\n",
      "Val Loss: 0.04018\n",
      "---------\n",
      "Epoch 593/1000, Train Loss: 0.03869, lr: 0.00025\n",
      "Val Loss: 0.04018\n",
      "---------\n",
      "Epoch 594/1000, Train Loss: 0.03869, lr: 0.00025\n",
      "Val Loss: 0.04018\n",
      "---------\n",
      "Epoch 595/1000, Train Loss: 0.03869, lr: 0.00025\n",
      "Val Loss: 0.04018\n",
      "---------\n",
      "Epoch 596/1000, Train Loss: 0.03869, lr: 0.00025\n",
      "Val Loss: 0.04018\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 597/1000, Train Loss: 0.03852, lr: 0.000125\n",
      "Val Loss: 0.03980\n",
      "---------\n",
      "Epoch 598/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.03980\n",
      "---------\n",
      "Epoch 599/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.03980\n",
      "---------\n",
      "Epoch 600/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.03980\n",
      "---------\n",
      "Epoch 601/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.03980\n",
      "---------\n",
      "Epoch 602/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.03980\n",
      "---------\n",
      "Epoch 603/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.03980\n",
      "---------\n",
      "Epoch 604/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.03980\n",
      "---------\n",
      "Epoch 605/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.03980\n",
      "---------\n",
      "Epoch 606/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.03980\n",
      "---------\n",
      "Epoch 607/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.03980\n",
      "---------\n",
      "Epoch 608/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.03980\n",
      "---------\n",
      "Epoch 609/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.03980\n",
      "---------\n",
      "Epoch 610/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.03980\n",
      "---------\n",
      "Epoch 611/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.03980\n",
      "---------\n",
      "Epoch 612/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.03980\n",
      "---------\n",
      "Epoch 613/1000, Train Loss: 0.03849, lr: 0.000125\n",
      "Val Loss: 0.03980\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 614/1000, Train Loss: 0.03841, lr: 6.25e-05\n",
      "Val Loss: 0.03971\n",
      "---------\n",
      "Epoch 615/1000, Train Loss: 0.03840, lr: 6.25e-05\n",
      "Val Loss: 0.03971\n",
      "---------\n",
      "Epoch 616/1000, Train Loss: 0.03839, lr: 6.25e-05\n",
      "Val Loss: 0.03971\n",
      "---------\n",
      "Epoch 617/1000, Train Loss: 0.03839, lr: 6.25e-05\n",
      "Val Loss: 0.03971\n",
      "---------\n",
      "Epoch 618/1000, Train Loss: 0.03839, lr: 6.25e-05\n",
      "Val Loss: 0.03971\n",
      "---------\n",
      "Epoch 619/1000, Train Loss: 0.03839, lr: 6.25e-05\n",
      "Val Loss: 0.03971\n",
      "---------\n",
      "Epoch 620/1000, Train Loss: 0.03839, lr: 6.25e-05\n",
      "Val Loss: 0.03971\n",
      "---------\n",
      "Epoch 621/1000, Train Loss: 0.03839, lr: 6.25e-05\n",
      "Val Loss: 0.03971\n",
      "---------\n",
      "Epoch 622/1000, Train Loss: 0.03839, lr: 6.25e-05\n",
      "Val Loss: 0.03971\n",
      "---------\n",
      "Epoch 623/1000, Train Loss: 0.03839, lr: 6.25e-05\n",
      "Val Loss: 0.03971\n",
      "---------\n",
      "Epoch 624/1000, Train Loss: 0.03839, lr: 6.25e-05\n",
      "Val Loss: 0.03971\n",
      "---------\n",
      "Epoch 625/1000, Train Loss: 0.03839, lr: 6.25e-05\n",
      "Val Loss: 0.03971\n",
      "---------\n",
      "Epoch 626/1000, Train Loss: 0.03839, lr: 6.25e-05\n",
      "Val Loss: 0.03971\n",
      "---------\n",
      "Epoch 627/1000, Train Loss: 0.03838, lr: 6.25e-05\n",
      "Val Loss: 0.03971\n",
      "---------\n",
      "Epoch 628/1000, Train Loss: 0.03838, lr: 6.25e-05\n",
      "Val Loss: 0.03971\n",
      "---------\n",
      "Epoch 629/1000, Train Loss: 0.03838, lr: 6.25e-05\n",
      "Val Loss: 0.03971\n",
      "---------\n",
      "Epoch 630/1000, Train Loss: 0.03838, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 631/1000, Train Loss: 0.03838, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 632/1000, Train Loss: 0.03838, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 633/1000, Train Loss: 0.03838, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 634/1000, Train Loss: 0.03838, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 635/1000, Train Loss: 0.03838, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 636/1000, Train Loss: 0.03838, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 637/1000, Train Loss: 0.03838, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 638/1000, Train Loss: 0.03838, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 639/1000, Train Loss: 0.03838, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 640/1000, Train Loss: 0.03838, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 641/1000, Train Loss: 0.03838, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 642/1000, Train Loss: 0.03837, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 643/1000, Train Loss: 0.03837, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 644/1000, Train Loss: 0.03837, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 645/1000, Train Loss: 0.03837, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 646/1000, Train Loss: 0.03837, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 647/1000, Train Loss: 0.03837, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 648/1000, Train Loss: 0.03837, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 649/1000, Train Loss: 0.03837, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 650/1000, Train Loss: 0.03837, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 651/1000, Train Loss: 0.03837, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 652/1000, Train Loss: 0.03837, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 653/1000, Train Loss: 0.03837, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 654/1000, Train Loss: 0.03837, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 655/1000, Train Loss: 0.03837, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 656/1000, Train Loss: 0.03837, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 657/1000, Train Loss: 0.03837, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 658/1000, Train Loss: 0.03837, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 659/1000, Train Loss: 0.03837, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 660/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 661/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 662/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 663/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 664/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 665/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 666/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 667/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 668/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 669/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 670/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 671/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 672/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 673/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 674/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 675/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 676/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 677/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 678/1000, Train Loss: 0.03836, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 679/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 680/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 681/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 682/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 683/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 684/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 685/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 686/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 687/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 688/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 689/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 690/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 691/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 692/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 693/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 694/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 695/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 696/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 697/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 698/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 699/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 700/1000, Train Loss: 0.03835, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 701/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 702/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 703/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 704/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 705/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 706/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 707/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 708/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 709/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 710/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 711/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 712/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 713/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 714/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 715/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 716/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 717/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 718/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03969\n",
      "---------\n",
      "Epoch 719/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 720/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 721/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 722/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 723/1000, Train Loss: 0.03834, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 724/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 725/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 726/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 727/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 728/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 729/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 730/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 731/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 732/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 733/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 734/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 735/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 736/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 737/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 738/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 739/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 740/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 741/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 742/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 743/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 744/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 745/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 746/1000, Train Loss: 0.03833, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 747/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 748/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 749/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 750/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 751/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 752/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 753/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 754/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 755/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 756/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 757/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 758/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 759/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 760/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 761/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 762/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 763/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 764/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 765/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 766/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 767/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 768/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03968\n",
      "---------\n",
      "Epoch 769/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 770/1000, Train Loss: 0.03832, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 771/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 772/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 773/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 774/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 775/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 776/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 777/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 778/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 779/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 780/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 781/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 782/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 783/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 784/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 785/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 786/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 787/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 788/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 789/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 790/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 791/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 792/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 793/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 794/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 795/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 796/1000, Train Loss: 0.03831, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 797/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 798/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 799/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 800/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 801/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 802/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 803/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 804/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 805/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 806/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 807/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 808/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 809/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 810/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 811/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 812/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 813/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 814/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 815/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 816/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 817/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 818/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 819/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 820/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 821/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 822/1000, Train Loss: 0.03830, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 823/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 824/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 825/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 826/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 827/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 828/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 829/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 830/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03967\n",
      "---------\n",
      "Epoch 831/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 832/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 833/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 834/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 835/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 836/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 837/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 838/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 839/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 840/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 841/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 842/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 843/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 844/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 845/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 846/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 847/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 848/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 849/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 850/1000, Train Loss: 0.03829, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 851/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 852/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 853/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 854/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 855/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 856/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 857/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 858/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 859/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 860/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 861/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 862/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 863/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 864/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 865/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 866/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 867/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 868/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 869/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 870/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 871/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 872/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 873/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 874/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 875/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 876/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 877/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 878/1000, Train Loss: 0.03828, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 879/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 880/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 881/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 882/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 883/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 884/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 885/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 886/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 887/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 888/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 889/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 890/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 891/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 892/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 893/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 894/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 895/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 896/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 897/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 898/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03966\n",
      "---------\n",
      "Epoch 899/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 900/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 901/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 902/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 903/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 904/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 905/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 906/1000, Train Loss: 0.03827, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 907/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 908/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 909/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 910/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 911/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 912/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 913/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 914/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 915/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 916/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 917/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 918/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 919/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 920/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 921/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 922/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 923/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 924/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 925/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 926/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 927/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 928/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 929/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 930/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 931/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 932/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 933/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 934/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 935/1000, Train Loss: 0.03826, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 936/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 937/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 938/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 939/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 940/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 941/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 942/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 943/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 944/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 945/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 946/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 947/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 948/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 949/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 950/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 951/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 952/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 953/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 954/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 955/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 956/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 957/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 958/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 959/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 960/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 961/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 962/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 963/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 964/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 965/1000, Train Loss: 0.03825, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 966/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 967/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 968/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 969/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 970/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 971/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 972/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 973/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 974/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 975/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 976/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 977/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 978/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 979/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03965\n",
      "---------\n",
      "Epoch 980/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 981/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 982/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 983/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 984/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 985/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 986/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 987/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 988/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 989/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 990/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 991/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 992/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 993/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 994/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 995/1000, Train Loss: 0.03824, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 996/1000, Train Loss: 0.03823, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 997/1000, Train Loss: 0.03823, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 998/1000, Train Loss: 0.03823, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 999/1000, Train Loss: 0.03823, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 1000/1000, Train Loss: 0.03823, lr: 6.25e-05\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "4455.614479064941 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1461, 32, 48, 6, 1), y_hat_i: (5, 32, 48, 6, 1), y_i: (5, 32, 48, 6, 1), batch.x: torch.Size([160, 48, 1, 6]), y: (1461, 32, 48, 6, 1)\n",
      "RMSE for t2m: 3.6038233826027106; MAE for t2m: 2.627954827249992;\n",
      "RMSE for sp: 1.8404689000538286; MAE for sp: 1.4035424776465297;\n",
      "RMSE for tcc: 0.29559322205848704; MAE for tcc: 0.20336912750235264;\n",
      "RMSE for u10: 1.393467301072252; MAE for u10: 1.0406954989727522;\n",
      "RMSE for v10: 1.3722929526884013; MAE for v10: 1.0224809235230767;\n",
      "RMSE for tp: 0.2942966976573223; MAE for tp: 0.07883280907120456;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 1, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1461, 32, 48, 6, 1), y_hat_i: (5, 32, 48, 6, 1), y_i: (5, 32, 48, 6, 1), batch.x: torch.Size([160, 48, 1, 6]), y: (1461, 32, 48, 6, 1)\n",
      "RMSE for t2m: 3.6038233826027106; MAE for t2m: 2.627954827249992;\n",
      "RMSE for sp: 1.8404689000538286; MAE for sp: 1.4035424776465297;\n",
      "RMSE for tcc: 0.29549905507714375; MAE for tcc: 0.2029079006710445;\n",
      "RMSE for u10: 1.393467301072252; MAE for u10: 1.0406954989727522;\n",
      "RMSE for v10: 1.3722929526884013; MAE for v10: 1.0224809235230767;\n",
      "RMSE for tp: 0.2942966976573223; MAE for tp: 0.07883280907120456;\n",
      "Epoch 1/1000, Train Loss: 0.07701, lr: 0.001--------------------------| 9.1% Complete\n",
      "Val Loss: 0.06799\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.06504, lr: 0.001\n",
      "Val Loss: 0.06144\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.05845, lr: 0.001\n",
      "Val Loss: 0.05875\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.05569, lr: 0.001\n",
      "Val Loss: 0.05369\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.05072, lr: 0.001\n",
      "Val Loss: 0.04969\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.04842, lr: 0.001\n",
      "Val Loss: 0.04733\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.04612, lr: 0.001\n",
      "Val Loss: 0.04598\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.04494, lr: 0.001\n",
      "Val Loss: 0.04485\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.04412, lr: 0.001\n",
      "Val Loss: 0.04420\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.04352, lr: 0.001\n",
      "Val Loss: 0.04373\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.04305, lr: 0.001\n",
      "Val Loss: 0.04296\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.04270, lr: 0.001\n",
      "Val Loss: 0.04269\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.04244, lr: 0.001\n",
      "Val Loss: 0.04254\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.04220, lr: 0.001\n",
      "Val Loss: 0.04253\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.04199, lr: 0.001\n",
      "Val Loss: 0.04245\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.04182, lr: 0.001\n",
      "Val Loss: 0.04233\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.04165, lr: 0.001\n",
      "Val Loss: 0.04220\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.04153, lr: 0.001\n",
      "Val Loss: 0.04208\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.04140, lr: 0.001\n",
      "Val Loss: 0.04190\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.04127, lr: 0.001\n",
      "Val Loss: 0.04180\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.04116, lr: 0.001\n",
      "Val Loss: 0.04169\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.04103, lr: 0.001\n",
      "Val Loss: 0.04162\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.04091, lr: 0.001\n",
      "Val Loss: 0.04148\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.04079, lr: 0.001\n",
      "Val Loss: 0.04136\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.04069, lr: 0.001\n",
      "Val Loss: 0.04127\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.04058, lr: 0.001\n",
      "Val Loss: 0.04117\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.04050, lr: 0.001\n",
      "Val Loss: 0.04112\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.04042, lr: 0.001\n",
      "Val Loss: 0.04100\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.04036, lr: 0.001\n",
      "Val Loss: 0.04091\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.04030, lr: 0.001\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.04023, lr: 0.001\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.04017, lr: 0.001\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.04012, lr: 0.001\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.04005, lr: 0.001\n",
      "Val Loss: 0.04049\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.03998, lr: 0.001\n",
      "Val Loss: 0.04039\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.03993, lr: 0.001\n",
      "Val Loss: 0.04033\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.03987, lr: 0.001\n",
      "Val Loss: 0.04027\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.03982, lr: 0.001\n",
      "Val Loss: 0.04027\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.03979, lr: 0.001\n",
      "Val Loss: 0.04030\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.03973, lr: 0.001\n",
      "Val Loss: 0.04038\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.03968, lr: 0.001\n",
      "Val Loss: 0.04041\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.03960, lr: 0.001\n",
      "Val Loss: 0.04041\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.03955, lr: 0.001\n",
      "Val Loss: 0.04039\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.03949, lr: 0.001\n",
      "Val Loss: 0.04034\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 45/1000, Train Loss: 0.03835, lr: 0.0005\n",
      "Val Loss: 0.03923\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.03822, lr: 0.0005\n",
      "Val Loss: 0.03920\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.03817, lr: 0.0005\n",
      "Val Loss: 0.03919\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.03813, lr: 0.0005\n",
      "Val Loss: 0.03918\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.03809, lr: 0.0005\n",
      "Val Loss: 0.03917\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.03805, lr: 0.0005\n",
      "Val Loss: 0.03917\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.03800, lr: 0.0005\n",
      "Val Loss: 0.03916\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.03796, lr: 0.0005\n",
      "Val Loss: 0.03913\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.03792, lr: 0.0005\n",
      "Val Loss: 0.03912\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.03789, lr: 0.0005\n",
      "Val Loss: 0.03911\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.03785, lr: 0.0005\n",
      "Val Loss: 0.03909\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.03782, lr: 0.0005\n",
      "Val Loss: 0.03908\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.03778, lr: 0.0005\n",
      "Val Loss: 0.03908\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.03774, lr: 0.0005\n",
      "Val Loss: 0.03908\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.03770, lr: 0.0005\n",
      "Val Loss: 0.03908\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.03767, lr: 0.0005\n",
      "Val Loss: 0.03906\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.03763, lr: 0.0005\n",
      "Val Loss: 0.03906\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.03759, lr: 0.0005\n",
      "Val Loss: 0.03904\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.03756, lr: 0.0005\n",
      "Val Loss: 0.03903\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.03752, lr: 0.0005\n",
      "Val Loss: 0.03902\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.03749, lr: 0.0005\n",
      "Val Loss: 0.03900\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.03746, lr: 0.0005\n",
      "Val Loss: 0.03899\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.03743, lr: 0.0005\n",
      "Val Loss: 0.03899\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.03739, lr: 0.0005\n",
      "Val Loss: 0.03898\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.03736, lr: 0.0005\n",
      "Val Loss: 0.03896\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.03733, lr: 0.0005\n",
      "Val Loss: 0.03893\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.03730, lr: 0.0005\n",
      "Val Loss: 0.03893\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.03727, lr: 0.0005\n",
      "Val Loss: 0.03893\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.03724, lr: 0.0005\n",
      "Val Loss: 0.03891\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.03721, lr: 0.0005\n",
      "Val Loss: 0.03891\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.03718, lr: 0.0005\n",
      "Val Loss: 0.03890\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.03715, lr: 0.0005\n",
      "Val Loss: 0.03887\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.03713, lr: 0.0005\n",
      "Val Loss: 0.03887\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.03710, lr: 0.0005\n",
      "Val Loss: 0.03884\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.03707, lr: 0.0005\n",
      "Val Loss: 0.03885\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.03704, lr: 0.0005\n",
      "Val Loss: 0.03883\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.03701, lr: 0.0005\n",
      "Val Loss: 0.03881\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.03698, lr: 0.0005\n",
      "Val Loss: 0.03878\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.03695, lr: 0.0005\n",
      "Val Loss: 0.03876\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.03692, lr: 0.0005\n",
      "Val Loss: 0.03877\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.03690, lr: 0.0005\n",
      "Val Loss: 0.03875\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.03687, lr: 0.0005\n",
      "Val Loss: 0.03872\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.03684, lr: 0.0005\n",
      "Val Loss: 0.03872\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.03682, lr: 0.0005\n",
      "Val Loss: 0.03870\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.03679, lr: 0.0005\n",
      "Val Loss: 0.03868\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.03676, lr: 0.0005\n",
      "Val Loss: 0.03866\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.03673, lr: 0.0005\n",
      "Val Loss: 0.03864\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.03670, lr: 0.0005\n",
      "Val Loss: 0.03861\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.03667, lr: 0.0005\n",
      "Val Loss: 0.03859\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.03665, lr: 0.0005\n",
      "Val Loss: 0.03859\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.03663, lr: 0.0005\n",
      "Val Loss: 0.03857\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.03660, lr: 0.0005\n",
      "Val Loss: 0.03857\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.03657, lr: 0.0005\n",
      "Val Loss: 0.03853\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.03655, lr: 0.0005\n",
      "Val Loss: 0.03849\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.03652, lr: 0.0005\n",
      "Val Loss: 0.03848\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.03649, lr: 0.0005\n",
      "Val Loss: 0.03846\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.03647, lr: 0.0005\n",
      "Val Loss: 0.03843\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.03644, lr: 0.0005\n",
      "Val Loss: 0.03840\n",
      "---------\n",
      "Epoch 103/1000, Train Loss: 0.03641, lr: 0.0005\n",
      "Val Loss: 0.03836\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.03638, lr: 0.0005\n",
      "Val Loss: 0.03835\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.03636, lr: 0.0005\n",
      "Val Loss: 0.03833\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.03633, lr: 0.0005\n",
      "Val Loss: 0.03830\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.03630, lr: 0.0005\n",
      "Val Loss: 0.03826\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.03628, lr: 0.0005\n",
      "Val Loss: 0.03825\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.03625, lr: 0.0005\n",
      "Val Loss: 0.03823\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.03621, lr: 0.0005\n",
      "Val Loss: 0.03823\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.03619, lr: 0.0005\n",
      "Val Loss: 0.03817\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.03615, lr: 0.0005\n",
      "Val Loss: 0.03812\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.03612, lr: 0.0005\n",
      "Val Loss: 0.03805\n",
      "---------\n",
      "Epoch 114/1000, Train Loss: 0.03608, lr: 0.0005\n",
      "Val Loss: 0.03801\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.03605, lr: 0.0005\n",
      "Val Loss: 0.03798\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.03602, lr: 0.0005\n",
      "Val Loss: 0.03795\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.03599, lr: 0.0005\n",
      "Val Loss: 0.03794\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.03596, lr: 0.0005\n",
      "Val Loss: 0.03790\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.03592, lr: 0.0005\n",
      "Val Loss: 0.03787\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.03590, lr: 0.0005\n",
      "Val Loss: 0.03783\n",
      "---------\n",
      "Epoch 121/1000, Train Loss: 0.03587, lr: 0.0005\n",
      "Val Loss: 0.03782\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.03585, lr: 0.0005\n",
      "Val Loss: 0.03780\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.03582, lr: 0.0005\n",
      "Val Loss: 0.03777\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.03579, lr: 0.0005\n",
      "Val Loss: 0.03773\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.03576, lr: 0.0005\n",
      "Val Loss: 0.03771\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.03574, lr: 0.0005\n",
      "Val Loss: 0.03769\n",
      "---------\n",
      "Epoch 127/1000, Train Loss: 0.03572, lr: 0.0005\n",
      "Val Loss: 0.03768\n",
      "---------\n",
      "Epoch 128/1000, Train Loss: 0.03569, lr: 0.0005\n",
      "Val Loss: 0.03765\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.03566, lr: 0.0005\n",
      "Val Loss: 0.03765\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.03564, lr: 0.0005\n",
      "Val Loss: 0.03761\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.03561, lr: 0.0005\n",
      "Val Loss: 0.03759\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.03559, lr: 0.0005\n",
      "Val Loss: 0.03758\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.03557, lr: 0.0005\n",
      "Val Loss: 0.03756\n",
      "---------\n",
      "Epoch 134/1000, Train Loss: 0.03554, lr: 0.0005\n",
      "Val Loss: 0.03755\n",
      "---------\n",
      "Epoch 135/1000, Train Loss: 0.03552, lr: 0.0005\n",
      "Val Loss: 0.03752\n",
      "---------\n",
      "Epoch 136/1000, Train Loss: 0.03549, lr: 0.0005\n",
      "Val Loss: 0.03750\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.03548, lr: 0.0005\n",
      "Val Loss: 0.03748\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.03545, lr: 0.0005\n",
      "Val Loss: 0.03748\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.03543, lr: 0.0005\n",
      "Val Loss: 0.03745\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.03541, lr: 0.0005\n",
      "Val Loss: 0.03745\n",
      "---------\n",
      "Epoch 141/1000, Train Loss: 0.03539, lr: 0.0005\n",
      "Val Loss: 0.03742\n",
      "---------\n",
      "Epoch 142/1000, Train Loss: 0.03537, lr: 0.0005\n",
      "Val Loss: 0.03743\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.03535, lr: 0.0005\n",
      "Val Loss: 0.03741\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.03533, lr: 0.0005\n",
      "Val Loss: 0.03741\n",
      "---------\n",
      "Epoch 145/1000, Train Loss: 0.03531, lr: 0.0005\n",
      "Val Loss: 0.03739\n",
      "---------\n",
      "Epoch 146/1000, Train Loss: 0.03529, lr: 0.0005\n",
      "Val Loss: 0.03737\n",
      "---------\n",
      "Epoch 147/1000, Train Loss: 0.03527, lr: 0.0005\n",
      "Val Loss: 0.03741\n",
      "---------\n",
      "Epoch 148/1000, Train Loss: 0.03525, lr: 0.0005\n",
      "Val Loss: 0.03741\n",
      "---------\n",
      "Epoch 149/1000, Train Loss: 0.03523, lr: 0.0005\n",
      "Val Loss: 0.03738\n",
      "---------\n",
      "Epoch 150/1000, Train Loss: 0.03521, lr: 0.0005\n",
      "Val Loss: 0.03740\n",
      "---------\n",
      "Epoch 151/1000, Train Loss: 0.03520, lr: 0.0005\n",
      "Val Loss: 0.03741\n",
      "---------\n",
      "Epoch 152/1000, Train Loss: 0.03518, lr: 0.0005\n",
      "Val Loss: 0.03738\n",
      "---------\n",
      "Epoch 153/1000, Train Loss: 0.03516, lr: 0.0005\n",
      "Val Loss: 0.03739\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 154/1000, Train Loss: 0.03476, lr: 0.00025\n",
      "Val Loss: 0.03708\n",
      "---------\n",
      "Epoch 155/1000, Train Loss: 0.03469, lr: 0.00025\n",
      "Val Loss: 0.03707\n",
      "---------\n",
      "Epoch 156/1000, Train Loss: 0.03467, lr: 0.00025\n",
      "Val Loss: 0.03706\n",
      "---------\n",
      "Epoch 157/1000, Train Loss: 0.03465, lr: 0.00025\n",
      "Val Loss: 0.03706\n",
      "---------\n",
      "Epoch 158/1000, Train Loss: 0.03463, lr: 0.00025\n",
      "Val Loss: 0.03705\n",
      "---------\n",
      "Epoch 159/1000, Train Loss: 0.03461, lr: 0.00025\n",
      "Val Loss: 0.03705\n",
      "---------\n",
      "Epoch 160/1000, Train Loss: 0.03460, lr: 0.00025\n",
      "Val Loss: 0.03705\n",
      "---------\n",
      "Epoch 161/1000, Train Loss: 0.03458, lr: 0.00025\n",
      "Val Loss: 0.03703\n",
      "---------\n",
      "Epoch 162/1000, Train Loss: 0.03457, lr: 0.00025\n",
      "Val Loss: 0.03704\n",
      "---------\n",
      "Epoch 163/1000, Train Loss: 0.03456, lr: 0.00025\n",
      "Val Loss: 0.03703\n",
      "---------\n",
      "Epoch 164/1000, Train Loss: 0.03454, lr: 0.00025\n",
      "Val Loss: 0.03702\n",
      "---------\n",
      "Epoch 165/1000, Train Loss: 0.03453, lr: 0.00025\n",
      "Val Loss: 0.03701\n",
      "---------\n",
      "Epoch 166/1000, Train Loss: 0.03452, lr: 0.00025\n",
      "Val Loss: 0.03701\n",
      "---------\n",
      "Epoch 167/1000, Train Loss: 0.03450, lr: 0.00025\n",
      "Val Loss: 0.03700\n",
      "---------\n",
      "Epoch 168/1000, Train Loss: 0.03449, lr: 0.00025\n",
      "Val Loss: 0.03701\n",
      "---------\n",
      "Epoch 169/1000, Train Loss: 0.03448, lr: 0.00025\n",
      "Val Loss: 0.03700\n",
      "---------\n",
      "Epoch 170/1000, Train Loss: 0.03447, lr: 0.00025\n",
      "Val Loss: 0.03700\n",
      "---------\n",
      "Epoch 171/1000, Train Loss: 0.03446, lr: 0.00025\n",
      "Val Loss: 0.03699\n",
      "---------\n",
      "Epoch 172/1000, Train Loss: 0.03445, lr: 0.00025\n",
      "Val Loss: 0.03699\n",
      "---------\n",
      "Epoch 173/1000, Train Loss: 0.03444, lr: 0.00025\n",
      "Val Loss: 0.03697\n",
      "---------\n",
      "Epoch 174/1000, Train Loss: 0.03442, lr: 0.00025\n",
      "Val Loss: 0.03697\n",
      "---------\n",
      "Epoch 175/1000, Train Loss: 0.03441, lr: 0.00025\n",
      "Val Loss: 0.03698\n",
      "---------\n",
      "Epoch 176/1000, Train Loss: 0.03440, lr: 0.00025\n",
      "Val Loss: 0.03698\n",
      "---------\n",
      "Epoch 177/1000, Train Loss: 0.03439, lr: 0.00025\n",
      "Val Loss: 0.03698\n",
      "---------\n",
      "Epoch 178/1000, Train Loss: 0.03438, lr: 0.00025\n",
      "Val Loss: 0.03697\n",
      "---------\n",
      "Epoch 179/1000, Train Loss: 0.03437, lr: 0.00025\n",
      "Val Loss: 0.03697\n",
      "---------\n",
      "Epoch 180/1000, Train Loss: 0.03436, lr: 0.00025\n",
      "Val Loss: 0.03698\n",
      "---------\n",
      "Epoch 181/1000, Train Loss: 0.03435, lr: 0.00025\n",
      "Val Loss: 0.03697\n",
      "---------\n",
      "Epoch 182/1000, Train Loss: 0.03434, lr: 0.00025\n",
      "Val Loss: 0.03696\n",
      "---------\n",
      "Epoch 183/1000, Train Loss: 0.03433, lr: 0.00025\n",
      "Val Loss: 0.03696\n",
      "---------\n",
      "Epoch 184/1000, Train Loss: 0.03432, lr: 0.00025\n",
      "Val Loss: 0.03696\n",
      "---------\n",
      "Epoch 185/1000, Train Loss: 0.03431, lr: 0.00025\n",
      "Val Loss: 0.03696\n",
      "---------\n",
      "Epoch 186/1000, Train Loss: 0.03430, lr: 0.00025\n",
      "Val Loss: 0.03695\n",
      "---------\n",
      "Epoch 187/1000, Train Loss: 0.03429, lr: 0.00025\n",
      "Val Loss: 0.03695\n",
      "---------\n",
      "Epoch 188/1000, Train Loss: 0.03428, lr: 0.00025\n",
      "Val Loss: 0.03695\n",
      "---------\n",
      "Epoch 189/1000, Train Loss: 0.03428, lr: 0.00025\n",
      "Val Loss: 0.03695\n",
      "---------\n",
      "Epoch 190/1000, Train Loss: 0.03427, lr: 0.00025\n",
      "Val Loss: 0.03696\n",
      "---------\n",
      "Epoch 191/1000, Train Loss: 0.03426, lr: 0.00025\n",
      "Val Loss: 0.03696\n",
      "---------\n",
      "Epoch 192/1000, Train Loss: 0.03425, lr: 0.00025\n",
      "Val Loss: 0.03695\n",
      "---------\n",
      "Epoch 193/1000, Train Loss: 0.03424, lr: 0.00025\n",
      "Val Loss: 0.03694\n",
      "---------\n",
      "Epoch 194/1000, Train Loss: 0.03423, lr: 0.00025\n",
      "Val Loss: 0.03693\n",
      "---------\n",
      "Epoch 195/1000, Train Loss: 0.03422, lr: 0.00025\n",
      "Val Loss: 0.03694\n",
      "---------\n",
      "Epoch 196/1000, Train Loss: 0.03421, lr: 0.00025\n",
      "Val Loss: 0.03694\n",
      "---------\n",
      "Epoch 197/1000, Train Loss: 0.03420, lr: 0.00025\n",
      "Val Loss: 0.03693\n",
      "---------\n",
      "Epoch 198/1000, Train Loss: 0.03420, lr: 0.00025\n",
      "Val Loss: 0.03693\n",
      "---------\n",
      "Epoch 199/1000, Train Loss: 0.03418, lr: 0.00025\n",
      "Val Loss: 0.03691\n",
      "---------\n",
      "Epoch 200/1000, Train Loss: 0.03418, lr: 0.00025\n",
      "Val Loss: 0.03691\n",
      "---------\n",
      "Epoch 201/1000, Train Loss: 0.03417, lr: 0.00025\n",
      "Val Loss: 0.03691\n",
      "---------\n",
      "Epoch 202/1000, Train Loss: 0.03416, lr: 0.00025\n",
      "Val Loss: 0.03690\n",
      "---------\n",
      "Epoch 203/1000, Train Loss: 0.03415, lr: 0.00025\n",
      "Val Loss: 0.03691\n",
      "---------\n",
      "Epoch 204/1000, Train Loss: 0.03414, lr: 0.00025\n",
      "Val Loss: 0.03692\n",
      "---------\n",
      "Epoch 205/1000, Train Loss: 0.03413, lr: 0.00025\n",
      "Val Loss: 0.03691\n",
      "---------\n",
      "Epoch 206/1000, Train Loss: 0.03413, lr: 0.00025\n",
      "Val Loss: 0.03691\n",
      "---------\n",
      "Epoch 207/1000, Train Loss: 0.03412, lr: 0.00025\n",
      "Val Loss: 0.03690\n",
      "---------\n",
      "Epoch 208/1000, Train Loss: 0.03411, lr: 0.00025\n",
      "Val Loss: 0.03691\n",
      "---------\n",
      "Epoch 209/1000, Train Loss: 0.03410, lr: 0.00025\n",
      "Val Loss: 0.03691\n",
      "---------\n",
      "Epoch 210/1000, Train Loss: 0.03410, lr: 0.00025\n",
      "Val Loss: 0.03691\n",
      "---------\n",
      "Epoch 211/1000, Train Loss: 0.03409, lr: 0.00025\n",
      "Val Loss: 0.03690\n",
      "---------\n",
      "Epoch 212/1000, Train Loss: 0.03408, lr: 0.00025\n",
      "Val Loss: 0.03691\n",
      "---------\n",
      "Epoch 213/1000, Train Loss: 0.03408, lr: 0.00025\n",
      "Val Loss: 0.03692\n",
      "---------\n",
      "Epoch 214/1000, Train Loss: 0.03406, lr: 0.00025\n",
      "Val Loss: 0.03690\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 215/1000, Train Loss: 0.03391, lr: 0.000125\n",
      "Val Loss: 0.03648\n",
      "---------\n",
      "Epoch 216/1000, Train Loss: 0.03389, lr: 0.000125\n",
      "Val Loss: 0.03647\n",
      "---------\n",
      "Epoch 217/1000, Train Loss: 0.03387, lr: 0.000125\n",
      "Val Loss: 0.03647\n",
      "---------\n",
      "Epoch 218/1000, Train Loss: 0.03386, lr: 0.000125\n",
      "Val Loss: 0.03647\n",
      "---------\n",
      "Epoch 219/1000, Train Loss: 0.03386, lr: 0.000125\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 220/1000, Train Loss: 0.03385, lr: 0.000125\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 221/1000, Train Loss: 0.03384, lr: 0.000125\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 222/1000, Train Loss: 0.03383, lr: 0.000125\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 223/1000, Train Loss: 0.03383, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 224/1000, Train Loss: 0.03382, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 225/1000, Train Loss: 0.03382, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 226/1000, Train Loss: 0.03381, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 227/1000, Train Loss: 0.03380, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 228/1000, Train Loss: 0.03380, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 229/1000, Train Loss: 0.03379, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 230/1000, Train Loss: 0.03379, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 231/1000, Train Loss: 0.03378, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 232/1000, Train Loss: 0.03378, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 233/1000, Train Loss: 0.03377, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 234/1000, Train Loss: 0.03376, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 235/1000, Train Loss: 0.03376, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 236/1000, Train Loss: 0.03376, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 237/1000, Train Loss: 0.03375, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 238/1000, Train Loss: 0.03375, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 239/1000, Train Loss: 0.03374, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 240/1000, Train Loss: 0.03373, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 241/1000, Train Loss: 0.03373, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 242/1000, Train Loss: 0.03372, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 243/1000, Train Loss: 0.03372, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 244/1000, Train Loss: 0.03371, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 245/1000, Train Loss: 0.03371, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 246/1000, Train Loss: 0.03370, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 247/1000, Train Loss: 0.03370, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 248/1000, Train Loss: 0.03369, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 249/1000, Train Loss: 0.03369, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 250/1000, Train Loss: 0.03369, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 251/1000, Train Loss: 0.03368, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 252/1000, Train Loss: 0.03368, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 253/1000, Train Loss: 0.03368, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 254/1000, Train Loss: 0.03367, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 255/1000, Train Loss: 0.03366, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 256/1000, Train Loss: 0.03366, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 257/1000, Train Loss: 0.03366, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 258/1000, Train Loss: 0.03365, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 259/1000, Train Loss: 0.03365, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 260/1000, Train Loss: 0.03364, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 261/1000, Train Loss: 0.03364, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 262/1000, Train Loss: 0.03363, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 263/1000, Train Loss: 0.03363, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 264/1000, Train Loss: 0.03362, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 265/1000, Train Loss: 0.03362, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 266/1000, Train Loss: 0.03362, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 267/1000, Train Loss: 0.03361, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 268/1000, Train Loss: 0.03361, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 269/1000, Train Loss: 0.03360, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 270/1000, Train Loss: 0.03360, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 271/1000, Train Loss: 0.03359, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 272/1000, Train Loss: 0.03359, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 273/1000, Train Loss: 0.03358, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 274/1000, Train Loss: 0.03358, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 275/1000, Train Loss: 0.03358, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 276/1000, Train Loss: 0.03357, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 277/1000, Train Loss: 0.03357, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 278/1000, Train Loss: 0.03357, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 279/1000, Train Loss: 0.03356, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 280/1000, Train Loss: 0.03356, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 281/1000, Train Loss: 0.03356, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 282/1000, Train Loss: 0.03355, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 283/1000, Train Loss: 0.03355, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 284/1000, Train Loss: 0.03354, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 285/1000, Train Loss: 0.03354, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 286/1000, Train Loss: 0.03354, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 287/1000, Train Loss: 0.03353, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 288/1000, Train Loss: 0.03353, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 289/1000, Train Loss: 0.03352, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 290/1000, Train Loss: 0.03352, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 291/1000, Train Loss: 0.03351, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 292/1000, Train Loss: 0.03351, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 293/1000, Train Loss: 0.03351, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 294/1000, Train Loss: 0.03350, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 295/1000, Train Loss: 0.03350, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 296/1000, Train Loss: 0.03350, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 297/1000, Train Loss: 0.03349, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 298/1000, Train Loss: 0.03349, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 299/1000, Train Loss: 0.03349, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 300/1000, Train Loss: 0.03348, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 301/1000, Train Loss: 0.03348, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 302/1000, Train Loss: 0.03348, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 303/1000, Train Loss: 0.03347, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 304/1000, Train Loss: 0.03347, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 305/1000, Train Loss: 0.03347, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 306/1000, Train Loss: 0.03346, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 307/1000, Train Loss: 0.03346, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 308/1000, Train Loss: 0.03346, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 309/1000, Train Loss: 0.03345, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 310/1000, Train Loss: 0.03345, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 311/1000, Train Loss: 0.03345, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 312/1000, Train Loss: 0.03344, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 313/1000, Train Loss: 0.03344, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 314/1000, Train Loss: 0.03344, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 315/1000, Train Loss: 0.03343, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 316/1000, Train Loss: 0.03343, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 317/1000, Train Loss: 0.03343, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 318/1000, Train Loss: 0.03342, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 319/1000, Train Loss: 0.03342, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 320/1000, Train Loss: 0.03341, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 321/1000, Train Loss: 0.03341, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 322/1000, Train Loss: 0.03341, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 323/1000, Train Loss: 0.03340, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 324/1000, Train Loss: 0.03340, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 325/1000, Train Loss: 0.03340, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 326/1000, Train Loss: 0.03339, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 327/1000, Train Loss: 0.03339, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 328/1000, Train Loss: 0.03339, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 329/1000, Train Loss: 0.03338, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 330/1000, Train Loss: 0.03338, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 331/1000, Train Loss: 0.03338, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 332/1000, Train Loss: 0.03338, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 333/1000, Train Loss: 0.03337, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 334/1000, Train Loss: 0.03337, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 335/1000, Train Loss: 0.03337, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 336/1000, Train Loss: 0.03336, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 337/1000, Train Loss: 0.03336, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 338/1000, Train Loss: 0.03335, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 339/1000, Train Loss: 0.03335, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 340/1000, Train Loss: 0.03335, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 341/1000, Train Loss: 0.03334, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 342/1000, Train Loss: 0.03334, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 343/1000, Train Loss: 0.03334, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 344/1000, Train Loss: 0.03333, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 345/1000, Train Loss: 0.03333, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 346/1000, Train Loss: 0.03332, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 347/1000, Train Loss: 0.03332, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 348/1000, Train Loss: 0.03332, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 349/1000, Train Loss: 0.03332, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 350/1000, Train Loss: 0.03331, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 351/1000, Train Loss: 0.03331, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 352/1000, Train Loss: 0.03331, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 353/1000, Train Loss: 0.03330, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 354/1000, Train Loss: 0.03330, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 355/1000, Train Loss: 0.03330, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 356/1000, Train Loss: 0.03329, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 357/1000, Train Loss: 0.03329, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 358/1000, Train Loss: 0.03329, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 359/1000, Train Loss: 0.03329, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 360/1000, Train Loss: 0.03328, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 361/1000, Train Loss: 0.03328, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 362/1000, Train Loss: 0.03328, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 363/1000, Train Loss: 0.03327, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 364/1000, Train Loss: 0.03327, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 365/1000, Train Loss: 0.03327, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 366/1000, Train Loss: 0.03326, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 367/1000, Train Loss: 0.03326, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 368/1000, Train Loss: 0.03326, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 369/1000, Train Loss: 0.03326, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 370/1000, Train Loss: 0.03325, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 371/1000, Train Loss: 0.03325, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 372/1000, Train Loss: 0.03325, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 373/1000, Train Loss: 0.03324, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 374/1000, Train Loss: 0.03324, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 375/1000, Train Loss: 0.03324, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 376/1000, Train Loss: 0.03324, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 377/1000, Train Loss: 0.03323, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 378/1000, Train Loss: 0.03323, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 379/1000, Train Loss: 0.03323, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 380/1000, Train Loss: 0.03322, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 381/1000, Train Loss: 0.03322, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 382/1000, Train Loss: 0.03322, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 383/1000, Train Loss: 0.03321, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 384/1000, Train Loss: 0.03321, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 385/1000, Train Loss: 0.03321, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 386/1000, Train Loss: 0.03321, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 387/1000, Train Loss: 0.03320, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 388/1000, Train Loss: 0.03320, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 389/1000, Train Loss: 0.03320, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 390/1000, Train Loss: 0.03320, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 391/1000, Train Loss: 0.03311, lr: 6.25e-05\n",
      "Val Loss: 0.03612\n",
      "---------\n",
      "Epoch 392/1000, Train Loss: 0.03309, lr: 6.25e-05\n",
      "Val Loss: 0.03612\n",
      "---------\n",
      "Epoch 393/1000, Train Loss: 0.03308, lr: 6.25e-05\n",
      "Val Loss: 0.03612\n",
      "---------\n",
      "Epoch 394/1000, Train Loss: 0.03308, lr: 6.25e-05\n",
      "Val Loss: 0.03612\n",
      "---------\n",
      "Epoch 395/1000, Train Loss: 0.03307, lr: 6.25e-05\n",
      "Val Loss: 0.03612\n",
      "---------\n",
      "Epoch 396/1000, Train Loss: 0.03307, lr: 6.25e-05\n",
      "Val Loss: 0.03612\n",
      "---------\n",
      "Epoch 397/1000, Train Loss: 0.03307, lr: 6.25e-05\n",
      "Val Loss: 0.03612\n",
      "---------\n",
      "Epoch 398/1000, Train Loss: 0.03306, lr: 6.25e-05\n",
      "Val Loss: 0.03612\n",
      "---------\n",
      "Epoch 399/1000, Train Loss: 0.03306, lr: 6.25e-05\n",
      "Val Loss: 0.03612\n",
      "---------\n",
      "Epoch 400/1000, Train Loss: 0.03306, lr: 6.25e-05\n",
      "Val Loss: 0.03612\n",
      "---------\n",
      "Epoch 401/1000, Train Loss: 0.03306, lr: 6.25e-05\n",
      "Val Loss: 0.03612\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 402/1000, Train Loss: 0.03301, lr: 3.125e-05\n",
      "Val Loss: 0.03609\n",
      "---------\n",
      "Epoch 403/1000, Train Loss: 0.03300, lr: 3.125e-05\n",
      "Val Loss: 0.03609\n",
      "---------\n",
      "Epoch 404/1000, Train Loss: 0.03300, lr: 3.125e-05\n",
      "Val Loss: 0.03609\n",
      "---------\n",
      "Epoch 405/1000, Train Loss: 0.03300, lr: 3.125e-05\n",
      "Val Loss: 0.03609\n",
      "---------\n",
      "Epoch 406/1000, Train Loss: 0.03300, lr: 3.125e-05\n",
      "Val Loss: 0.03609\n",
      "---------\n",
      "Epoch 407/1000, Train Loss: 0.03299, lr: 3.125e-05\n",
      "Val Loss: 0.03609\n",
      "---------\n",
      "Epoch 408/1000, Train Loss: 0.03299, lr: 3.125e-05\n",
      "Val Loss: 0.03609\n",
      "---------\n",
      "Epoch 409/1000, Train Loss: 0.03299, lr: 3.125e-05\n",
      "Val Loss: 0.03609\n",
      "---------\n",
      "Epoch 410/1000, Train Loss: 0.03299, lr: 3.125e-05\n",
      "Val Loss: 0.03609\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 411/1000, Train Loss: 0.03295, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 412/1000, Train Loss: 0.03294, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 413/1000, Train Loss: 0.03294, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 414/1000, Train Loss: 0.03294, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 415/1000, Train Loss: 0.03294, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 416/1000, Train Loss: 0.03294, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 417/1000, Train Loss: 0.03294, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 418/1000, Train Loss: 0.03294, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 419/1000, Train Loss: 0.03294, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 420/1000, Train Loss: 0.03293, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 421/1000, Train Loss: 0.03293, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 422/1000, Train Loss: 0.03293, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 423/1000, Train Loss: 0.03293, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 424/1000, Train Loss: 0.03293, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 425/1000, Train Loss: 0.03293, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 426/1000, Train Loss: 0.03293, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 427/1000, Train Loss: 0.03293, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 428/1000, Train Loss: 0.03293, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 429/1000, Train Loss: 0.03293, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 430/1000, Train Loss: 0.03293, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 431/1000, Train Loss: 0.03292, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 432/1000, Train Loss: 0.03292, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 433/1000, Train Loss: 0.03292, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 434/1000, Train Loss: 0.03292, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 435/1000, Train Loss: 0.03292, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 436/1000, Train Loss: 0.03292, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 437/1000, Train Loss: 0.03292, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 438/1000, Train Loss: 0.03292, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 439/1000, Train Loss: 0.03292, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 440/1000, Train Loss: 0.03292, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 441/1000, Train Loss: 0.03292, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 442/1000, Train Loss: 0.03292, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 443/1000, Train Loss: 0.03291, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 444/1000, Train Loss: 0.03291, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 445/1000, Train Loss: 0.03291, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 446/1000, Train Loss: 0.03291, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 447/1000, Train Loss: 0.03291, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 448/1000, Train Loss: 0.03291, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 449/1000, Train Loss: 0.03291, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 450/1000, Train Loss: 0.03291, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 451/1000, Train Loss: 0.03291, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 452/1000, Train Loss: 0.03291, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "Epoch 453/1000, Train Loss: 0.03291, lr: 1.5625e-05\n",
      "Val Loss: 0.03607\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 454/1000, Train Loss: 0.03289, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 455/1000, Train Loss: 0.03288, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 456/1000, Train Loss: 0.03288, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 457/1000, Train Loss: 0.03288, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 458/1000, Train Loss: 0.03288, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 459/1000, Train Loss: 0.03288, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 460/1000, Train Loss: 0.03288, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 461/1000, Train Loss: 0.03288, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 462/1000, Train Loss: 0.03288, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 463/1000, Train Loss: 0.03288, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 464/1000, Train Loss: 0.03288, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 465/1000, Train Loss: 0.03288, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 466/1000, Train Loss: 0.03288, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 467/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 468/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 469/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 470/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 471/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 472/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 473/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 474/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 475/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 476/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 477/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 478/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 479/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 480/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 481/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 482/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 483/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 484/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 485/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 486/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 487/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 488/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 489/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 490/1000, Train Loss: 0.03287, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 491/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 492/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 493/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 494/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 495/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 496/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 497/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 498/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 499/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 500/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 501/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 502/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 503/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 504/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 505/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 506/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 507/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 508/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 509/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 510/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 511/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 512/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 513/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 514/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 515/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 516/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 517/1000, Train Loss: 0.03286, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 518/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 519/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 520/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 521/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 522/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 523/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 524/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 525/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 526/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 527/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 528/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 529/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 530/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 531/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 532/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 533/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 534/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 535/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 536/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 537/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 538/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 539/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 540/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 541/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 542/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 543/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 544/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 545/1000, Train Loss: 0.03285, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 546/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 547/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 548/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 549/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 550/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 551/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 552/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 553/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 554/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 555/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 556/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 557/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 558/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 559/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 560/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 561/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 562/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 563/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 564/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 565/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 566/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 567/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 568/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 569/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 570/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 571/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 572/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 573/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 574/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 575/1000, Train Loss: 0.03284, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 576/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 577/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 578/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 579/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 580/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 581/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 582/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 583/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 584/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 585/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 586/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 587/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 588/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 589/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 590/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 591/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 592/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 593/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 594/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 595/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 596/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 597/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 598/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 599/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 600/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 601/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 602/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 603/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 604/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 605/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 606/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 607/1000, Train Loss: 0.03283, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 608/1000, Train Loss: 0.03282, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 609/1000, Train Loss: 0.03282, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 610/1000, Train Loss: 0.03282, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 611/1000, Train Loss: 0.03282, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 612/1000, Train Loss: 0.03282, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 613/1000, Train Loss: 0.03282, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 614/1000, Train Loss: 0.03282, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 615/1000, Train Loss: 0.03282, lr: 7.8125e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 616/1000, Train Loss: 0.03281, lr: 3.90625e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 617/1000, Train Loss: 0.03281, lr: 3.90625e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 618/1000, Train Loss: 0.03281, lr: 3.90625e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 619/1000, Train Loss: 0.03281, lr: 3.90625e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 620/1000, Train Loss: 0.03281, lr: 3.90625e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 621/1000, Train Loss: 0.03281, lr: 3.90625e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 622/1000, Train Loss: 0.03281, lr: 3.90625e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 623/1000, Train Loss: 0.03281, lr: 3.90625e-06\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 624/1000, Train Loss: 0.03280, lr: 1.953125e-06\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 625/1000, Train Loss: 0.03280, lr: 1.953125e-06\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 626/1000, Train Loss: 0.03280, lr: 1.953125e-06\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 627/1000, Train Loss: 0.03280, lr: 1.953125e-06\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 628/1000, Train Loss: 0.03280, lr: 1.953125e-06\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 629/1000, Train Loss: 0.03280, lr: 1.953125e-06\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 630/1000, Train Loss: 0.03280, lr: 1.953125e-06\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 631/1000, Train Loss: 0.03280, lr: 1.953125e-06\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 632/1000, Train Loss: 0.03280, lr: 1.953125e-06\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 633/1000, Train Loss: 0.03280, lr: 1.953125e-06\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 634/1000, Train Loss: 0.03280, lr: 1.953125e-06\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 635/1000, Train Loss: 0.03279, lr: 9.765625e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 636/1000, Train Loss: 0.03279, lr: 9.765625e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 637/1000, Train Loss: 0.03279, lr: 9.765625e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 638/1000, Train Loss: 0.03279, lr: 9.765625e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 639/1000, Train Loss: 0.03279, lr: 9.765625e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 640/1000, Train Loss: 0.03279, lr: 9.765625e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 641/1000, Train Loss: 0.03279, lr: 9.765625e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 642/1000, Train Loss: 0.03279, lr: 9.765625e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 643/1000, Train Loss: 0.03279, lr: 4.8828125e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 644/1000, Train Loss: 0.03279, lr: 4.8828125e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 645/1000, Train Loss: 0.03279, lr: 4.8828125e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 646/1000, Train Loss: 0.03279, lr: 4.8828125e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 647/1000, Train Loss: 0.03279, lr: 4.8828125e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 648/1000, Train Loss: 0.03279, lr: 4.8828125e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 649/1000, Train Loss: 0.03279, lr: 4.8828125e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 650/1000, Train Loss: 0.03279, lr: 4.8828125e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 651/1000, Train Loss: 0.03279, lr: 2.44140625e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 652/1000, Train Loss: 0.03279, lr: 2.44140625e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 653/1000, Train Loss: 0.03279, lr: 2.44140625e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 654/1000, Train Loss: 0.03279, lr: 2.44140625e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 655/1000, Train Loss: 0.03279, lr: 2.44140625e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 656/1000, Train Loss: 0.03279, lr: 2.44140625e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 657/1000, Train Loss: 0.03279, lr: 2.44140625e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 658/1000, Train Loss: 0.03279, lr: 1.220703125e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 659/1000, Train Loss: 0.03279, lr: 1.220703125e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 660/1000, Train Loss: 0.03279, lr: 1.220703125e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 661/1000, Train Loss: 0.03279, lr: 1.220703125e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 662/1000, Train Loss: 0.03279, lr: 1.220703125e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 663/1000, Train Loss: 0.03279, lr: 1.220703125e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 664/1000, Train Loss: 0.03279, lr: 1.220703125e-07\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 665/1000, Train Loss: 0.03279, lr: 6.103515625e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 666/1000, Train Loss: 0.03279, lr: 6.103515625e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 667/1000, Train Loss: 0.03279, lr: 6.103515625e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 668/1000, Train Loss: 0.03279, lr: 6.103515625e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 669/1000, Train Loss: 0.03279, lr: 6.103515625e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 670/1000, Train Loss: 0.03279, lr: 6.103515625e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 671/1000, Train Loss: 0.03279, lr: 6.103515625e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 672/1000, Train Loss: 0.03279, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 673/1000, Train Loss: 0.03279, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 674/1000, Train Loss: 0.03279, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 675/1000, Train Loss: 0.03279, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 676/1000, Train Loss: 0.03279, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 677/1000, Train Loss: 0.03279, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 678/1000, Train Loss: 0.03279, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 679/1000, Train Loss: 0.03279, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 680/1000, Train Loss: 0.03279, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 681/1000, Train Loss: 0.03279, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 682/1000, Train Loss: 0.03279, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 683/1000, Train Loss: 0.03279, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Early stopping ....\n",
      "2998.324413061142 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1462, 32, 48, 6, 1), y_hat_i: (6, 32, 48, 6, 1), y_i: (6, 32, 48, 6, 1), batch.x: torch.Size([192, 48, 2, 6]), y: (1462, 32, 48, 6, 1)\n",
      "RMSE for t2m: 2.544008967730298; MAE for t2m: 1.9021330031559003;\n",
      "RMSE for sp: 1.5009535374356437; MAE for sp: 1.1359891769147097;\n",
      "RMSE for tcc: 0.2893024852440646; MAE for tcc: 0.20172272232449326;\n",
      "RMSE for u10: 1.2550166843423138; MAE for u10: 0.9347951752876626;\n",
      "RMSE for v10: 1.2346885566967076; MAE for v10: 0.9070193386319565;\n",
      "RMSE for tp: 0.2907819247214189; MAE for tp: 0.0798874160100423;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 2, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1462, 32, 48, 6, 1), y_hat_i: (6, 32, 48, 6, 1), y_i: (6, 32, 48, 6, 1), batch.x: torch.Size([192, 48, 2, 6]), y: (1462, 32, 48, 6, 1)\n",
      "RMSE for t2m: 2.544008967730298; MAE for t2m: 1.9021330031559003;\n",
      "RMSE for sp: 1.5009535374356437; MAE for sp: 1.1359891769147097;\n",
      "RMSE for tcc: 0.2888837450830432; MAE for tcc: 0.20047533273022639;\n",
      "RMSE for u10: 1.2550166843423138; MAE for u10: 0.9347951752876626;\n",
      "RMSE for v10: 1.2346885566967076; MAE for v10: 0.9070193386319565;\n",
      "RMSE for tp: 0.2907819247214189; MAE for tp: 0.0798874160100423;\n",
      "Epoch 1/1000, Train Loss: 0.07160, lr: 0.001--------------------------| 18.2% Complete\n",
      "Val Loss: 0.06053\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.05453, lr: 0.001\n",
      "Val Loss: 0.05460\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.04944, lr: 0.001\n",
      "Val Loss: 0.05156\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.04786, lr: 0.001\n",
      "Val Loss: 0.04900\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.04683, lr: 0.001\n",
      "Val Loss: 0.04776\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.04607, lr: 0.001\n",
      "Val Loss: 0.04658\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.04486, lr: 0.001\n",
      "Val Loss: 0.04515\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.04370, lr: 0.001\n",
      "Val Loss: 0.04416\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.04283, lr: 0.001\n",
      "Val Loss: 0.04342\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.04224, lr: 0.001\n",
      "Val Loss: 0.04305\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.04183, lr: 0.001\n",
      "Val Loss: 0.04297\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.04153, lr: 0.001\n",
      "Val Loss: 0.04268\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.04124, lr: 0.001\n",
      "Val Loss: 0.04234\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.04101, lr: 0.001\n",
      "Val Loss: 0.04208\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.04081, lr: 0.001\n",
      "Val Loss: 0.04166\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.04065, lr: 0.001\n",
      "Val Loss: 0.04141\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.04051, lr: 0.001\n",
      "Val Loss: 0.04125\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.04039, lr: 0.001\n",
      "Val Loss: 0.04115\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.04029, lr: 0.001\n",
      "Val Loss: 0.04103\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.04016, lr: 0.001\n",
      "Val Loss: 0.04089\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.04008, lr: 0.001\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.03996, lr: 0.001\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.03988, lr: 0.001\n",
      "Val Loss: 0.04043\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.03979, lr: 0.001\n",
      "Val Loss: 0.04031\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.03971, lr: 0.001\n",
      "Val Loss: 0.04025\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.03964, lr: 0.001\n",
      "Val Loss: 0.04014\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.03958, lr: 0.001\n",
      "Val Loss: 0.04005\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.03954, lr: 0.001\n",
      "Val Loss: 0.03990\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.03947, lr: 0.001\n",
      "Val Loss: 0.03982\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.03941, lr: 0.001\n",
      "Val Loss: 0.03977\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.03935, lr: 0.001\n",
      "Val Loss: 0.03974\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.03929, lr: 0.001\n",
      "Val Loss: 0.03971\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.03924, lr: 0.001\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.03919, lr: 0.001\n",
      "Val Loss: 0.03964\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.03914, lr: 0.001\n",
      "Val Loss: 0.03963\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.03909, lr: 0.001\n",
      "Val Loss: 0.03958\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.03905, lr: 0.001\n",
      "Val Loss: 0.03957\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.03901, lr: 0.001\n",
      "Val Loss: 0.03957\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.03897, lr: 0.001\n",
      "Val Loss: 0.03956\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.03893, lr: 0.001\n",
      "Val Loss: 0.03953\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.03889, lr: 0.001\n",
      "Val Loss: 0.03951\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.03885, lr: 0.001\n",
      "Val Loss: 0.03950\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.03881, lr: 0.001\n",
      "Val Loss: 0.03947\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.03878, lr: 0.001\n",
      "Val Loss: 0.03944\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.03874, lr: 0.001\n",
      "Val Loss: 0.03943\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.03871, lr: 0.001\n",
      "Val Loss: 0.03941\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.03868, lr: 0.001\n",
      "Val Loss: 0.03937\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.03865, lr: 0.001\n",
      "Val Loss: 0.03936\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.03862, lr: 0.001\n",
      "Val Loss: 0.03931\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.03858, lr: 0.001\n",
      "Val Loss: 0.03931\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.03855, lr: 0.001\n",
      "Val Loss: 0.03929\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.03852, lr: 0.001\n",
      "Val Loss: 0.03927\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.03849, lr: 0.001\n",
      "Val Loss: 0.03926\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.03846, lr: 0.001\n",
      "Val Loss: 0.03923\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.03842, lr: 0.001\n",
      "Val Loss: 0.03921\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.03839, lr: 0.001\n",
      "Val Loss: 0.03920\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.03836, lr: 0.001\n",
      "Val Loss: 0.03918\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.03833, lr: 0.001\n",
      "Val Loss: 0.03916\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.03830, lr: 0.001\n",
      "Val Loss: 0.03915\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.03828, lr: 0.001\n",
      "Val Loss: 0.03915\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.03826, lr: 0.001\n",
      "Val Loss: 0.03914\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.03823, lr: 0.001\n",
      "Val Loss: 0.03912\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.03820, lr: 0.001\n",
      "Val Loss: 0.03913\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.03818, lr: 0.001\n",
      "Val Loss: 0.03912\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.03816, lr: 0.001\n",
      "Val Loss: 0.03911\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.03814, lr: 0.001\n",
      "Val Loss: 0.03910\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.03812, lr: 0.001\n",
      "Val Loss: 0.03910\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.03809, lr: 0.001\n",
      "Val Loss: 0.03908\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.03808, lr: 0.001\n",
      "Val Loss: 0.03904\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.03805, lr: 0.001\n",
      "Val Loss: 0.03901\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.03804, lr: 0.001\n",
      "Val Loss: 0.03897\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.03802, lr: 0.001\n",
      "Val Loss: 0.03895\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.03801, lr: 0.001\n",
      "Val Loss: 0.03892\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.03800, lr: 0.001\n",
      "Val Loss: 0.03888\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.03798, lr: 0.001\n",
      "Val Loss: 0.03884\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.03797, lr: 0.001\n",
      "Val Loss: 0.03883\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.03796, lr: 0.001\n",
      "Val Loss: 0.03879\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.03794, lr: 0.001\n",
      "Val Loss: 0.03879\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.03794, lr: 0.001\n",
      "Val Loss: 0.03873\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.03790, lr: 0.001\n",
      "Val Loss: 0.03870\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.03789, lr: 0.001\n",
      "Val Loss: 0.03868\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.03788, lr: 0.001\n",
      "Val Loss: 0.03867\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.03786, lr: 0.001\n",
      "Val Loss: 0.03864\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.03785, lr: 0.001\n",
      "Val Loss: 0.03862\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.03782, lr: 0.001\n",
      "Val Loss: 0.03861\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.03780, lr: 0.001\n",
      "Val Loss: 0.03860\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.03778, lr: 0.001\n",
      "Val Loss: 0.03859\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.03776, lr: 0.001\n",
      "Val Loss: 0.03857\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.03775, lr: 0.001\n",
      "Val Loss: 0.03853\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.03773, lr: 0.001\n",
      "Val Loss: 0.03851\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.03771, lr: 0.001\n",
      "Val Loss: 0.03847\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.03769, lr: 0.001\n",
      "Val Loss: 0.03842\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.03765, lr: 0.001\n",
      "Val Loss: 0.03838\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.03762, lr: 0.001\n",
      "Val Loss: 0.03835\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.03759, lr: 0.001\n",
      "Val Loss: 0.03834\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.03757, lr: 0.001\n",
      "Val Loss: 0.03831\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.03754, lr: 0.001\n",
      "Val Loss: 0.03827\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.03752, lr: 0.001\n",
      "Val Loss: 0.03824\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.03750, lr: 0.001\n",
      "Val Loss: 0.03821\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.03748, lr: 0.001\n",
      "Val Loss: 0.03817\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.03746, lr: 0.001\n",
      "Val Loss: 0.03814\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.03744, lr: 0.001\n",
      "Val Loss: 0.03811\n",
      "---------\n",
      "Epoch 103/1000, Train Loss: 0.03743, lr: 0.001\n",
      "Val Loss: 0.03809\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.03740, lr: 0.001\n",
      "Val Loss: 0.03806\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.03738, lr: 0.001\n",
      "Val Loss: 0.03804\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.03737, lr: 0.001\n",
      "Val Loss: 0.03801\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.03735, lr: 0.001\n",
      "Val Loss: 0.03798\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.03733, lr: 0.001\n",
      "Val Loss: 0.03797\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.03733, lr: 0.001\n",
      "Val Loss: 0.03793\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.03730, lr: 0.001\n",
      "Val Loss: 0.03793\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.03729, lr: 0.001\n",
      "Val Loss: 0.03794\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.03728, lr: 0.001\n",
      "Val Loss: 0.03792\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.03726, lr: 0.001\n",
      "Val Loss: 0.03792\n",
      "---------\n",
      "Epoch 114/1000, Train Loss: 0.03726, lr: 0.001\n",
      "Val Loss: 0.03794\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.03724, lr: 0.001\n",
      "Val Loss: 0.03790\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.03722, lr: 0.001\n",
      "Val Loss: 0.03789\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.03723, lr: 0.001\n",
      "Val Loss: 0.03786\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.03719, lr: 0.001\n",
      "Val Loss: 0.03788\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.03718, lr: 0.001\n",
      "Val Loss: 0.03788\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.03717, lr: 0.001\n",
      "Val Loss: 0.03787\n",
      "---------\n",
      "Epoch 121/1000, Train Loss: 0.03715, lr: 0.001\n",
      "Val Loss: 0.03784\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.03713, lr: 0.001\n",
      "Val Loss: 0.03785\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.03712, lr: 0.001\n",
      "Val Loss: 0.03784\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.03710, lr: 0.001\n",
      "Val Loss: 0.03782\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.03709, lr: 0.001\n",
      "Val Loss: 0.03781\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.03710, lr: 0.001\n",
      "Val Loss: 0.03786\n",
      "---------\n",
      "Epoch 127/1000, Train Loss: 0.03707, lr: 0.001\n",
      "Val Loss: 0.03782\n",
      "---------\n",
      "Epoch 128/1000, Train Loss: 0.03706, lr: 0.001\n",
      "Val Loss: 0.03784\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.03704, lr: 0.001\n",
      "Val Loss: 0.03781\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.03703, lr: 0.001\n",
      "Val Loss: 0.03782\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.03702, lr: 0.001\n",
      "Val Loss: 0.03781\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.03702, lr: 0.001\n",
      "Val Loss: 0.03782\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.03700, lr: 0.001\n",
      "Val Loss: 0.03781\n",
      "---------\n",
      "Epoch 134/1000, Train Loss: 0.03699, lr: 0.001\n",
      "Val Loss: 0.03781\n",
      "---------\n",
      "Epoch 135/1000, Train Loss: 0.03697, lr: 0.001\n",
      "Val Loss: 0.03779\n",
      "---------\n",
      "Epoch 136/1000, Train Loss: 0.03696, lr: 0.001\n",
      "Val Loss: 0.03780\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.03696, lr: 0.001\n",
      "Val Loss: 0.03780\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.03694, lr: 0.001\n",
      "Val Loss: 0.03779\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.03693, lr: 0.001\n",
      "Val Loss: 0.03779\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.03692, lr: 0.001\n",
      "Val Loss: 0.03777\n",
      "---------\n",
      "Epoch 141/1000, Train Loss: 0.03692, lr: 0.001\n",
      "Val Loss: 0.03777\n",
      "---------\n",
      "Epoch 142/1000, Train Loss: 0.03690, lr: 0.001\n",
      "Val Loss: 0.03775\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.03687, lr: 0.001\n",
      "Val Loss: 0.03775\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.03687, lr: 0.001\n",
      "Val Loss: 0.03774\n",
      "---------\n",
      "Epoch 145/1000, Train Loss: 0.03687, lr: 0.001\n",
      "Val Loss: 0.03773\n",
      "---------\n",
      "Epoch 146/1000, Train Loss: 0.03686, lr: 0.001\n",
      "Val Loss: 0.03774\n",
      "---------\n",
      "Epoch 147/1000, Train Loss: 0.03685, lr: 0.001\n",
      "Val Loss: 0.03772\n",
      "---------\n",
      "Epoch 148/1000, Train Loss: 0.03684, lr: 0.001\n",
      "Val Loss: 0.03770\n",
      "---------\n",
      "Epoch 149/1000, Train Loss: 0.03682, lr: 0.001\n",
      "Val Loss: 0.03770\n",
      "---------\n",
      "Epoch 150/1000, Train Loss: 0.03683, lr: 0.001\n",
      "Val Loss: 0.03774\n",
      "---------\n",
      "Epoch 151/1000, Train Loss: 0.03681, lr: 0.001\n",
      "Val Loss: 0.03769\n",
      "---------\n",
      "Epoch 152/1000, Train Loss: 0.03679, lr: 0.001\n",
      "Val Loss: 0.03768\n",
      "---------\n",
      "Epoch 153/1000, Train Loss: 0.03678, lr: 0.001\n",
      "Val Loss: 0.03769\n",
      "---------\n",
      "Epoch 154/1000, Train Loss: 0.03678, lr: 0.001\n",
      "Val Loss: 0.03768\n",
      "---------\n",
      "Epoch 155/1000, Train Loss: 0.03676, lr: 0.001\n",
      "Val Loss: 0.03768\n",
      "---------\n",
      "Epoch 156/1000, Train Loss: 0.03675, lr: 0.001\n",
      "Val Loss: 0.03766\n",
      "---------\n",
      "Epoch 157/1000, Train Loss: 0.03674, lr: 0.001\n",
      "Val Loss: 0.03765\n",
      "---------\n",
      "Epoch 158/1000, Train Loss: 0.03674, lr: 0.001\n",
      "Val Loss: 0.03765\n",
      "---------\n",
      "Epoch 159/1000, Train Loss: 0.03672, lr: 0.001\n",
      "Val Loss: 0.03764\n",
      "---------\n",
      "Epoch 160/1000, Train Loss: 0.03672, lr: 0.001\n",
      "Val Loss: 0.03765\n",
      "---------\n",
      "Epoch 161/1000, Train Loss: 0.03670, lr: 0.001\n",
      "Val Loss: 0.03763\n",
      "---------\n",
      "Epoch 162/1000, Train Loss: 0.03672, lr: 0.001\n",
      "Val Loss: 0.03762\n",
      "---------\n",
      "Epoch 163/1000, Train Loss: 0.03670, lr: 0.001\n",
      "Val Loss: 0.03759\n",
      "---------\n",
      "Epoch 164/1000, Train Loss: 0.03668, lr: 0.001\n",
      "Val Loss: 0.03760\n",
      "---------\n",
      "Epoch 165/1000, Train Loss: 0.03667, lr: 0.001\n",
      "Val Loss: 0.03758\n",
      "---------\n",
      "Epoch 166/1000, Train Loss: 0.03666, lr: 0.001\n",
      "Val Loss: 0.03760\n",
      "---------\n",
      "Epoch 167/1000, Train Loss: 0.03668, lr: 0.001\n",
      "Val Loss: 0.03757\n",
      "---------\n",
      "Epoch 168/1000, Train Loss: 0.03664, lr: 0.001\n",
      "Val Loss: 0.03757\n",
      "---------\n",
      "Epoch 169/1000, Train Loss: 0.03664, lr: 0.001\n",
      "Val Loss: 0.03758\n",
      "---------\n",
      "Epoch 170/1000, Train Loss: 0.03663, lr: 0.001\n",
      "Val Loss: 0.03755\n",
      "---------\n",
      "Epoch 171/1000, Train Loss: 0.03663, lr: 0.001\n",
      "Val Loss: 0.03757\n",
      "---------\n",
      "Epoch 172/1000, Train Loss: 0.03663, lr: 0.001\n",
      "Val Loss: 0.03754\n",
      "---------\n",
      "Epoch 173/1000, Train Loss: 0.03661, lr: 0.001\n",
      "Val Loss: 0.03755\n",
      "---------\n",
      "Epoch 174/1000, Train Loss: 0.03661, lr: 0.001\n",
      "Val Loss: 0.03754\n",
      "---------\n",
      "Epoch 175/1000, Train Loss: 0.03660, lr: 0.001\n",
      "Val Loss: 0.03754\n",
      "---------\n",
      "Epoch 176/1000, Train Loss: 0.03659, lr: 0.001\n",
      "Val Loss: 0.03755\n",
      "---------\n",
      "Epoch 177/1000, Train Loss: 0.03660, lr: 0.001\n",
      "Val Loss: 0.03752\n",
      "---------\n",
      "Epoch 178/1000, Train Loss: 0.03659, lr: 0.001\n",
      "Val Loss: 0.03756\n",
      "---------\n",
      "Epoch 179/1000, Train Loss: 0.03658, lr: 0.001\n",
      "Val Loss: 0.03754\n",
      "---------\n",
      "Epoch 180/1000, Train Loss: 0.03657, lr: 0.001\n",
      "Val Loss: 0.03754\n",
      "---------\n",
      "Epoch 181/1000, Train Loss: 0.03658, lr: 0.001\n",
      "Val Loss: 0.03750\n",
      "---------\n",
      "Epoch 182/1000, Train Loss: 0.03658, lr: 0.001\n",
      "Val Loss: 0.03759\n",
      "---------\n",
      "Epoch 183/1000, Train Loss: 0.03659, lr: 0.001\n",
      "Val Loss: 0.03780\n",
      "---------\n",
      "Epoch 184/1000, Train Loss: 0.03661, lr: 0.001\n",
      "Val Loss: 0.03834\n",
      "---------\n",
      "Epoch 185/1000, Train Loss: 0.03671, lr: 0.001\n",
      "Val Loss: 0.03851\n",
      "---------\n",
      "Epoch 186/1000, Train Loss: 0.03667, lr: 0.001\n",
      "Val Loss: 0.03819\n",
      "---------\n",
      "Epoch 187/1000, Train Loss: 0.03663, lr: 0.001\n",
      "Val Loss: 0.03824\n",
      "---------\n",
      "Epoch 188/1000, Train Loss: 0.03663, lr: 0.001\n",
      "Val Loss: 0.03817\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 189/1000, Train Loss: 0.03596, lr: 0.0005\n",
      "Val Loss: 0.03723\n",
      "---------\n",
      "Epoch 190/1000, Train Loss: 0.03590, lr: 0.0005\n",
      "Val Loss: 0.03722\n",
      "---------\n",
      "Epoch 191/1000, Train Loss: 0.03588, lr: 0.0005\n",
      "Val Loss: 0.03719\n",
      "---------\n",
      "Epoch 192/1000, Train Loss: 0.03587, lr: 0.0005\n",
      "Val Loss: 0.03719\n",
      "---------\n",
      "Epoch 193/1000, Train Loss: 0.03587, lr: 0.0005\n",
      "Val Loss: 0.03720\n",
      "---------\n",
      "Epoch 194/1000, Train Loss: 0.03586, lr: 0.0005\n",
      "Val Loss: 0.03719\n",
      "---------\n",
      "Epoch 195/1000, Train Loss: 0.03586, lr: 0.0005\n",
      "Val Loss: 0.03719\n",
      "---------\n",
      "Epoch 196/1000, Train Loss: 0.03585, lr: 0.0005\n",
      "Val Loss: 0.03720\n",
      "---------\n",
      "Epoch 197/1000, Train Loss: 0.03585, lr: 0.0005\n",
      "Val Loss: 0.03720\n",
      "---------\n",
      "Epoch 198/1000, Train Loss: 0.03584, lr: 0.0005\n",
      "Val Loss: 0.03720\n",
      "---------\n",
      "Epoch 199/1000, Train Loss: 0.03583, lr: 0.0005\n",
      "Val Loss: 0.03719\n",
      "---------\n",
      "Epoch 200/1000, Train Loss: 0.03583, lr: 0.0005\n",
      "Val Loss: 0.03719\n",
      "---------\n",
      "Epoch 201/1000, Train Loss: 0.03582, lr: 0.0005\n",
      "Val Loss: 0.03719\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 202/1000, Train Loss: 0.03539, lr: 0.00025\n",
      "Val Loss: 0.03677\n",
      "---------\n",
      "Epoch 203/1000, Train Loss: 0.03535, lr: 0.00025\n",
      "Val Loss: 0.03678\n",
      "---------\n",
      "Epoch 204/1000, Train Loss: 0.03535, lr: 0.00025\n",
      "Val Loss: 0.03678\n",
      "---------\n",
      "Epoch 205/1000, Train Loss: 0.03535, lr: 0.00025\n",
      "Val Loss: 0.03678\n",
      "---------\n",
      "Epoch 206/1000, Train Loss: 0.03535, lr: 0.00025\n",
      "Val Loss: 0.03678\n",
      "---------\n",
      "Epoch 207/1000, Train Loss: 0.03535, lr: 0.00025\n",
      "Val Loss: 0.03678\n",
      "---------\n",
      "Epoch 208/1000, Train Loss: 0.03534, lr: 0.00025\n",
      "Val Loss: 0.03678\n",
      "---------\n",
      "Epoch 209/1000, Train Loss: 0.03534, lr: 0.00025\n",
      "Val Loss: 0.03678\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 210/1000, Train Loss: 0.03513, lr: 0.000125\n",
      "Val Loss: 0.03671\n",
      "---------\n",
      "Epoch 211/1000, Train Loss: 0.03512, lr: 0.000125\n",
      "Val Loss: 0.03671\n",
      "---------\n",
      "Epoch 212/1000, Train Loss: 0.03512, lr: 0.000125\n",
      "Val Loss: 0.03670\n",
      "---------\n",
      "Epoch 213/1000, Train Loss: 0.03512, lr: 0.000125\n",
      "Val Loss: 0.03669\n",
      "---------\n",
      "Epoch 214/1000, Train Loss: 0.03511, lr: 0.000125\n",
      "Val Loss: 0.03669\n",
      "---------\n",
      "Epoch 215/1000, Train Loss: 0.03511, lr: 0.000125\n",
      "Val Loss: 0.03669\n",
      "---------\n",
      "Epoch 216/1000, Train Loss: 0.03511, lr: 0.000125\n",
      "Val Loss: 0.03668\n",
      "---------\n",
      "Epoch 217/1000, Train Loss: 0.03511, lr: 0.000125\n",
      "Val Loss: 0.03668\n",
      "---------\n",
      "Epoch 218/1000, Train Loss: 0.03511, lr: 0.000125\n",
      "Val Loss: 0.03667\n",
      "---------\n",
      "Epoch 219/1000, Train Loss: 0.03510, lr: 0.000125\n",
      "Val Loss: 0.03667\n",
      "---------\n",
      "Epoch 220/1000, Train Loss: 0.03510, lr: 0.000125\n",
      "Val Loss: 0.03667\n",
      "---------\n",
      "Epoch 221/1000, Train Loss: 0.03510, lr: 0.000125\n",
      "Val Loss: 0.03667\n",
      "---------\n",
      "Epoch 222/1000, Train Loss: 0.03510, lr: 0.000125\n",
      "Val Loss: 0.03666\n",
      "---------\n",
      "Epoch 223/1000, Train Loss: 0.03510, lr: 0.000125\n",
      "Val Loss: 0.03666\n",
      "---------\n",
      "Epoch 224/1000, Train Loss: 0.03509, lr: 0.000125\n",
      "Val Loss: 0.03666\n",
      "---------\n",
      "Epoch 225/1000, Train Loss: 0.03509, lr: 0.000125\n",
      "Val Loss: 0.03666\n",
      "---------\n",
      "Epoch 226/1000, Train Loss: 0.03509, lr: 0.000125\n",
      "Val Loss: 0.03665\n",
      "---------\n",
      "Epoch 227/1000, Train Loss: 0.03509, lr: 0.000125\n",
      "Val Loss: 0.03665\n",
      "---------\n",
      "Epoch 228/1000, Train Loss: 0.03509, lr: 0.000125\n",
      "Val Loss: 0.03665\n",
      "---------\n",
      "Epoch 229/1000, Train Loss: 0.03508, lr: 0.000125\n",
      "Val Loss: 0.03665\n",
      "---------\n",
      "Epoch 230/1000, Train Loss: 0.03508, lr: 0.000125\n",
      "Val Loss: 0.03665\n",
      "---------\n",
      "Epoch 231/1000, Train Loss: 0.03508, lr: 0.000125\n",
      "Val Loss: 0.03665\n",
      "---------\n",
      "Epoch 232/1000, Train Loss: 0.03508, lr: 0.000125\n",
      "Val Loss: 0.03664\n",
      "---------\n",
      "Epoch 233/1000, Train Loss: 0.03508, lr: 0.000125\n",
      "Val Loss: 0.03664\n",
      "---------\n",
      "Epoch 234/1000, Train Loss: 0.03507, lr: 0.000125\n",
      "Val Loss: 0.03664\n",
      "---------\n",
      "Epoch 235/1000, Train Loss: 0.03507, lr: 0.000125\n",
      "Val Loss: 0.03664\n",
      "---------\n",
      "Epoch 236/1000, Train Loss: 0.03507, lr: 0.000125\n",
      "Val Loss: 0.03664\n",
      "---------\n",
      "Epoch 237/1000, Train Loss: 0.03507, lr: 0.000125\n",
      "Val Loss: 0.03664\n",
      "---------\n",
      "Epoch 238/1000, Train Loss: 0.03507, lr: 0.000125\n",
      "Val Loss: 0.03663\n",
      "---------\n",
      "Epoch 239/1000, Train Loss: 0.03507, lr: 0.000125\n",
      "Val Loss: 0.03663\n",
      "---------\n",
      "Epoch 240/1000, Train Loss: 0.03506, lr: 0.000125\n",
      "Val Loss: 0.03663\n",
      "---------\n",
      "Epoch 241/1000, Train Loss: 0.03506, lr: 0.000125\n",
      "Val Loss: 0.03663\n",
      "---------\n",
      "Epoch 242/1000, Train Loss: 0.03506, lr: 0.000125\n",
      "Val Loss: 0.03663\n",
      "---------\n",
      "Epoch 243/1000, Train Loss: 0.03506, lr: 0.000125\n",
      "Val Loss: 0.03663\n",
      "---------\n",
      "Epoch 244/1000, Train Loss: 0.03506, lr: 0.000125\n",
      "Val Loss: 0.03663\n",
      "---------\n",
      "Epoch 245/1000, Train Loss: 0.03506, lr: 0.000125\n",
      "Val Loss: 0.03662\n",
      "---------\n",
      "Epoch 246/1000, Train Loss: 0.03505, lr: 0.000125\n",
      "Val Loss: 0.03662\n",
      "---------\n",
      "Epoch 247/1000, Train Loss: 0.03505, lr: 0.000125\n",
      "Val Loss: 0.03662\n",
      "---------\n",
      "Epoch 248/1000, Train Loss: 0.03505, lr: 0.000125\n",
      "Val Loss: 0.03662\n",
      "---------\n",
      "Epoch 249/1000, Train Loss: 0.03505, lr: 0.000125\n",
      "Val Loss: 0.03662\n",
      "---------\n",
      "Epoch 250/1000, Train Loss: 0.03505, lr: 0.000125\n",
      "Val Loss: 0.03662\n",
      "---------\n",
      "Epoch 251/1000, Train Loss: 0.03505, lr: 0.000125\n",
      "Val Loss: 0.03662\n",
      "---------\n",
      "Epoch 252/1000, Train Loss: 0.03504, lr: 0.000125\n",
      "Val Loss: 0.03662\n",
      "---------\n",
      "Epoch 253/1000, Train Loss: 0.03504, lr: 0.000125\n",
      "Val Loss: 0.03661\n",
      "---------\n",
      "Epoch 254/1000, Train Loss: 0.03504, lr: 0.000125\n",
      "Val Loss: 0.03661\n",
      "---------\n",
      "Epoch 255/1000, Train Loss: 0.03504, lr: 0.000125\n",
      "Val Loss: 0.03661\n",
      "---------\n",
      "Epoch 256/1000, Train Loss: 0.03504, lr: 0.000125\n",
      "Val Loss: 0.03661\n",
      "---------\n",
      "Epoch 257/1000, Train Loss: 0.03504, lr: 0.000125\n",
      "Val Loss: 0.03661\n",
      "---------\n",
      "Epoch 258/1000, Train Loss: 0.03504, lr: 0.000125\n",
      "Val Loss: 0.03661\n",
      "---------\n",
      "Epoch 259/1000, Train Loss: 0.03503, lr: 0.000125\n",
      "Val Loss: 0.03661\n",
      "---------\n",
      "Epoch 260/1000, Train Loss: 0.03503, lr: 0.000125\n",
      "Val Loss: 0.03661\n",
      "---------\n",
      "Epoch 261/1000, Train Loss: 0.03503, lr: 0.000125\n",
      "Val Loss: 0.03661\n",
      "---------\n",
      "Epoch 262/1000, Train Loss: 0.03503, lr: 0.000125\n",
      "Val Loss: 0.03660\n",
      "---------\n",
      "Epoch 263/1000, Train Loss: 0.03503, lr: 0.000125\n",
      "Val Loss: 0.03660\n",
      "---------\n",
      "Epoch 264/1000, Train Loss: 0.03503, lr: 0.000125\n",
      "Val Loss: 0.03660\n",
      "---------\n",
      "Epoch 265/1000, Train Loss: 0.03503, lr: 0.000125\n",
      "Val Loss: 0.03660\n",
      "---------\n",
      "Epoch 266/1000, Train Loss: 0.03502, lr: 0.000125\n",
      "Val Loss: 0.03660\n",
      "---------\n",
      "Epoch 267/1000, Train Loss: 0.03502, lr: 0.000125\n",
      "Val Loss: 0.03660\n",
      "---------\n",
      "Epoch 268/1000, Train Loss: 0.03502, lr: 0.000125\n",
      "Val Loss: 0.03660\n",
      "---------\n",
      "Epoch 269/1000, Train Loss: 0.03502, lr: 0.000125\n",
      "Val Loss: 0.03660\n",
      "---------\n",
      "Epoch 270/1000, Train Loss: 0.03502, lr: 0.000125\n",
      "Val Loss: 0.03660\n",
      "---------\n",
      "Epoch 271/1000, Train Loss: 0.03502, lr: 0.000125\n",
      "Val Loss: 0.03659\n",
      "---------\n",
      "Epoch 272/1000, Train Loss: 0.03502, lr: 0.000125\n",
      "Val Loss: 0.03659\n",
      "---------\n",
      "Epoch 273/1000, Train Loss: 0.03502, lr: 0.000125\n",
      "Val Loss: 0.03659\n",
      "---------\n",
      "Epoch 274/1000, Train Loss: 0.03501, lr: 0.000125\n",
      "Val Loss: 0.03659\n",
      "---------\n",
      "Epoch 275/1000, Train Loss: 0.03501, lr: 0.000125\n",
      "Val Loss: 0.03659\n",
      "---------\n",
      "Epoch 276/1000, Train Loss: 0.03501, lr: 0.000125\n",
      "Val Loss: 0.03659\n",
      "---------\n",
      "Epoch 277/1000, Train Loss: 0.03501, lr: 0.000125\n",
      "Val Loss: 0.03659\n",
      "---------\n",
      "Epoch 278/1000, Train Loss: 0.03501, lr: 0.000125\n",
      "Val Loss: 0.03659\n",
      "---------\n",
      "Epoch 279/1000, Train Loss: 0.03501, lr: 0.000125\n",
      "Val Loss: 0.03659\n",
      "---------\n",
      "Epoch 280/1000, Train Loss: 0.03501, lr: 0.000125\n",
      "Val Loss: 0.03658\n",
      "---------\n",
      "Epoch 281/1000, Train Loss: 0.03500, lr: 0.000125\n",
      "Val Loss: 0.03658\n",
      "---------\n",
      "Epoch 282/1000, Train Loss: 0.03500, lr: 0.000125\n",
      "Val Loss: 0.03658\n",
      "---------\n",
      "Epoch 283/1000, Train Loss: 0.03500, lr: 0.000125\n",
      "Val Loss: 0.03658\n",
      "---------\n",
      "Epoch 284/1000, Train Loss: 0.03500, lr: 0.000125\n",
      "Val Loss: 0.03658\n",
      "---------\n",
      "Epoch 285/1000, Train Loss: 0.03500, lr: 0.000125\n",
      "Val Loss: 0.03658\n",
      "---------\n",
      "Epoch 286/1000, Train Loss: 0.03500, lr: 0.000125\n",
      "Val Loss: 0.03658\n",
      "---------\n",
      "Epoch 287/1000, Train Loss: 0.03500, lr: 0.000125\n",
      "Val Loss: 0.03658\n",
      "---------\n",
      "Epoch 288/1000, Train Loss: 0.03500, lr: 0.000125\n",
      "Val Loss: 0.03658\n",
      "---------\n",
      "Epoch 289/1000, Train Loss: 0.03499, lr: 0.000125\n",
      "Val Loss: 0.03657\n",
      "---------\n",
      "Epoch 290/1000, Train Loss: 0.03499, lr: 0.000125\n",
      "Val Loss: 0.03657\n",
      "---------\n",
      "Epoch 291/1000, Train Loss: 0.03499, lr: 0.000125\n",
      "Val Loss: 0.03657\n",
      "---------\n",
      "Epoch 292/1000, Train Loss: 0.03499, lr: 0.000125\n",
      "Val Loss: 0.03657\n",
      "---------\n",
      "Epoch 293/1000, Train Loss: 0.03499, lr: 0.000125\n",
      "Val Loss: 0.03657\n",
      "---------\n",
      "Epoch 294/1000, Train Loss: 0.03499, lr: 0.000125\n",
      "Val Loss: 0.03657\n",
      "---------\n",
      "Epoch 295/1000, Train Loss: 0.03499, lr: 0.000125\n",
      "Val Loss: 0.03657\n",
      "---------\n",
      "Epoch 296/1000, Train Loss: 0.03499, lr: 0.000125\n",
      "Val Loss: 0.03657\n",
      "---------\n",
      "Epoch 297/1000, Train Loss: 0.03499, lr: 0.000125\n",
      "Val Loss: 0.03657\n",
      "---------\n",
      "Epoch 298/1000, Train Loss: 0.03498, lr: 0.000125\n",
      "Val Loss: 0.03657\n",
      "---------\n",
      "Epoch 299/1000, Train Loss: 0.03498, lr: 0.000125\n",
      "Val Loss: 0.03656\n",
      "---------\n",
      "Epoch 300/1000, Train Loss: 0.03498, lr: 0.000125\n",
      "Val Loss: 0.03656\n",
      "---------\n",
      "Epoch 301/1000, Train Loss: 0.03498, lr: 0.000125\n",
      "Val Loss: 0.03656\n",
      "---------\n",
      "Epoch 302/1000, Train Loss: 0.03498, lr: 0.000125\n",
      "Val Loss: 0.03656\n",
      "---------\n",
      "Epoch 303/1000, Train Loss: 0.03498, lr: 0.000125\n",
      "Val Loss: 0.03656\n",
      "---------\n",
      "Epoch 304/1000, Train Loss: 0.03498, lr: 0.000125\n",
      "Val Loss: 0.03656\n",
      "---------\n",
      "Epoch 305/1000, Train Loss: 0.03498, lr: 0.000125\n",
      "Val Loss: 0.03656\n",
      "---------\n",
      "Epoch 306/1000, Train Loss: 0.03498, lr: 0.000125\n",
      "Val Loss: 0.03656\n",
      "---------\n",
      "Epoch 307/1000, Train Loss: 0.03497, lr: 0.000125\n",
      "Val Loss: 0.03656\n",
      "---------\n",
      "Epoch 308/1000, Train Loss: 0.03497, lr: 0.000125\n",
      "Val Loss: 0.03656\n",
      "---------\n",
      "Epoch 309/1000, Train Loss: 0.03497, lr: 0.000125\n",
      "Val Loss: 0.03655\n",
      "---------\n",
      "Epoch 310/1000, Train Loss: 0.03497, lr: 0.000125\n",
      "Val Loss: 0.03655\n",
      "---------\n",
      "Epoch 311/1000, Train Loss: 0.03497, lr: 0.000125\n",
      "Val Loss: 0.03655\n",
      "---------\n",
      "Epoch 312/1000, Train Loss: 0.03497, lr: 0.000125\n",
      "Val Loss: 0.03655\n",
      "---------\n",
      "Epoch 313/1000, Train Loss: 0.03497, lr: 0.000125\n",
      "Val Loss: 0.03655\n",
      "---------\n",
      "Epoch 314/1000, Train Loss: 0.03497, lr: 0.000125\n",
      "Val Loss: 0.03655\n",
      "---------\n",
      "Epoch 315/1000, Train Loss: 0.03496, lr: 0.000125\n",
      "Val Loss: 0.03655\n",
      "---------\n",
      "Epoch 316/1000, Train Loss: 0.03496, lr: 0.000125\n",
      "Val Loss: 0.03655\n",
      "---------\n",
      "Epoch 317/1000, Train Loss: 0.03496, lr: 0.000125\n",
      "Val Loss: 0.03655\n",
      "---------\n",
      "Epoch 318/1000, Train Loss: 0.03496, lr: 0.000125\n",
      "Val Loss: 0.03655\n",
      "---------\n",
      "Epoch 319/1000, Train Loss: 0.03496, lr: 0.000125\n",
      "Val Loss: 0.03655\n",
      "---------\n",
      "Epoch 320/1000, Train Loss: 0.03496, lr: 0.000125\n",
      "Val Loss: 0.03654\n",
      "---------\n",
      "Epoch 321/1000, Train Loss: 0.03496, lr: 0.000125\n",
      "Val Loss: 0.03654\n",
      "---------\n",
      "Epoch 322/1000, Train Loss: 0.03496, lr: 0.000125\n",
      "Val Loss: 0.03654\n",
      "---------\n",
      "Epoch 323/1000, Train Loss: 0.03496, lr: 0.000125\n",
      "Val Loss: 0.03654\n",
      "---------\n",
      "Epoch 324/1000, Train Loss: 0.03495, lr: 0.000125\n",
      "Val Loss: 0.03654\n",
      "---------\n",
      "Epoch 325/1000, Train Loss: 0.03495, lr: 0.000125\n",
      "Val Loss: 0.03654\n",
      "---------\n",
      "Epoch 326/1000, Train Loss: 0.03495, lr: 0.000125\n",
      "Val Loss: 0.03654\n",
      "---------\n",
      "Epoch 327/1000, Train Loss: 0.03495, lr: 0.000125\n",
      "Val Loss: 0.03654\n",
      "---------\n",
      "Epoch 328/1000, Train Loss: 0.03495, lr: 0.000125\n",
      "Val Loss: 0.03654\n",
      "---------\n",
      "Epoch 329/1000, Train Loss: 0.03495, lr: 0.000125\n",
      "Val Loss: 0.03654\n",
      "---------\n",
      "Epoch 330/1000, Train Loss: 0.03495, lr: 0.000125\n",
      "Val Loss: 0.03654\n",
      "---------\n",
      "Epoch 331/1000, Train Loss: 0.03495, lr: 0.000125\n",
      "Val Loss: 0.03653\n",
      "---------\n",
      "Epoch 332/1000, Train Loss: 0.03495, lr: 0.000125\n",
      "Val Loss: 0.03653\n",
      "---------\n",
      "Epoch 333/1000, Train Loss: 0.03495, lr: 0.000125\n",
      "Val Loss: 0.03653\n",
      "---------\n",
      "Epoch 334/1000, Train Loss: 0.03494, lr: 0.000125\n",
      "Val Loss: 0.03653\n",
      "---------\n",
      "Epoch 335/1000, Train Loss: 0.03494, lr: 0.000125\n",
      "Val Loss: 0.03653\n",
      "---------\n",
      "Epoch 336/1000, Train Loss: 0.03494, lr: 0.000125\n",
      "Val Loss: 0.03653\n",
      "---------\n",
      "Epoch 337/1000, Train Loss: 0.03494, lr: 0.000125\n",
      "Val Loss: 0.03653\n",
      "---------\n",
      "Epoch 338/1000, Train Loss: 0.03494, lr: 0.000125\n",
      "Val Loss: 0.03653\n",
      "---------\n",
      "Epoch 339/1000, Train Loss: 0.03494, lr: 0.000125\n",
      "Val Loss: 0.03653\n",
      "---------\n",
      "Epoch 340/1000, Train Loss: 0.03494, lr: 0.000125\n",
      "Val Loss: 0.03653\n",
      "---------\n",
      "Epoch 341/1000, Train Loss: 0.03494, lr: 0.000125\n",
      "Val Loss: 0.03652\n",
      "---------\n",
      "Epoch 342/1000, Train Loss: 0.03494, lr: 0.000125\n",
      "Val Loss: 0.03652\n",
      "---------\n",
      "Epoch 343/1000, Train Loss: 0.03493, lr: 0.000125\n",
      "Val Loss: 0.03652\n",
      "---------\n",
      "Epoch 344/1000, Train Loss: 0.03493, lr: 0.000125\n",
      "Val Loss: 0.03652\n",
      "---------\n",
      "Epoch 345/1000, Train Loss: 0.03493, lr: 0.000125\n",
      "Val Loss: 0.03652\n",
      "---------\n",
      "Epoch 346/1000, Train Loss: 0.03493, lr: 0.000125\n",
      "Val Loss: 0.03652\n",
      "---------\n",
      "Epoch 347/1000, Train Loss: 0.03493, lr: 0.000125\n",
      "Val Loss: 0.03652\n",
      "---------\n",
      "Epoch 348/1000, Train Loss: 0.03493, lr: 0.000125\n",
      "Val Loss: 0.03652\n",
      "---------\n",
      "Epoch 349/1000, Train Loss: 0.03493, lr: 0.000125\n",
      "Val Loss: 0.03652\n",
      "---------\n",
      "Epoch 350/1000, Train Loss: 0.03493, lr: 0.000125\n",
      "Val Loss: 0.03652\n",
      "---------\n",
      "Epoch 351/1000, Train Loss: 0.03493, lr: 0.000125\n",
      "Val Loss: 0.03652\n",
      "---------\n",
      "Epoch 352/1000, Train Loss: 0.03493, lr: 0.000125\n",
      "Val Loss: 0.03652\n",
      "---------\n",
      "Epoch 353/1000, Train Loss: 0.03492, lr: 0.000125\n",
      "Val Loss: 0.03651\n",
      "---------\n",
      "Epoch 354/1000, Train Loss: 0.03492, lr: 0.000125\n",
      "Val Loss: 0.03651\n",
      "---------\n",
      "Epoch 355/1000, Train Loss: 0.03492, lr: 0.000125\n",
      "Val Loss: 0.03651\n",
      "---------\n",
      "Epoch 356/1000, Train Loss: 0.03492, lr: 0.000125\n",
      "Val Loss: 0.03651\n",
      "---------\n",
      "Epoch 357/1000, Train Loss: 0.03492, lr: 0.000125\n",
      "Val Loss: 0.03651\n",
      "---------\n",
      "Epoch 358/1000, Train Loss: 0.03492, lr: 0.000125\n",
      "Val Loss: 0.03651\n",
      "---------\n",
      "Epoch 359/1000, Train Loss: 0.03492, lr: 0.000125\n",
      "Val Loss: 0.03651\n",
      "---------\n",
      "Epoch 360/1000, Train Loss: 0.03492, lr: 0.000125\n",
      "Val Loss: 0.03651\n",
      "---------\n",
      "Epoch 361/1000, Train Loss: 0.03492, lr: 0.000125\n",
      "Val Loss: 0.03651\n",
      "---------\n",
      "Epoch 362/1000, Train Loss: 0.03492, lr: 0.000125\n",
      "Val Loss: 0.03651\n",
      "---------\n",
      "Epoch 363/1000, Train Loss: 0.03492, lr: 0.000125\n",
      "Val Loss: 0.03651\n",
      "---------\n",
      "Epoch 364/1000, Train Loss: 0.03491, lr: 0.000125\n",
      "Val Loss: 0.03650\n",
      "---------\n",
      "Epoch 365/1000, Train Loss: 0.03491, lr: 0.000125\n",
      "Val Loss: 0.03650\n",
      "---------\n",
      "Epoch 366/1000, Train Loss: 0.03491, lr: 0.000125\n",
      "Val Loss: 0.03650\n",
      "---------\n",
      "Epoch 367/1000, Train Loss: 0.03491, lr: 0.000125\n",
      "Val Loss: 0.03650\n",
      "---------\n",
      "Epoch 368/1000, Train Loss: 0.03491, lr: 0.000125\n",
      "Val Loss: 0.03650\n",
      "---------\n",
      "Epoch 369/1000, Train Loss: 0.03491, lr: 0.000125\n",
      "Val Loss: 0.03650\n",
      "---------\n",
      "Epoch 370/1000, Train Loss: 0.03491, lr: 0.000125\n",
      "Val Loss: 0.03650\n",
      "---------\n",
      "Epoch 371/1000, Train Loss: 0.03491, lr: 0.000125\n",
      "Val Loss: 0.03650\n",
      "---------\n",
      "Epoch 372/1000, Train Loss: 0.03491, lr: 0.000125\n",
      "Val Loss: 0.03650\n",
      "---------\n",
      "Epoch 373/1000, Train Loss: 0.03491, lr: 0.000125\n",
      "Val Loss: 0.03650\n",
      "---------\n",
      "Epoch 374/1000, Train Loss: 0.03490, lr: 0.000125\n",
      "Val Loss: 0.03650\n",
      "---------\n",
      "Epoch 375/1000, Train Loss: 0.03490, lr: 0.000125\n",
      "Val Loss: 0.03650\n",
      "---------\n",
      "Epoch 376/1000, Train Loss: 0.03490, lr: 0.000125\n",
      "Val Loss: 0.03649\n",
      "---------\n",
      "Epoch 377/1000, Train Loss: 0.03490, lr: 0.000125\n",
      "Val Loss: 0.03649\n",
      "---------\n",
      "Epoch 378/1000, Train Loss: 0.03490, lr: 0.000125\n",
      "Val Loss: 0.03649\n",
      "---------\n",
      "Epoch 379/1000, Train Loss: 0.03490, lr: 0.000125\n",
      "Val Loss: 0.03649\n",
      "---------\n",
      "Epoch 380/1000, Train Loss: 0.03490, lr: 0.000125\n",
      "Val Loss: 0.03649\n",
      "---------\n",
      "Epoch 381/1000, Train Loss: 0.03490, lr: 0.000125\n",
      "Val Loss: 0.03649\n",
      "---------\n",
      "Epoch 382/1000, Train Loss: 0.03490, lr: 0.000125\n",
      "Val Loss: 0.03649\n",
      "---------\n",
      "Epoch 383/1000, Train Loss: 0.03490, lr: 0.000125\n",
      "Val Loss: 0.03649\n",
      "---------\n",
      "Epoch 384/1000, Train Loss: 0.03489, lr: 0.000125\n",
      "Val Loss: 0.03649\n",
      "---------\n",
      "Epoch 385/1000, Train Loss: 0.03489, lr: 0.000125\n",
      "Val Loss: 0.03649\n",
      "---------\n",
      "Epoch 386/1000, Train Loss: 0.03489, lr: 0.000125\n",
      "Val Loss: 0.03649\n",
      "---------\n",
      "Epoch 387/1000, Train Loss: 0.03489, lr: 0.000125\n",
      "Val Loss: 0.03649\n",
      "---------\n",
      "Epoch 388/1000, Train Loss: 0.03489, lr: 0.000125\n",
      "Val Loss: 0.03649\n",
      "---------\n",
      "Epoch 389/1000, Train Loss: 0.03489, lr: 0.000125\n",
      "Val Loss: 0.03648\n",
      "---------\n",
      "Epoch 390/1000, Train Loss: 0.03489, lr: 0.000125\n",
      "Val Loss: 0.03648\n",
      "---------\n",
      "Epoch 391/1000, Train Loss: 0.03489, lr: 0.000125\n",
      "Val Loss: 0.03648\n",
      "---------\n",
      "Epoch 392/1000, Train Loss: 0.03489, lr: 0.000125\n",
      "Val Loss: 0.03648\n",
      "---------\n",
      "Epoch 393/1000, Train Loss: 0.03489, lr: 0.000125\n",
      "Val Loss: 0.03648\n",
      "---------\n",
      "Epoch 394/1000, Train Loss: 0.03489, lr: 0.000125\n",
      "Val Loss: 0.03648\n",
      "---------\n",
      "Epoch 395/1000, Train Loss: 0.03488, lr: 0.000125\n",
      "Val Loss: 0.03648\n",
      "---------\n",
      "Epoch 396/1000, Train Loss: 0.03488, lr: 0.000125\n",
      "Val Loss: 0.03648\n",
      "---------\n",
      "Epoch 397/1000, Train Loss: 0.03488, lr: 0.000125\n",
      "Val Loss: 0.03648\n",
      "---------\n",
      "Epoch 398/1000, Train Loss: 0.03488, lr: 0.000125\n",
      "Val Loss: 0.03648\n",
      "---------\n",
      "Epoch 399/1000, Train Loss: 0.03488, lr: 0.000125\n",
      "Val Loss: 0.03648\n",
      "---------\n",
      "Epoch 400/1000, Train Loss: 0.03488, lr: 0.000125\n",
      "Val Loss: 0.03648\n",
      "---------\n",
      "Epoch 401/1000, Train Loss: 0.03488, lr: 0.000125\n",
      "Val Loss: 0.03648\n",
      "---------\n",
      "Epoch 402/1000, Train Loss: 0.03488, lr: 0.000125\n",
      "Val Loss: 0.03647\n",
      "---------\n",
      "Epoch 403/1000, Train Loss: 0.03488, lr: 0.000125\n",
      "Val Loss: 0.03647\n",
      "---------\n",
      "Epoch 404/1000, Train Loss: 0.03488, lr: 0.000125\n",
      "Val Loss: 0.03647\n",
      "---------\n",
      "Epoch 405/1000, Train Loss: 0.03488, lr: 0.000125\n",
      "Val Loss: 0.03647\n",
      "---------\n",
      "Epoch 406/1000, Train Loss: 0.03487, lr: 0.000125\n",
      "Val Loss: 0.03647\n",
      "---------\n",
      "Epoch 407/1000, Train Loss: 0.03487, lr: 0.000125\n",
      "Val Loss: 0.03647\n",
      "---------\n",
      "Epoch 408/1000, Train Loss: 0.03487, lr: 0.000125\n",
      "Val Loss: 0.03647\n",
      "---------\n",
      "Epoch 409/1000, Train Loss: 0.03487, lr: 0.000125\n",
      "Val Loss: 0.03647\n",
      "---------\n",
      "Epoch 410/1000, Train Loss: 0.03487, lr: 0.000125\n",
      "Val Loss: 0.03647\n",
      "---------\n",
      "Epoch 411/1000, Train Loss: 0.03487, lr: 0.000125\n",
      "Val Loss: 0.03647\n",
      "---------\n",
      "Epoch 412/1000, Train Loss: 0.03487, lr: 0.000125\n",
      "Val Loss: 0.03647\n",
      "---------\n",
      "Epoch 413/1000, Train Loss: 0.03487, lr: 0.000125\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 414/1000, Train Loss: 0.03487, lr: 0.000125\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 415/1000, Train Loss: 0.03487, lr: 0.000125\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 416/1000, Train Loss: 0.03486, lr: 0.000125\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 417/1000, Train Loss: 0.03486, lr: 0.000125\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 418/1000, Train Loss: 0.03486, lr: 0.000125\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 419/1000, Train Loss: 0.03486, lr: 0.000125\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 420/1000, Train Loss: 0.03486, lr: 0.000125\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 421/1000, Train Loss: 0.03486, lr: 0.000125\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 422/1000, Train Loss: 0.03486, lr: 0.000125\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 423/1000, Train Loss: 0.03486, lr: 0.000125\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 424/1000, Train Loss: 0.03486, lr: 0.000125\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 425/1000, Train Loss: 0.03486, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 426/1000, Train Loss: 0.03486, lr: 0.000125\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 427/1000, Train Loss: 0.03485, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 428/1000, Train Loss: 0.03485, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 429/1000, Train Loss: 0.03485, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 430/1000, Train Loss: 0.03485, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 431/1000, Train Loss: 0.03485, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 432/1000, Train Loss: 0.03485, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 433/1000, Train Loss: 0.03485, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 434/1000, Train Loss: 0.03485, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 435/1000, Train Loss: 0.03485, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 436/1000, Train Loss: 0.03485, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 437/1000, Train Loss: 0.03485, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 438/1000, Train Loss: 0.03484, lr: 0.000125\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 439/1000, Train Loss: 0.03484, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 440/1000, Train Loss: 0.03484, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 441/1000, Train Loss: 0.03484, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 442/1000, Train Loss: 0.03484, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 443/1000, Train Loss: 0.03484, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 444/1000, Train Loss: 0.03484, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 445/1000, Train Loss: 0.03484, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 446/1000, Train Loss: 0.03484, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 447/1000, Train Loss: 0.03484, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 448/1000, Train Loss: 0.03484, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 449/1000, Train Loss: 0.03484, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 450/1000, Train Loss: 0.03483, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 451/1000, Train Loss: 0.03483, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 452/1000, Train Loss: 0.03483, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 453/1000, Train Loss: 0.03483, lr: 0.000125\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 454/1000, Train Loss: 0.03483, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 455/1000, Train Loss: 0.03483, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 456/1000, Train Loss: 0.03483, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 457/1000, Train Loss: 0.03483, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 458/1000, Train Loss: 0.03483, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 459/1000, Train Loss: 0.03483, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 460/1000, Train Loss: 0.03483, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 461/1000, Train Loss: 0.03483, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 462/1000, Train Loss: 0.03482, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 463/1000, Train Loss: 0.03482, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 464/1000, Train Loss: 0.03482, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 465/1000, Train Loss: 0.03482, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 466/1000, Train Loss: 0.03482, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 467/1000, Train Loss: 0.03482, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 468/1000, Train Loss: 0.03482, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 469/1000, Train Loss: 0.03482, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 470/1000, Train Loss: 0.03482, lr: 0.000125\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 471/1000, Train Loss: 0.03482, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 472/1000, Train Loss: 0.03482, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 473/1000, Train Loss: 0.03482, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 474/1000, Train Loss: 0.03481, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 475/1000, Train Loss: 0.03481, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 476/1000, Train Loss: 0.03481, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 477/1000, Train Loss: 0.03481, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 478/1000, Train Loss: 0.03481, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 479/1000, Train Loss: 0.03481, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 480/1000, Train Loss: 0.03481, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 481/1000, Train Loss: 0.03481, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 482/1000, Train Loss: 0.03481, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 483/1000, Train Loss: 0.03481, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 484/1000, Train Loss: 0.03481, lr: 0.000125\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 485/1000, Train Loss: 0.03481, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 486/1000, Train Loss: 0.03480, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 487/1000, Train Loss: 0.03480, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 488/1000, Train Loss: 0.03480, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 489/1000, Train Loss: 0.03480, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 490/1000, Train Loss: 0.03480, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 491/1000, Train Loss: 0.03480, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 492/1000, Train Loss: 0.03480, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 493/1000, Train Loss: 0.03480, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 494/1000, Train Loss: 0.03480, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 495/1000, Train Loss: 0.03480, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 496/1000, Train Loss: 0.03480, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 497/1000, Train Loss: 0.03480, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 498/1000, Train Loss: 0.03479, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 499/1000, Train Loss: 0.03479, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 500/1000, Train Loss: 0.03479, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 501/1000, Train Loss: 0.03479, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 502/1000, Train Loss: 0.03479, lr: 0.000125\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 503/1000, Train Loss: 0.03479, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 504/1000, Train Loss: 0.03479, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 505/1000, Train Loss: 0.03479, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 506/1000, Train Loss: 0.03479, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 507/1000, Train Loss: 0.03479, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 508/1000, Train Loss: 0.03479, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 509/1000, Train Loss: 0.03479, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 510/1000, Train Loss: 0.03478, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 511/1000, Train Loss: 0.03478, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 512/1000, Train Loss: 0.03478, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 513/1000, Train Loss: 0.03478, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 514/1000, Train Loss: 0.03478, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 515/1000, Train Loss: 0.03478, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 516/1000, Train Loss: 0.03478, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 517/1000, Train Loss: 0.03478, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 518/1000, Train Loss: 0.03478, lr: 0.000125\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 519/1000, Train Loss: 0.03478, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 520/1000, Train Loss: 0.03478, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 521/1000, Train Loss: 0.03478, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 522/1000, Train Loss: 0.03478, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 523/1000, Train Loss: 0.03477, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 524/1000, Train Loss: 0.03477, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 525/1000, Train Loss: 0.03477, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 526/1000, Train Loss: 0.03477, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 527/1000, Train Loss: 0.03477, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 528/1000, Train Loss: 0.03477, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 529/1000, Train Loss: 0.03477, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 530/1000, Train Loss: 0.03477, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 531/1000, Train Loss: 0.03477, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 532/1000, Train Loss: 0.03477, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 533/1000, Train Loss: 0.03477, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 534/1000, Train Loss: 0.03477, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 535/1000, Train Loss: 0.03476, lr: 0.000125\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 536/1000, Train Loss: 0.03476, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 537/1000, Train Loss: 0.03476, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 538/1000, Train Loss: 0.03476, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 539/1000, Train Loss: 0.03476, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 540/1000, Train Loss: 0.03476, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 541/1000, Train Loss: 0.03476, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 542/1000, Train Loss: 0.03476, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 543/1000, Train Loss: 0.03476, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 544/1000, Train Loss: 0.03476, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 545/1000, Train Loss: 0.03476, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 546/1000, Train Loss: 0.03476, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 547/1000, Train Loss: 0.03476, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 548/1000, Train Loss: 0.03475, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 549/1000, Train Loss: 0.03475, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 550/1000, Train Loss: 0.03475, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 551/1000, Train Loss: 0.03475, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 552/1000, Train Loss: 0.03475, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 553/1000, Train Loss: 0.03475, lr: 0.000125\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 554/1000, Train Loss: 0.03475, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 555/1000, Train Loss: 0.03475, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 556/1000, Train Loss: 0.03475, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 557/1000, Train Loss: 0.03475, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 558/1000, Train Loss: 0.03475, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 559/1000, Train Loss: 0.03475, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 560/1000, Train Loss: 0.03475, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 561/1000, Train Loss: 0.03475, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 562/1000, Train Loss: 0.03474, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 563/1000, Train Loss: 0.03474, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 564/1000, Train Loss: 0.03474, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 565/1000, Train Loss: 0.03474, lr: 0.000125\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 566/1000, Train Loss: 0.03474, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 567/1000, Train Loss: 0.03474, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 568/1000, Train Loss: 0.03474, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 569/1000, Train Loss: 0.03474, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 570/1000, Train Loss: 0.03474, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 571/1000, Train Loss: 0.03474, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 572/1000, Train Loss: 0.03474, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 573/1000, Train Loss: 0.03474, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 574/1000, Train Loss: 0.03474, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 575/1000, Train Loss: 0.03473, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 576/1000, Train Loss: 0.03473, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 577/1000, Train Loss: 0.03473, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 578/1000, Train Loss: 0.03473, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 579/1000, Train Loss: 0.03473, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 580/1000, Train Loss: 0.03473, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 581/1000, Train Loss: 0.03473, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 582/1000, Train Loss: 0.03473, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 583/1000, Train Loss: 0.03473, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 584/1000, Train Loss: 0.03473, lr: 0.000125\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 585/1000, Train Loss: 0.03473, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 586/1000, Train Loss: 0.03473, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 587/1000, Train Loss: 0.03472, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 588/1000, Train Loss: 0.03472, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 589/1000, Train Loss: 0.03472, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 590/1000, Train Loss: 0.03472, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 591/1000, Train Loss: 0.03472, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 592/1000, Train Loss: 0.03472, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 593/1000, Train Loss: 0.03472, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 594/1000, Train Loss: 0.03472, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 595/1000, Train Loss: 0.03472, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 596/1000, Train Loss: 0.03472, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 597/1000, Train Loss: 0.03472, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 598/1000, Train Loss: 0.03472, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 599/1000, Train Loss: 0.03472, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 600/1000, Train Loss: 0.03472, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 601/1000, Train Loss: 0.03471, lr: 0.000125\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 602/1000, Train Loss: 0.03471, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 603/1000, Train Loss: 0.03471, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 604/1000, Train Loss: 0.03471, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 605/1000, Train Loss: 0.03471, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 606/1000, Train Loss: 0.03471, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 607/1000, Train Loss: 0.03471, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 608/1000, Train Loss: 0.03471, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 609/1000, Train Loss: 0.03471, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 610/1000, Train Loss: 0.03471, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 611/1000, Train Loss: 0.03471, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 612/1000, Train Loss: 0.03471, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 613/1000, Train Loss: 0.03471, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 614/1000, Train Loss: 0.03470, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 615/1000, Train Loss: 0.03470, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 616/1000, Train Loss: 0.03470, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 617/1000, Train Loss: 0.03470, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 618/1000, Train Loss: 0.03470, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 619/1000, Train Loss: 0.03470, lr: 0.000125\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 620/1000, Train Loss: 0.03470, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 621/1000, Train Loss: 0.03470, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 622/1000, Train Loss: 0.03470, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 623/1000, Train Loss: 0.03470, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 624/1000, Train Loss: 0.03470, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 625/1000, Train Loss: 0.03470, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 626/1000, Train Loss: 0.03470, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 627/1000, Train Loss: 0.03470, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 628/1000, Train Loss: 0.03470, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 629/1000, Train Loss: 0.03470, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 630/1000, Train Loss: 0.03469, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 631/1000, Train Loss: 0.03469, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 632/1000, Train Loss: 0.03469, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 633/1000, Train Loss: 0.03469, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 634/1000, Train Loss: 0.03469, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 635/1000, Train Loss: 0.03469, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 636/1000, Train Loss: 0.03469, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 637/1000, Train Loss: 0.03469, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 638/1000, Train Loss: 0.03469, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 639/1000, Train Loss: 0.03469, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 640/1000, Train Loss: 0.03469, lr: 0.000125\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 641/1000, Train Loss: 0.03469, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 642/1000, Train Loss: 0.03469, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 643/1000, Train Loss: 0.03468, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 644/1000, Train Loss: 0.03468, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 645/1000, Train Loss: 0.03468, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 646/1000, Train Loss: 0.03468, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 647/1000, Train Loss: 0.03468, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 648/1000, Train Loss: 0.03468, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 649/1000, Train Loss: 0.03468, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 650/1000, Train Loss: 0.03468, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 651/1000, Train Loss: 0.03468, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 652/1000, Train Loss: 0.03468, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 653/1000, Train Loss: 0.03468, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 654/1000, Train Loss: 0.03468, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 655/1000, Train Loss: 0.03468, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 656/1000, Train Loss: 0.03468, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 657/1000, Train Loss: 0.03468, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 658/1000, Train Loss: 0.03467, lr: 0.000125\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 659/1000, Train Loss: 0.03467, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 660/1000, Train Loss: 0.03467, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 661/1000, Train Loss: 0.03467, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 662/1000, Train Loss: 0.03467, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 663/1000, Train Loss: 0.03467, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 664/1000, Train Loss: 0.03467, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 665/1000, Train Loss: 0.03467, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 666/1000, Train Loss: 0.03467, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 667/1000, Train Loss: 0.03467, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 668/1000, Train Loss: 0.03467, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 669/1000, Train Loss: 0.03467, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 670/1000, Train Loss: 0.03467, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 671/1000, Train Loss: 0.03467, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 672/1000, Train Loss: 0.03467, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 673/1000, Train Loss: 0.03466, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 674/1000, Train Loss: 0.03466, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 675/1000, Train Loss: 0.03466, lr: 0.000125\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 676/1000, Train Loss: 0.03466, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 677/1000, Train Loss: 0.03466, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 678/1000, Train Loss: 0.03466, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 679/1000, Train Loss: 0.03466, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 680/1000, Train Loss: 0.03466, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 681/1000, Train Loss: 0.03466, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 682/1000, Train Loss: 0.03466, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 683/1000, Train Loss: 0.03466, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 684/1000, Train Loss: 0.03466, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 685/1000, Train Loss: 0.03466, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 686/1000, Train Loss: 0.03466, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 687/1000, Train Loss: 0.03466, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 688/1000, Train Loss: 0.03465, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 689/1000, Train Loss: 0.03465, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 690/1000, Train Loss: 0.03465, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 691/1000, Train Loss: 0.03465, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 692/1000, Train Loss: 0.03465, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 693/1000, Train Loss: 0.03465, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 694/1000, Train Loss: 0.03465, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 695/1000, Train Loss: 0.03465, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 696/1000, Train Loss: 0.03465, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 697/1000, Train Loss: 0.03465, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 698/1000, Train Loss: 0.03465, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 699/1000, Train Loss: 0.03465, lr: 0.000125\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 700/1000, Train Loss: 0.03465, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 701/1000, Train Loss: 0.03465, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 702/1000, Train Loss: 0.03465, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 703/1000, Train Loss: 0.03464, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 704/1000, Train Loss: 0.03464, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 705/1000, Train Loss: 0.03464, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 706/1000, Train Loss: 0.03464, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 707/1000, Train Loss: 0.03464, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 708/1000, Train Loss: 0.03464, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 709/1000, Train Loss: 0.03464, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 710/1000, Train Loss: 0.03464, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 711/1000, Train Loss: 0.03464, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 712/1000, Train Loss: 0.03464, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 713/1000, Train Loss: 0.03464, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 714/1000, Train Loss: 0.03464, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 715/1000, Train Loss: 0.03464, lr: 0.000125\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 716/1000, Train Loss: 0.03464, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 717/1000, Train Loss: 0.03463, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 718/1000, Train Loss: 0.03463, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 719/1000, Train Loss: 0.03463, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 720/1000, Train Loss: 0.03463, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 721/1000, Train Loss: 0.03463, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 722/1000, Train Loss: 0.03463, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 723/1000, Train Loss: 0.03463, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 724/1000, Train Loss: 0.03463, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 725/1000, Train Loss: 0.03463, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 726/1000, Train Loss: 0.03463, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 727/1000, Train Loss: 0.03463, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 728/1000, Train Loss: 0.03463, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 729/1000, Train Loss: 0.03463, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 730/1000, Train Loss: 0.03463, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 731/1000, Train Loss: 0.03463, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 732/1000, Train Loss: 0.03462, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 733/1000, Train Loss: 0.03462, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 734/1000, Train Loss: 0.03462, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 735/1000, Train Loss: 0.03462, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 736/1000, Train Loss: 0.03462, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 737/1000, Train Loss: 0.03462, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 738/1000, Train Loss: 0.03462, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 739/1000, Train Loss: 0.03462, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 740/1000, Train Loss: 0.03462, lr: 0.000125\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 741/1000, Train Loss: 0.03462, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 742/1000, Train Loss: 0.03462, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 743/1000, Train Loss: 0.03462, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 744/1000, Train Loss: 0.03462, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 745/1000, Train Loss: 0.03462, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 746/1000, Train Loss: 0.03462, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 747/1000, Train Loss: 0.03462, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 748/1000, Train Loss: 0.03461, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 749/1000, Train Loss: 0.03461, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 750/1000, Train Loss: 0.03461, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 751/1000, Train Loss: 0.03461, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 752/1000, Train Loss: 0.03461, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 753/1000, Train Loss: 0.03461, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 754/1000, Train Loss: 0.03461, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 755/1000, Train Loss: 0.03461, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 756/1000, Train Loss: 0.03461, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 757/1000, Train Loss: 0.03461, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 758/1000, Train Loss: 0.03461, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 759/1000, Train Loss: 0.03461, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 760/1000, Train Loss: 0.03461, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 761/1000, Train Loss: 0.03461, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 762/1000, Train Loss: 0.03461, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 763/1000, Train Loss: 0.03461, lr: 0.000125\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 764/1000, Train Loss: 0.03460, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 765/1000, Train Loss: 0.03460, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 766/1000, Train Loss: 0.03460, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 767/1000, Train Loss: 0.03460, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 768/1000, Train Loss: 0.03460, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 769/1000, Train Loss: 0.03460, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 770/1000, Train Loss: 0.03460, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 771/1000, Train Loss: 0.03460, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 772/1000, Train Loss: 0.03460, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 773/1000, Train Loss: 0.03460, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 774/1000, Train Loss: 0.03460, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 775/1000, Train Loss: 0.03460, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 776/1000, Train Loss: 0.03460, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 777/1000, Train Loss: 0.03460, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 778/1000, Train Loss: 0.03460, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 779/1000, Train Loss: 0.03460, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 780/1000, Train Loss: 0.03459, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 781/1000, Train Loss: 0.03459, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 782/1000, Train Loss: 0.03459, lr: 0.000125\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 783/1000, Train Loss: 0.03459, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 784/1000, Train Loss: 0.03459, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 785/1000, Train Loss: 0.03459, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 786/1000, Train Loss: 0.03459, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 787/1000, Train Loss: 0.03459, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 788/1000, Train Loss: 0.03459, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 789/1000, Train Loss: 0.03459, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 790/1000, Train Loss: 0.03459, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 791/1000, Train Loss: 0.03459, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 792/1000, Train Loss: 0.03459, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 793/1000, Train Loss: 0.03459, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 794/1000, Train Loss: 0.03459, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 795/1000, Train Loss: 0.03459, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 796/1000, Train Loss: 0.03458, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 797/1000, Train Loss: 0.03458, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 798/1000, Train Loss: 0.03458, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 799/1000, Train Loss: 0.03458, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 800/1000, Train Loss: 0.03458, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 801/1000, Train Loss: 0.03458, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 802/1000, Train Loss: 0.03458, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 803/1000, Train Loss: 0.03458, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 804/1000, Train Loss: 0.03458, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 805/1000, Train Loss: 0.03458, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 806/1000, Train Loss: 0.03458, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 807/1000, Train Loss: 0.03458, lr: 0.000125\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 808/1000, Train Loss: 0.03458, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 809/1000, Train Loss: 0.03458, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 810/1000, Train Loss: 0.03458, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 811/1000, Train Loss: 0.03458, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 812/1000, Train Loss: 0.03457, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 813/1000, Train Loss: 0.03457, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 814/1000, Train Loss: 0.03457, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 815/1000, Train Loss: 0.03457, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 816/1000, Train Loss: 0.03457, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 817/1000, Train Loss: 0.03457, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 818/1000, Train Loss: 0.03457, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 819/1000, Train Loss: 0.03457, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 820/1000, Train Loss: 0.03457, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 821/1000, Train Loss: 0.03457, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 822/1000, Train Loss: 0.03457, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 823/1000, Train Loss: 0.03457, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 824/1000, Train Loss: 0.03457, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 825/1000, Train Loss: 0.03457, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 826/1000, Train Loss: 0.03457, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 827/1000, Train Loss: 0.03457, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 828/1000, Train Loss: 0.03457, lr: 0.000125\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 829/1000, Train Loss: 0.03456, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 830/1000, Train Loss: 0.03456, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 831/1000, Train Loss: 0.03456, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 832/1000, Train Loss: 0.03456, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 833/1000, Train Loss: 0.03456, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 834/1000, Train Loss: 0.03456, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 835/1000, Train Loss: 0.03456, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 836/1000, Train Loss: 0.03456, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 837/1000, Train Loss: 0.03456, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 838/1000, Train Loss: 0.03456, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 839/1000, Train Loss: 0.03456, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 840/1000, Train Loss: 0.03456, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 841/1000, Train Loss: 0.03456, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 842/1000, Train Loss: 0.03456, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 843/1000, Train Loss: 0.03456, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 844/1000, Train Loss: 0.03456, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 845/1000, Train Loss: 0.03455, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 846/1000, Train Loss: 0.03455, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 847/1000, Train Loss: 0.03455, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 848/1000, Train Loss: 0.03455, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 849/1000, Train Loss: 0.03455, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 850/1000, Train Loss: 0.03455, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 851/1000, Train Loss: 0.03455, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 852/1000, Train Loss: 0.03455, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 853/1000, Train Loss: 0.03455, lr: 0.000125\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 854/1000, Train Loss: 0.03455, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 855/1000, Train Loss: 0.03455, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 856/1000, Train Loss: 0.03455, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 857/1000, Train Loss: 0.03455, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 858/1000, Train Loss: 0.03455, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 859/1000, Train Loss: 0.03455, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 860/1000, Train Loss: 0.03455, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 861/1000, Train Loss: 0.03454, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 862/1000, Train Loss: 0.03454, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 863/1000, Train Loss: 0.03454, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 864/1000, Train Loss: 0.03454, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 865/1000, Train Loss: 0.03454, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 866/1000, Train Loss: 0.03454, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 867/1000, Train Loss: 0.03454, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 868/1000, Train Loss: 0.03454, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 869/1000, Train Loss: 0.03454, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 870/1000, Train Loss: 0.03454, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 871/1000, Train Loss: 0.03454, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 872/1000, Train Loss: 0.03454, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 873/1000, Train Loss: 0.03454, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 874/1000, Train Loss: 0.03454, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 875/1000, Train Loss: 0.03454, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 876/1000, Train Loss: 0.03454, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 877/1000, Train Loss: 0.03454, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 878/1000, Train Loss: 0.03454, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 879/1000, Train Loss: 0.03453, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 880/1000, Train Loss: 0.03453, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 881/1000, Train Loss: 0.03453, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 882/1000, Train Loss: 0.03453, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 883/1000, Train Loss: 0.03453, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 884/1000, Train Loss: 0.03453, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 885/1000, Train Loss: 0.03453, lr: 0.000125\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 886/1000, Train Loss: 0.03453, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 887/1000, Train Loss: 0.03453, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 888/1000, Train Loss: 0.03453, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 889/1000, Train Loss: 0.03453, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 890/1000, Train Loss: 0.03453, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 891/1000, Train Loss: 0.03453, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 892/1000, Train Loss: 0.03453, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 893/1000, Train Loss: 0.03453, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 894/1000, Train Loss: 0.03453, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 895/1000, Train Loss: 0.03452, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 896/1000, Train Loss: 0.03452, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 897/1000, Train Loss: 0.03452, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 898/1000, Train Loss: 0.03452, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 899/1000, Train Loss: 0.03452, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 900/1000, Train Loss: 0.03452, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 901/1000, Train Loss: 0.03452, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 902/1000, Train Loss: 0.03452, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 903/1000, Train Loss: 0.03452, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 904/1000, Train Loss: 0.03452, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 905/1000, Train Loss: 0.03452, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 906/1000, Train Loss: 0.03452, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 907/1000, Train Loss: 0.03452, lr: 0.000125\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 908/1000, Train Loss: 0.03452, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 909/1000, Train Loss: 0.03452, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 910/1000, Train Loss: 0.03452, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 911/1000, Train Loss: 0.03452, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 912/1000, Train Loss: 0.03452, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 913/1000, Train Loss: 0.03451, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 914/1000, Train Loss: 0.03451, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 915/1000, Train Loss: 0.03451, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 916/1000, Train Loss: 0.03451, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 917/1000, Train Loss: 0.03451, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 918/1000, Train Loss: 0.03451, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 919/1000, Train Loss: 0.03451, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 920/1000, Train Loss: 0.03451, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 921/1000, Train Loss: 0.03451, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 922/1000, Train Loss: 0.03451, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 923/1000, Train Loss: 0.03451, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 924/1000, Train Loss: 0.03451, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 925/1000, Train Loss: 0.03451, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 926/1000, Train Loss: 0.03451, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 927/1000, Train Loss: 0.03451, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 928/1000, Train Loss: 0.03451, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 929/1000, Train Loss: 0.03451, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 930/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 931/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 932/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 933/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 934/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 935/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 936/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 937/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 938/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 939/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 940/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 941/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 942/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 943/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 944/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 945/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 946/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 947/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 948/1000, Train Loss: 0.03450, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 949/1000, Train Loss: 0.03449, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 950/1000, Train Loss: 0.03449, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 951/1000, Train Loss: 0.03449, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 952/1000, Train Loss: 0.03449, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 953/1000, Train Loss: 0.03449, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 954/1000, Train Loss: 0.03449, lr: 0.000125\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 955/1000, Train Loss: 0.03449, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 956/1000, Train Loss: 0.03449, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 957/1000, Train Loss: 0.03449, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 958/1000, Train Loss: 0.03449, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 959/1000, Train Loss: 0.03449, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 960/1000, Train Loss: 0.03449, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 961/1000, Train Loss: 0.03449, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 962/1000, Train Loss: 0.03449, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 963/1000, Train Loss: 0.03449, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 964/1000, Train Loss: 0.03449, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 965/1000, Train Loss: 0.03449, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 966/1000, Train Loss: 0.03449, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 967/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 968/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 969/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 970/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 971/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 972/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 973/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 974/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 975/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 976/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 977/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 978/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 979/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 980/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 981/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 982/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 983/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 984/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 985/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "Epoch 986/1000, Train Loss: 0.03448, lr: 0.000125\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "Epoch 987/1000, Train Loss: 0.03447, lr: 0.000125\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "Epoch 988/1000, Train Loss: 0.03447, lr: 0.000125\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "Epoch 989/1000, Train Loss: 0.03447, lr: 0.000125\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "Epoch 990/1000, Train Loss: 0.03447, lr: 0.000125\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "Epoch 991/1000, Train Loss: 0.03447, lr: 0.000125\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "Epoch 992/1000, Train Loss: 0.03447, lr: 0.000125\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "Epoch 993/1000, Train Loss: 0.03447, lr: 0.000125\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "Epoch 994/1000, Train Loss: 0.03447, lr: 0.000125\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "Epoch 995/1000, Train Loss: 0.03447, lr: 0.000125\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "Epoch 996/1000, Train Loss: 0.03447, lr: 0.000125\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "Epoch 997/1000, Train Loss: 0.03447, lr: 0.000125\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "Epoch 998/1000, Train Loss: 0.03447, lr: 0.000125\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "Epoch 999/1000, Train Loss: 0.03447, lr: 0.000125\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "Epoch 1000/1000, Train Loss: 0.03447, lr: 0.000125\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "5194.871256828308 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1461, 32, 48, 6, 1), y_hat_i: (5, 32, 48, 6, 1), y_i: (5, 32, 48, 6, 1), batch.x: torch.Size([160, 48, 3, 6]), y: (1461, 32, 48, 6, 1)\n",
      "RMSE for t2m: 2.4819216234434256; MAE for t2m: 1.8710852329818406;\n",
      "RMSE for sp: 1.2948400775940152; MAE for sp: 0.9716134109607313;\n",
      "RMSE for tcc: 0.297561044223986; MAE for tcc: 0.2018341077823903;\n",
      "RMSE for u10: 1.308227924890908; MAE for u10: 0.9721107230326415;\n",
      "RMSE for v10: 1.2930173525795576; MAE for v10: 0.9574715373530243;\n",
      "RMSE for tp: 0.286665045347692; MAE for tp: 0.07727209442644001;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 3, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1461, 32, 48, 6, 1), y_hat_i: (5, 32, 48, 6, 1), y_i: (5, 32, 48, 6, 1), batch.x: torch.Size([160, 48, 3, 6]), y: (1461, 32, 48, 6, 1)\n",
      "RMSE for t2m: 2.4819216234434256; MAE for t2m: 1.8710852329818406;\n",
      "RMSE for sp: 1.2948400775940152; MAE for sp: 0.9716134109607313;\n",
      "RMSE for tcc: 0.29738422882605525; MAE for tcc: 0.201195995270859;\n",
      "RMSE for u10: 1.308227924890908; MAE for u10: 0.9721107230326415;\n",
      "RMSE for v10: 1.2930173525795576; MAE for v10: 0.9574715373530243;\n",
      "RMSE for tp: 0.286665045347692; MAE for tp: 0.07727209442644001;\n",
      "Epoch 1/1000, Train Loss: 0.07086, lr: 0.001--------------------------| 27.3% Complete\n",
      "Val Loss: 0.06015\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.05768, lr: 0.001\n",
      "Val Loss: 0.05590\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.05482, lr: 0.001\n",
      "Val Loss: 0.05310\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.05134, lr: 0.001\n",
      "Val Loss: 0.05016\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.04945, lr: 0.001\n",
      "Val Loss: 0.04904\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.04832, lr: 0.001\n",
      "Val Loss: 0.04815\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.04708, lr: 0.001\n",
      "Val Loss: 0.04671\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.04583, lr: 0.001\n",
      "Val Loss: 0.04559\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.04495, lr: 0.001\n",
      "Val Loss: 0.04520\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.04436, lr: 0.001\n",
      "Val Loss: 0.04484\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.04386, lr: 0.001\n",
      "Val Loss: 0.04449\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.04291, lr: 0.001\n",
      "Val Loss: 0.04365\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.04109, lr: 0.001\n",
      "Val Loss: 0.04198\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.04011, lr: 0.001\n",
      "Val Loss: 0.04138\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.03961, lr: 0.001\n",
      "Val Loss: 0.04107\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.03927, lr: 0.001\n",
      "Val Loss: 0.04087\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.03903, lr: 0.001\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.03879, lr: 0.001\n",
      "Val Loss: 0.04023\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.03858, lr: 0.001\n",
      "Val Loss: 0.04000\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.03840, lr: 0.001\n",
      "Val Loss: 0.03976\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.03823, lr: 0.001\n",
      "Val Loss: 0.03960\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.03809, lr: 0.001\n",
      "Val Loss: 0.03933\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.03795, lr: 0.001\n",
      "Val Loss: 0.03909\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.03781, lr: 0.001\n",
      "Val Loss: 0.03889\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.03767, lr: 0.001\n",
      "Val Loss: 0.03875\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.03758, lr: 0.001\n",
      "Val Loss: 0.03854\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.03746, lr: 0.001\n",
      "Val Loss: 0.03836\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.03738, lr: 0.001\n",
      "Val Loss: 0.03821\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.03730, lr: 0.001\n",
      "Val Loss: 0.03817\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.03721, lr: 0.001\n",
      "Val Loss: 0.03809\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.03713, lr: 0.001\n",
      "Val Loss: 0.03802\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.03705, lr: 0.001\n",
      "Val Loss: 0.03800\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.03699, lr: 0.001\n",
      "Val Loss: 0.03792\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.03692, lr: 0.001\n",
      "Val Loss: 0.03786\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.03686, lr: 0.001\n",
      "Val Loss: 0.03778\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.03679, lr: 0.001\n",
      "Val Loss: 0.03769\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.03672, lr: 0.001\n",
      "Val Loss: 0.03772\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.03667, lr: 0.001\n",
      "Val Loss: 0.03765\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.03662, lr: 0.001\n",
      "Val Loss: 0.03764\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.03656, lr: 0.001\n",
      "Val Loss: 0.03761\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.03648, lr: 0.001\n",
      "Val Loss: 0.03761\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.03643, lr: 0.001\n",
      "Val Loss: 0.03757\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.03637, lr: 0.001\n",
      "Val Loss: 0.03757\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.03631, lr: 0.001\n",
      "Val Loss: 0.03752\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.03625, lr: 0.001\n",
      "Val Loss: 0.03743\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.03621, lr: 0.001\n",
      "Val Loss: 0.03738\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.03616, lr: 0.001\n",
      "Val Loss: 0.03738\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.03611, lr: 0.001\n",
      "Val Loss: 0.03733\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.03607, lr: 0.001\n",
      "Val Loss: 0.03731\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.03603, lr: 0.001\n",
      "Val Loss: 0.03727\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.03598, lr: 0.001\n",
      "Val Loss: 0.03721\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.03594, lr: 0.001\n",
      "Val Loss: 0.03721\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.03590, lr: 0.001\n",
      "Val Loss: 0.03721\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.03588, lr: 0.001\n",
      "Val Loss: 0.03722\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.03585, lr: 0.001\n",
      "Val Loss: 0.03718\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.03580, lr: 0.001\n",
      "Val Loss: 0.03718\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.03577, lr: 0.001\n",
      "Val Loss: 0.03717\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.03575, lr: 0.001\n",
      "Val Loss: 0.03718\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.03572, lr: 0.001\n",
      "Val Loss: 0.03716\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.03568, lr: 0.001\n",
      "Val Loss: 0.03711\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.03565, lr: 0.001\n",
      "Val Loss: 0.03709\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.03560, lr: 0.001\n",
      "Val Loss: 0.03711\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.03557, lr: 0.001\n",
      "Val Loss: 0.03708\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.03554, lr: 0.001\n",
      "Val Loss: 0.03707\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.03552, lr: 0.001\n",
      "Val Loss: 0.03708\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.03549, lr: 0.001\n",
      "Val Loss: 0.03705\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.03546, lr: 0.001\n",
      "Val Loss: 0.03697\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.03542, lr: 0.001\n",
      "Val Loss: 0.03699\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.03540, lr: 0.001\n",
      "Val Loss: 0.03691\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.03537, lr: 0.001\n",
      "Val Loss: 0.03689\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.03533, lr: 0.001\n",
      "Val Loss: 0.03680\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.03530, lr: 0.001\n",
      "Val Loss: 0.03685\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.03527, lr: 0.001\n",
      "Val Loss: 0.03678\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.03524, lr: 0.001\n",
      "Val Loss: 0.03673\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.03522, lr: 0.001\n",
      "Val Loss: 0.03677\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.03519, lr: 0.001\n",
      "Val Loss: 0.03672\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.03516, lr: 0.001\n",
      "Val Loss: 0.03670\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.03516, lr: 0.001\n",
      "Val Loss: 0.03668\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.03512, lr: 0.001\n",
      "Val Loss: 0.03669\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.03511, lr: 0.001\n",
      "Val Loss: 0.03670\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.03509, lr: 0.001\n",
      "Val Loss: 0.03663\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.03506, lr: 0.001\n",
      "Val Loss: 0.03666\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.03504, lr: 0.001\n",
      "Val Loss: 0.03663\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.03501, lr: 0.001\n",
      "Val Loss: 0.03670\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.03499, lr: 0.001\n",
      "Val Loss: 0.03665\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.03496, lr: 0.001\n",
      "Val Loss: 0.03665\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.03493, lr: 0.001\n",
      "Val Loss: 0.03665\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.03493, lr: 0.001\n",
      "Val Loss: 0.03659\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.03490, lr: 0.001\n",
      "Val Loss: 0.03654\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.03487, lr: 0.001\n",
      "Val Loss: 0.03663\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.03485, lr: 0.001\n",
      "Val Loss: 0.03656\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.03483, lr: 0.001\n",
      "Val Loss: 0.03663\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.03482, lr: 0.001\n",
      "Val Loss: 0.03660\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.03479, lr: 0.001\n",
      "Val Loss: 0.03665\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.03475, lr: 0.001\n",
      "Val Loss: 0.03664\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.03474, lr: 0.001\n",
      "Val Loss: 0.03669\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 97/1000, Train Loss: 0.03389, lr: 0.0005\n",
      "Val Loss: 0.03537\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.03379, lr: 0.0005\n",
      "Val Loss: 0.03540\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.03376, lr: 0.0005\n",
      "Val Loss: 0.03539\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.03373, lr: 0.0005\n",
      "Val Loss: 0.03538\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.03370, lr: 0.0005\n",
      "Val Loss: 0.03539\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.03368, lr: 0.0005\n",
      "Val Loss: 0.03540\n",
      "---------\n",
      "Epoch 103/1000, Train Loss: 0.03366, lr: 0.0005\n",
      "Val Loss: 0.03539\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.03363, lr: 0.0005\n",
      "Val Loss: 0.03539\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 105/1000, Train Loss: 0.03319, lr: 0.00025\n",
      "Val Loss: 0.03506\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.03314, lr: 0.00025\n",
      "Val Loss: 0.03501\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.03312, lr: 0.00025\n",
      "Val Loss: 0.03498\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.03310, lr: 0.00025\n",
      "Val Loss: 0.03496\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.03309, lr: 0.00025\n",
      "Val Loss: 0.03495\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.03308, lr: 0.00025\n",
      "Val Loss: 0.03493\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.03306, lr: 0.00025\n",
      "Val Loss: 0.03492\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.03305, lr: 0.00025\n",
      "Val Loss: 0.03490\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.03304, lr: 0.00025\n",
      "Val Loss: 0.03489\n",
      "---------\n",
      "Epoch 114/1000, Train Loss: 0.03303, lr: 0.00025\n",
      "Val Loss: 0.03488\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.03302, lr: 0.00025\n",
      "Val Loss: 0.03488\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.03301, lr: 0.00025\n",
      "Val Loss: 0.03487\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.03299, lr: 0.00025\n",
      "Val Loss: 0.03486\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.03298, lr: 0.00025\n",
      "Val Loss: 0.03485\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.03297, lr: 0.00025\n",
      "Val Loss: 0.03484\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.03296, lr: 0.00025\n",
      "Val Loss: 0.03484\n",
      "---------\n",
      "Epoch 121/1000, Train Loss: 0.03295, lr: 0.00025\n",
      "Val Loss: 0.03483\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.03294, lr: 0.00025\n",
      "Val Loss: 0.03483\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.03293, lr: 0.00025\n",
      "Val Loss: 0.03482\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.03292, lr: 0.00025\n",
      "Val Loss: 0.03482\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.03291, lr: 0.00025\n",
      "Val Loss: 0.03481\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.03290, lr: 0.00025\n",
      "Val Loss: 0.03480\n",
      "---------\n",
      "Epoch 127/1000, Train Loss: 0.03289, lr: 0.00025\n",
      "Val Loss: 0.03480\n",
      "---------\n",
      "Epoch 128/1000, Train Loss: 0.03288, lr: 0.00025\n",
      "Val Loss: 0.03480\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.03287, lr: 0.00025\n",
      "Val Loss: 0.03479\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.03286, lr: 0.00025\n",
      "Val Loss: 0.03479\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.03285, lr: 0.00025\n",
      "Val Loss: 0.03479\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.03284, lr: 0.00025\n",
      "Val Loss: 0.03478\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.03283, lr: 0.00025\n",
      "Val Loss: 0.03478\n",
      "---------\n",
      "Epoch 134/1000, Train Loss: 0.03282, lr: 0.00025\n",
      "Val Loss: 0.03478\n",
      "---------\n",
      "Epoch 135/1000, Train Loss: 0.03281, lr: 0.00025\n",
      "Val Loss: 0.03477\n",
      "---------\n",
      "Epoch 136/1000, Train Loss: 0.03280, lr: 0.00025\n",
      "Val Loss: 0.03477\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.03279, lr: 0.00025\n",
      "Val Loss: 0.03476\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.03278, lr: 0.00025\n",
      "Val Loss: 0.03476\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.03277, lr: 0.00025\n",
      "Val Loss: 0.03476\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.03276, lr: 0.00025\n",
      "Val Loss: 0.03476\n",
      "---------\n",
      "Epoch 141/1000, Train Loss: 0.03275, lr: 0.00025\n",
      "Val Loss: 0.03475\n",
      "---------\n",
      "Epoch 142/1000, Train Loss: 0.03274, lr: 0.00025\n",
      "Val Loss: 0.03475\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.03273, lr: 0.00025\n",
      "Val Loss: 0.03475\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.03272, lr: 0.00025\n",
      "Val Loss: 0.03475\n",
      "---------\n",
      "Epoch 145/1000, Train Loss: 0.03271, lr: 0.00025\n",
      "Val Loss: 0.03475\n",
      "---------\n",
      "Epoch 146/1000, Train Loss: 0.03270, lr: 0.00025\n",
      "Val Loss: 0.03474\n",
      "---------\n",
      "Epoch 147/1000, Train Loss: 0.03269, lr: 0.00025\n",
      "Val Loss: 0.03475\n",
      "---------\n",
      "Epoch 148/1000, Train Loss: 0.03268, lr: 0.00025\n",
      "Val Loss: 0.03474\n",
      "---------\n",
      "Epoch 149/1000, Train Loss: 0.03268, lr: 0.00025\n",
      "Val Loss: 0.03474\n",
      "---------\n",
      "Epoch 150/1000, Train Loss: 0.03267, lr: 0.00025\n",
      "Val Loss: 0.03474\n",
      "---------\n",
      "Epoch 151/1000, Train Loss: 0.03266, lr: 0.00025\n",
      "Val Loss: 0.03474\n",
      "---------\n",
      "Epoch 152/1000, Train Loss: 0.03265, lr: 0.00025\n",
      "Val Loss: 0.03474\n",
      "---------\n",
      "Epoch 153/1000, Train Loss: 0.03264, lr: 0.00025\n",
      "Val Loss: 0.03473\n",
      "---------\n",
      "Epoch 154/1000, Train Loss: 0.03263, lr: 0.00025\n",
      "Val Loss: 0.03473\n",
      "---------\n",
      "Epoch 155/1000, Train Loss: 0.03262, lr: 0.00025\n",
      "Val Loss: 0.03473\n",
      "---------\n",
      "Epoch 156/1000, Train Loss: 0.03262, lr: 0.00025\n",
      "Val Loss: 0.03473\n",
      "---------\n",
      "Epoch 157/1000, Train Loss: 0.03261, lr: 0.00025\n",
      "Val Loss: 0.03473\n",
      "---------\n",
      "Epoch 158/1000, Train Loss: 0.03260, lr: 0.00025\n",
      "Val Loss: 0.03473\n",
      "---------\n",
      "Epoch 159/1000, Train Loss: 0.03259, lr: 0.00025\n",
      "Val Loss: 0.03472\n",
      "---------\n",
      "Epoch 160/1000, Train Loss: 0.03258, lr: 0.00025\n",
      "Val Loss: 0.03472\n",
      "---------\n",
      "Epoch 161/1000, Train Loss: 0.03257, lr: 0.00025\n",
      "Val Loss: 0.03472\n",
      "---------\n",
      "Epoch 162/1000, Train Loss: 0.03257, lr: 0.00025\n",
      "Val Loss: 0.03472\n",
      "---------\n",
      "Epoch 163/1000, Train Loss: 0.03256, lr: 0.00025\n",
      "Val Loss: 0.03473\n",
      "---------\n",
      "Epoch 164/1000, Train Loss: 0.03255, lr: 0.00025\n",
      "Val Loss: 0.03472\n",
      "---------\n",
      "Epoch 165/1000, Train Loss: 0.03254, lr: 0.00025\n",
      "Val Loss: 0.03472\n",
      "---------\n",
      "Epoch 166/1000, Train Loss: 0.03253, lr: 0.00025\n",
      "Val Loss: 0.03472\n",
      "---------\n",
      "Epoch 167/1000, Train Loss: 0.03252, lr: 0.00025\n",
      "Val Loss: 0.03472\n",
      "---------\n",
      "Epoch 168/1000, Train Loss: 0.03252, lr: 0.00025\n",
      "Val Loss: 0.03472\n",
      "---------\n",
      "Epoch 169/1000, Train Loss: 0.03251, lr: 0.00025\n",
      "Val Loss: 0.03472\n",
      "---------\n",
      "Epoch 170/1000, Train Loss: 0.03250, lr: 0.00025\n",
      "Val Loss: 0.03471\n",
      "---------\n",
      "Epoch 171/1000, Train Loss: 0.03249, lr: 0.00025\n",
      "Val Loss: 0.03471\n",
      "---------\n",
      "Epoch 172/1000, Train Loss: 0.03248, lr: 0.00025\n",
      "Val Loss: 0.03472\n",
      "---------\n",
      "Epoch 173/1000, Train Loss: 0.03248, lr: 0.00025\n",
      "Val Loss: 0.03472\n",
      "---------\n",
      "Epoch 174/1000, Train Loss: 0.03247, lr: 0.00025\n",
      "Val Loss: 0.03472\n",
      "---------\n",
      "Epoch 175/1000, Train Loss: 0.03246, lr: 0.00025\n",
      "Val Loss: 0.03471\n",
      "---------\n",
      "Epoch 176/1000, Train Loss: 0.03246, lr: 0.00025\n",
      "Val Loss: 0.03471\n",
      "---------\n",
      "Epoch 177/1000, Train Loss: 0.03245, lr: 0.00025\n",
      "Val Loss: 0.03471\n",
      "---------\n",
      "Epoch 178/1000, Train Loss: 0.03244, lr: 0.00025\n",
      "Val Loss: 0.03471\n",
      "---------\n",
      "Epoch 179/1000, Train Loss: 0.03243, lr: 0.00025\n",
      "Val Loss: 0.03471\n",
      "---------\n",
      "Epoch 180/1000, Train Loss: 0.03243, lr: 0.00025\n",
      "Val Loss: 0.03471\n",
      "---------\n",
      "Epoch 181/1000, Train Loss: 0.03242, lr: 0.00025\n",
      "Val Loss: 0.03471\n",
      "---------\n",
      "Epoch 182/1000, Train Loss: 0.03241, lr: 0.00025\n",
      "Val Loss: 0.03471\n",
      "---------\n",
      "Epoch 183/1000, Train Loss: 0.03240, lr: 0.00025\n",
      "Val Loss: 0.03472\n",
      "---------\n",
      "Epoch 184/1000, Train Loss: 0.03240, lr: 0.00025\n",
      "Val Loss: 0.03471\n",
      "---------\n",
      "Epoch 185/1000, Train Loss: 0.03239, lr: 0.00025\n",
      "Val Loss: 0.03471\n",
      "---------\n",
      "Epoch 186/1000, Train Loss: 0.03238, lr: 0.00025\n",
      "Val Loss: 0.03471\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 187/1000, Train Loss: 0.03220, lr: 0.000125\n",
      "Val Loss: 0.03469\n",
      "---------\n",
      "Epoch 188/1000, Train Loss: 0.03216, lr: 0.000125\n",
      "Val Loss: 0.03468\n",
      "---------\n",
      "Epoch 189/1000, Train Loss: 0.03215, lr: 0.000125\n",
      "Val Loss: 0.03468\n",
      "---------\n",
      "Epoch 190/1000, Train Loss: 0.03214, lr: 0.000125\n",
      "Val Loss: 0.03467\n",
      "---------\n",
      "Epoch 191/1000, Train Loss: 0.03213, lr: 0.000125\n",
      "Val Loss: 0.03467\n",
      "---------\n",
      "Epoch 192/1000, Train Loss: 0.03213, lr: 0.000125\n",
      "Val Loss: 0.03467\n",
      "---------\n",
      "Epoch 193/1000, Train Loss: 0.03212, lr: 0.000125\n",
      "Val Loss: 0.03467\n",
      "---------\n",
      "Epoch 194/1000, Train Loss: 0.03211, lr: 0.000125\n",
      "Val Loss: 0.03467\n",
      "---------\n",
      "Epoch 195/1000, Train Loss: 0.03211, lr: 0.000125\n",
      "Val Loss: 0.03467\n",
      "---------\n",
      "Epoch 196/1000, Train Loss: 0.03210, lr: 0.000125\n",
      "Val Loss: 0.03466\n",
      "---------\n",
      "Epoch 197/1000, Train Loss: 0.03209, lr: 0.000125\n",
      "Val Loss: 0.03466\n",
      "---------\n",
      "Epoch 198/1000, Train Loss: 0.03209, lr: 0.000125\n",
      "Val Loss: 0.03466\n",
      "---------\n",
      "Epoch 199/1000, Train Loss: 0.03208, lr: 0.000125\n",
      "Val Loss: 0.03466\n",
      "---------\n",
      "Epoch 200/1000, Train Loss: 0.03208, lr: 0.000125\n",
      "Val Loss: 0.03466\n",
      "---------\n",
      "Epoch 201/1000, Train Loss: 0.03207, lr: 0.000125\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 202/1000, Train Loss: 0.03206, lr: 0.000125\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 203/1000, Train Loss: 0.03206, lr: 0.000125\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 204/1000, Train Loss: 0.03205, lr: 0.000125\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 205/1000, Train Loss: 0.03205, lr: 0.000125\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 206/1000, Train Loss: 0.03204, lr: 0.000125\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 207/1000, Train Loss: 0.03204, lr: 0.000125\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 208/1000, Train Loss: 0.03203, lr: 0.000125\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 209/1000, Train Loss: 0.03203, lr: 0.000125\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 210/1000, Train Loss: 0.03202, lr: 0.000125\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 211/1000, Train Loss: 0.03202, lr: 0.000125\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 212/1000, Train Loss: 0.03201, lr: 0.000125\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 213/1000, Train Loss: 0.03200, lr: 0.000125\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 214/1000, Train Loss: 0.03200, lr: 0.000125\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 215/1000, Train Loss: 0.03199, lr: 0.000125\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 216/1000, Train Loss: 0.03199, lr: 0.000125\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 217/1000, Train Loss: 0.03198, lr: 0.000125\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 218/1000, Train Loss: 0.03198, lr: 0.000125\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 219/1000, Train Loss: 0.03197, lr: 0.000125\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 220/1000, Train Loss: 0.03197, lr: 0.000125\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 221/1000, Train Loss: 0.03196, lr: 0.000125\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 222/1000, Train Loss: 0.03196, lr: 0.000125\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 223/1000, Train Loss: 0.03195, lr: 0.000125\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 224/1000, Train Loss: 0.03195, lr: 0.000125\n",
      "Val Loss: 0.03463\n",
      "---------\n",
      "Epoch 225/1000, Train Loss: 0.03194, lr: 0.000125\n",
      "Val Loss: 0.03463\n",
      "---------\n",
      "Epoch 226/1000, Train Loss: 0.03194, lr: 0.000125\n",
      "Val Loss: 0.03463\n",
      "---------\n",
      "Epoch 227/1000, Train Loss: 0.03194, lr: 0.000125\n",
      "Val Loss: 0.03463\n",
      "---------\n",
      "Epoch 228/1000, Train Loss: 0.03193, lr: 0.000125\n",
      "Val Loss: 0.03463\n",
      "---------\n",
      "Epoch 229/1000, Train Loss: 0.03193, lr: 0.000125\n",
      "Val Loss: 0.03463\n",
      "---------\n",
      "Epoch 230/1000, Train Loss: 0.03192, lr: 0.000125\n",
      "Val Loss: 0.03463\n",
      "---------\n",
      "Epoch 231/1000, Train Loss: 0.03192, lr: 0.000125\n",
      "Val Loss: 0.03463\n",
      "---------\n",
      "Epoch 232/1000, Train Loss: 0.03191, lr: 0.000125\n",
      "Val Loss: 0.03463\n",
      "---------\n",
      "Epoch 233/1000, Train Loss: 0.03191, lr: 0.000125\n",
      "Val Loss: 0.03463\n",
      "---------\n",
      "Epoch 234/1000, Train Loss: 0.03190, lr: 0.000125\n",
      "Val Loss: 0.03463\n",
      "---------\n",
      "Epoch 235/1000, Train Loss: 0.03190, lr: 0.000125\n",
      "Val Loss: 0.03463\n",
      "---------\n",
      "Epoch 236/1000, Train Loss: 0.03189, lr: 0.000125\n",
      "Val Loss: 0.03463\n",
      "---------\n",
      "Epoch 237/1000, Train Loss: 0.03189, lr: 0.000125\n",
      "Val Loss: 0.03463\n",
      "---------\n",
      "Epoch 238/1000, Train Loss: 0.03189, lr: 0.000125\n",
      "Val Loss: 0.03463\n",
      "---------\n",
      "Epoch 239/1000, Train Loss: 0.03188, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 240/1000, Train Loss: 0.03188, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 241/1000, Train Loss: 0.03187, lr: 0.000125\n",
      "Val Loss: 0.03463\n",
      "---------\n",
      "Epoch 242/1000, Train Loss: 0.03187, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 243/1000, Train Loss: 0.03186, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 244/1000, Train Loss: 0.03186, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 245/1000, Train Loss: 0.03186, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 246/1000, Train Loss: 0.03185, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 247/1000, Train Loss: 0.03185, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 248/1000, Train Loss: 0.03184, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 249/1000, Train Loss: 0.03184, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 250/1000, Train Loss: 0.03183, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 251/1000, Train Loss: 0.03183, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 252/1000, Train Loss: 0.03182, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 253/1000, Train Loss: 0.03182, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 254/1000, Train Loss: 0.03182, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 255/1000, Train Loss: 0.03181, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 256/1000, Train Loss: 0.03181, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 257/1000, Train Loss: 0.03180, lr: 0.000125\n",
      "Val Loss: 0.03461\n",
      "---------\n",
      "Epoch 258/1000, Train Loss: 0.03180, lr: 0.000125\n",
      "Val Loss: 0.03461\n",
      "---------\n",
      "Epoch 259/1000, Train Loss: 0.03179, lr: 0.000125\n",
      "Val Loss: 0.03461\n",
      "---------\n",
      "Epoch 260/1000, Train Loss: 0.03179, lr: 0.000125\n",
      "Val Loss: 0.03461\n",
      "---------\n",
      "Epoch 261/1000, Train Loss: 0.03179, lr: 0.000125\n",
      "Val Loss: 0.03461\n",
      "---------\n",
      "Epoch 262/1000, Train Loss: 0.03178, lr: 0.000125\n",
      "Val Loss: 0.03461\n",
      "---------\n",
      "Epoch 263/1000, Train Loss: 0.03178, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 264/1000, Train Loss: 0.03177, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 265/1000, Train Loss: 0.03177, lr: 0.000125\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 266/1000, Train Loss: 0.03167, lr: 6.25e-05\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 267/1000, Train Loss: 0.03166, lr: 6.25e-05\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 268/1000, Train Loss: 0.03165, lr: 6.25e-05\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 269/1000, Train Loss: 0.03165, lr: 6.25e-05\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 270/1000, Train Loss: 0.03164, lr: 6.25e-05\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 271/1000, Train Loss: 0.03164, lr: 6.25e-05\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 272/1000, Train Loss: 0.03164, lr: 6.25e-05\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 273/1000, Train Loss: 0.03163, lr: 6.25e-05\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 274/1000, Train Loss: 0.03163, lr: 6.25e-05\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 275/1000, Train Loss: 0.03159, lr: 3.125e-05\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 276/1000, Train Loss: 0.03158, lr: 3.125e-05\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 277/1000, Train Loss: 0.03158, lr: 3.125e-05\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 278/1000, Train Loss: 0.03157, lr: 3.125e-05\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 279/1000, Train Loss: 0.03157, lr: 3.125e-05\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 280/1000, Train Loss: 0.03157, lr: 3.125e-05\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 281/1000, Train Loss: 0.03157, lr: 3.125e-05\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 282/1000, Train Loss: 0.03156, lr: 3.125e-05\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 283/1000, Train Loss: 0.03156, lr: 3.125e-05\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 284/1000, Train Loss: 0.03156, lr: 3.125e-05\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 285/1000, Train Loss: 0.03156, lr: 3.125e-05\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 286/1000, Train Loss: 0.03156, lr: 3.125e-05\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 287/1000, Train Loss: 0.03155, lr: 3.125e-05\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 288/1000, Train Loss: 0.03155, lr: 3.125e-05\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 289/1000, Train Loss: 0.03155, lr: 3.125e-05\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 290/1000, Train Loss: 0.03152, lr: 1.5625e-05\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 291/1000, Train Loss: 0.03151, lr: 1.5625e-05\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 292/1000, Train Loss: 0.03151, lr: 1.5625e-05\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 293/1000, Train Loss: 0.03151, lr: 1.5625e-05\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 294/1000, Train Loss: 0.03151, lr: 1.5625e-05\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 295/1000, Train Loss: 0.03151, lr: 1.5625e-05\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 296/1000, Train Loss: 0.03150, lr: 1.5625e-05\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 297/1000, Train Loss: 0.03150, lr: 1.5625e-05\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 298/1000, Train Loss: 0.03148, lr: 7.8125e-06\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 299/1000, Train Loss: 0.03148, lr: 7.8125e-06\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 300/1000, Train Loss: 0.03148, lr: 7.8125e-06\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 301/1000, Train Loss: 0.03148, lr: 7.8125e-06\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 302/1000, Train Loss: 0.03148, lr: 7.8125e-06\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 303/1000, Train Loss: 0.03148, lr: 7.8125e-06\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 304/1000, Train Loss: 0.03148, lr: 7.8125e-06\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 305/1000, Train Loss: 0.03146, lr: 3.90625e-06\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 306/1000, Train Loss: 0.03146, lr: 3.90625e-06\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 307/1000, Train Loss: 0.03146, lr: 3.90625e-06\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 308/1000, Train Loss: 0.03146, lr: 3.90625e-06\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 309/1000, Train Loss: 0.03146, lr: 3.90625e-06\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 310/1000, Train Loss: 0.03146, lr: 3.90625e-06\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 311/1000, Train Loss: 0.03146, lr: 3.90625e-06\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 312/1000, Train Loss: 0.03145, lr: 1.953125e-06\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 313/1000, Train Loss: 0.03145, lr: 1.953125e-06\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 314/1000, Train Loss: 0.03145, lr: 1.953125e-06\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 315/1000, Train Loss: 0.03145, lr: 1.953125e-06\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 316/1000, Train Loss: 0.03145, lr: 1.953125e-06\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 317/1000, Train Loss: 0.03145, lr: 1.953125e-06\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 318/1000, Train Loss: 0.03145, lr: 1.953125e-06\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 319/1000, Train Loss: 0.03144, lr: 9.765625e-07\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 320/1000, Train Loss: 0.03144, lr: 9.765625e-07\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 321/1000, Train Loss: 0.03144, lr: 9.765625e-07\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 322/1000, Train Loss: 0.03144, lr: 9.765625e-07\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 323/1000, Train Loss: 0.03144, lr: 9.765625e-07\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 324/1000, Train Loss: 0.03144, lr: 9.765625e-07\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 325/1000, Train Loss: 0.03144, lr: 9.765625e-07\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 326/1000, Train Loss: 0.03144, lr: 4.8828125e-07\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 327/1000, Train Loss: 0.03144, lr: 4.8828125e-07\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 328/1000, Train Loss: 0.03144, lr: 4.8828125e-07\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 329/1000, Train Loss: 0.03144, lr: 4.8828125e-07\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 330/1000, Train Loss: 0.03144, lr: 4.8828125e-07\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Early stopping ....\n",
      "2070.3031327724457 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1460, 32, 48, 6, 1), y_hat_i: (4, 32, 48, 6, 1), y_i: (4, 32, 48, 6, 1), batch.x: torch.Size([128, 48, 4, 6]), y: (1460, 32, 48, 6, 1)\n",
      "RMSE for t2m: 1.907279574019925; MAE for t2m: 1.4486949041651724;\n",
      "RMSE for sp: 1.6378359266273277; MAE for sp: 1.246571679241581;\n",
      "RMSE for tcc: 0.29556081975487997; MAE for tcc: 0.2041128817318724;\n",
      "RMSE for u10: 1.2569074222186456; MAE for u10: 0.9319756104386183;\n",
      "RMSE for v10: 1.2249928149161138; MAE for v10: 0.9029035042508461;\n",
      "RMSE for tp: 0.2929636782651917; MAE for tp: 0.07937671364915175;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 4, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1460, 32, 48, 6, 1), y_hat_i: (4, 32, 48, 6, 1), y_i: (4, 32, 48, 6, 1), batch.x: torch.Size([128, 48, 4, 6]), y: (1460, 32, 48, 6, 1)\n",
      "RMSE for t2m: 1.907279574019925; MAE for t2m: 1.4486949041651724;\n",
      "RMSE for sp: 1.6378359266273277; MAE for sp: 1.246571679241581;\n",
      "RMSE for tcc: 0.29489803775422796; MAE for tcc: 0.20258192521848392;\n",
      "RMSE for u10: 1.2569074222186456; MAE for u10: 0.9319756104386183;\n",
      "RMSE for v10: 1.2249928149161138; MAE for v10: 0.9029035042508461;\n",
      "RMSE for tp: 0.2929636782651917; MAE for tp: 0.07937671364915175;\n",
      "Epoch 1/1000, Train Loss: 0.06708, lr: 0.001--------------------------| 36.4% Complete\n",
      "Val Loss: 0.05727\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.05083, lr: 0.001\n",
      "Val Loss: 0.04969\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.04754, lr: 0.001\n",
      "Val Loss: 0.04778\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.04594, lr: 0.001\n",
      "Val Loss: 0.04658\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.04490, lr: 0.001\n",
      "Val Loss: 0.04577\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.04392, lr: 0.001\n",
      "Val Loss: 0.04532\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.04176, lr: 0.001\n",
      "Val Loss: 0.04219\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.04032, lr: 0.001\n",
      "Val Loss: 0.04112\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.04016, lr: 0.001\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.03938, lr: 0.001\n",
      "Val Loss: 0.04031\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.03912, lr: 0.001\n",
      "Val Loss: 0.04039\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.03873, lr: 0.001\n",
      "Val Loss: 0.04007\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.03830, lr: 0.001\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.03802, lr: 0.001\n",
      "Val Loss: 0.03935\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.03779, lr: 0.001\n",
      "Val Loss: 0.03907\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.03762, lr: 0.001\n",
      "Val Loss: 0.03890\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.03744, lr: 0.001\n",
      "Val Loss: 0.03874\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.03732, lr: 0.001\n",
      "Val Loss: 0.03861\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.03720, lr: 0.001\n",
      "Val Loss: 0.03848\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.03710, lr: 0.001\n",
      "Val Loss: 0.03828\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.03698, lr: 0.001\n",
      "Val Loss: 0.03822\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.03690, lr: 0.001\n",
      "Val Loss: 0.03811\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.03682, lr: 0.001\n",
      "Val Loss: 0.03806\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.03674, lr: 0.001\n",
      "Val Loss: 0.03806\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.03667, lr: 0.001\n",
      "Val Loss: 0.03809\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.03656, lr: 0.001\n",
      "Val Loss: 0.03807\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.03648, lr: 0.001\n",
      "Val Loss: 0.03797\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.03641, lr: 0.001\n",
      "Val Loss: 0.03802\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.03633, lr: 0.001\n",
      "Val Loss: 0.03789\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.03630, lr: 0.001\n",
      "Val Loss: 0.03793\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.03621, lr: 0.001\n",
      "Val Loss: 0.03771\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.03611, lr: 0.001\n",
      "Val Loss: 0.03767\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.03606, lr: 0.001\n",
      "Val Loss: 0.03760\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.03596, lr: 0.001\n",
      "Val Loss: 0.03742\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.03587, lr: 0.001\n",
      "Val Loss: 0.03753\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.03582, lr: 0.001\n",
      "Val Loss: 0.03741\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.03571, lr: 0.001\n",
      "Val Loss: 0.03736\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.03563, lr: 0.001\n",
      "Val Loss: 0.03727\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.03552, lr: 0.001\n",
      "Val Loss: 0.03716\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.03546, lr: 0.001\n",
      "Val Loss: 0.03708\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.03540, lr: 0.001\n",
      "Val Loss: 0.03706\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.03534, lr: 0.001\n",
      "Val Loss: 0.03689\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.03525, lr: 0.001\n",
      "Val Loss: 0.03682\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.03519, lr: 0.001\n",
      "Val Loss: 0.03672\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.03511, lr: 0.001\n",
      "Val Loss: 0.03669\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.03509, lr: 0.001\n",
      "Val Loss: 0.03662\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.03503, lr: 0.001\n",
      "Val Loss: 0.03655\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.03498, lr: 0.001\n",
      "Val Loss: 0.03653\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.03491, lr: 0.001\n",
      "Val Loss: 0.03652\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.03485, lr: 0.001\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.03479, lr: 0.001\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.03473, lr: 0.001\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.03464, lr: 0.001\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.03463, lr: 0.001\n",
      "Val Loss: 0.03638\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.03459, lr: 0.001\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.03454, lr: 0.001\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.03451, lr: 0.001\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.03445, lr: 0.001\n",
      "Val Loss: 0.03616\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.03443, lr: 0.001\n",
      "Val Loss: 0.03616\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.03434, lr: 0.001\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.03431, lr: 0.001\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.03427, lr: 0.001\n",
      "Val Loss: 0.03632\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.03423, lr: 0.001\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.03423, lr: 0.001\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.03425, lr: 0.001\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 66/1000, Train Loss: 0.03355, lr: 0.0005\n",
      "Val Loss: 0.03569\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.03342, lr: 0.0005\n",
      "Val Loss: 0.03566\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.03337, lr: 0.0005\n",
      "Val Loss: 0.03565\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.03332, lr: 0.0005\n",
      "Val Loss: 0.03567\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.03329, lr: 0.0005\n",
      "Val Loss: 0.03569\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.03325, lr: 0.0005\n",
      "Val Loss: 0.03566\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.03321, lr: 0.0005\n",
      "Val Loss: 0.03566\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.03318, lr: 0.0005\n",
      "Val Loss: 0.03565\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.03314, lr: 0.0005\n",
      "Val Loss: 0.03564\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.03312, lr: 0.0005\n",
      "Val Loss: 0.03560\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.03308, lr: 0.0005\n",
      "Val Loss: 0.03561\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.03304, lr: 0.0005\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.03302, lr: 0.0005\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.03299, lr: 0.0005\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.03298, lr: 0.0005\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.03294, lr: 0.0005\n",
      "Val Loss: 0.03560\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.03294, lr: 0.0005\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.03291, lr: 0.0005\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.03287, lr: 0.0005\n",
      "Val Loss: 0.03563\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.03285, lr: 0.0005\n",
      "Val Loss: 0.03561\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.03280, lr: 0.0005\n",
      "Val Loss: 0.03555\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.03275, lr: 0.0005\n",
      "Val Loss: 0.03547\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.03270, lr: 0.0005\n",
      "Val Loss: 0.03542\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.03266, lr: 0.0005\n",
      "Val Loss: 0.03536\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.03262, lr: 0.0005\n",
      "Val Loss: 0.03537\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.03258, lr: 0.0005\n",
      "Val Loss: 0.03536\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.03254, lr: 0.0005\n",
      "Val Loss: 0.03542\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.03253, lr: 0.0005\n",
      "Val Loss: 0.03536\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.03250, lr: 0.0005\n",
      "Val Loss: 0.03537\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.03248, lr: 0.0005\n",
      "Val Loss: 0.03538\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.03244, lr: 0.0005\n",
      "Val Loss: 0.03536\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 97/1000, Train Loss: 0.03215, lr: 0.00025\n",
      "Val Loss: 0.03554\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.03209, lr: 0.00025\n",
      "Val Loss: 0.03544\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.03204, lr: 0.00025\n",
      "Val Loss: 0.03542\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.03200, lr: 0.00025\n",
      "Val Loss: 0.03540\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.03198, lr: 0.00025\n",
      "Val Loss: 0.03539\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.03195, lr: 0.00025\n",
      "Val Loss: 0.03539\n",
      "---------\n",
      "Epoch 103/1000, Train Loss: 0.03193, lr: 0.00025\n",
      "Val Loss: 0.03537\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 104/1000, Train Loss: 0.03174, lr: 0.000125\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.03167, lr: 0.000125\n",
      "Val Loss: 0.03452\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.03164, lr: 0.000125\n",
      "Val Loss: 0.03451\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.03162, lr: 0.000125\n",
      "Val Loss: 0.03451\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.03160, lr: 0.000125\n",
      "Val Loss: 0.03450\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.03159, lr: 0.000125\n",
      "Val Loss: 0.03450\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.03157, lr: 0.000125\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.03156, lr: 0.000125\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.03155, lr: 0.000125\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.03153, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 114/1000, Train Loss: 0.03152, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.03150, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.03149, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.03148, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.03147, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.03146, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.03145, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 121/1000, Train Loss: 0.03143, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.03142, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.03141, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.03140, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.03139, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.03138, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 127/1000, Train Loss: 0.03137, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 128/1000, Train Loss: 0.03136, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.03135, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.03134, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.03133, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.03132, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.03131, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 134/1000, Train Loss: 0.03130, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 135/1000, Train Loss: 0.03129, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 136/1000, Train Loss: 0.03128, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.03127, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.03126, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.03125, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.03124, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 141/1000, Train Loss: 0.03123, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 142/1000, Train Loss: 0.03123, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.03121, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.03121, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 145/1000, Train Loss: 0.03120, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 146/1000, Train Loss: 0.03119, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 147/1000, Train Loss: 0.03118, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 148/1000, Train Loss: 0.03117, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 149/1000, Train Loss: 0.03116, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 150/1000, Train Loss: 0.03115, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 151/1000, Train Loss: 0.03109, lr: 6.25e-05\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 152/1000, Train Loss: 0.03106, lr: 6.25e-05\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 153/1000, Train Loss: 0.03105, lr: 6.25e-05\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 154/1000, Train Loss: 0.03104, lr: 6.25e-05\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 155/1000, Train Loss: 0.03103, lr: 6.25e-05\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 156/1000, Train Loss: 0.03102, lr: 6.25e-05\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 157/1000, Train Loss: 0.03102, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 158/1000, Train Loss: 0.03101, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 159/1000, Train Loss: 0.03100, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 160/1000, Train Loss: 0.03100, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 161/1000, Train Loss: 0.03099, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 162/1000, Train Loss: 0.03098, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 163/1000, Train Loss: 0.03098, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 164/1000, Train Loss: 0.03097, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 165/1000, Train Loss: 0.03096, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 166/1000, Train Loss: 0.03096, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 167/1000, Train Loss: 0.03095, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 168/1000, Train Loss: 0.03095, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 169/1000, Train Loss: 0.03094, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 170/1000, Train Loss: 0.03094, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 171/1000, Train Loss: 0.03093, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 172/1000, Train Loss: 0.03093, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 173/1000, Train Loss: 0.03092, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 174/1000, Train Loss: 0.03092, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 175/1000, Train Loss: 0.03091, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 176/1000, Train Loss: 0.03091, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "Epoch 177/1000, Train Loss: 0.03090, lr: 6.25e-05\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 178/1000, Train Loss: 0.03086, lr: 3.125e-05\n",
      "Val Loss: 0.03403\n",
      "---------\n",
      "Epoch 179/1000, Train Loss: 0.03085, lr: 3.125e-05\n",
      "Val Loss: 0.03403\n",
      "---------\n",
      "Epoch 180/1000, Train Loss: 0.03084, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 181/1000, Train Loss: 0.03084, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 182/1000, Train Loss: 0.03083, lr: 3.125e-05\n",
      "Val Loss: 0.03403\n",
      "---------\n",
      "Epoch 183/1000, Train Loss: 0.03083, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 184/1000, Train Loss: 0.03083, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 185/1000, Train Loss: 0.03082, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 186/1000, Train Loss: 0.03082, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 187/1000, Train Loss: 0.03081, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 188/1000, Train Loss: 0.03081, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 189/1000, Train Loss: 0.03081, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 190/1000, Train Loss: 0.03080, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 191/1000, Train Loss: 0.03080, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 192/1000, Train Loss: 0.03080, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 193/1000, Train Loss: 0.03079, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 194/1000, Train Loss: 0.03079, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 195/1000, Train Loss: 0.03079, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 196/1000, Train Loss: 0.03079, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 197/1000, Train Loss: 0.03078, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 198/1000, Train Loss: 0.03078, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 199/1000, Train Loss: 0.03078, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 200/1000, Train Loss: 0.03077, lr: 3.125e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 201/1000, Train Loss: 0.03075, lr: 1.5625e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 202/1000, Train Loss: 0.03074, lr: 1.5625e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 203/1000, Train Loss: 0.03074, lr: 1.5625e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 204/1000, Train Loss: 0.03073, lr: 1.5625e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 205/1000, Train Loss: 0.03073, lr: 1.5625e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 206/1000, Train Loss: 0.03073, lr: 1.5625e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 207/1000, Train Loss: 0.03073, lr: 1.5625e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 208/1000, Train Loss: 0.03072, lr: 1.5625e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 209/1000, Train Loss: 0.03072, lr: 1.5625e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 210/1000, Train Loss: 0.03072, lr: 1.5625e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 211/1000, Train Loss: 0.03072, lr: 1.5625e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 212/1000, Train Loss: 0.03072, lr: 1.5625e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 213/1000, Train Loss: 0.03071, lr: 1.5625e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 214/1000, Train Loss: 0.03071, lr: 1.5625e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 215/1000, Train Loss: 0.03070, lr: 7.8125e-06\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 216/1000, Train Loss: 0.03069, lr: 7.8125e-06\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 217/1000, Train Loss: 0.03069, lr: 7.8125e-06\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 218/1000, Train Loss: 0.03069, lr: 7.8125e-06\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 219/1000, Train Loss: 0.03069, lr: 7.8125e-06\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 220/1000, Train Loss: 0.03068, lr: 7.8125e-06\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 221/1000, Train Loss: 0.03068, lr: 7.8125e-06\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 222/1000, Train Loss: 0.03068, lr: 7.8125e-06\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 223/1000, Train Loss: 0.03068, lr: 7.8125e-06\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 224/1000, Train Loss: 0.03068, lr: 7.8125e-06\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 225/1000, Train Loss: 0.03068, lr: 7.8125e-06\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 226/1000, Train Loss: 0.03067, lr: 3.90625e-06\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 227/1000, Train Loss: 0.03066, lr: 3.90625e-06\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 228/1000, Train Loss: 0.03066, lr: 3.90625e-06\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 229/1000, Train Loss: 0.03066, lr: 3.90625e-06\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 230/1000, Train Loss: 0.03066, lr: 3.90625e-06\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 231/1000, Train Loss: 0.03066, lr: 3.90625e-06\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 232/1000, Train Loss: 0.03066, lr: 3.90625e-06\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 233/1000, Train Loss: 0.03066, lr: 3.90625e-06\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 234/1000, Train Loss: 0.03065, lr: 1.953125e-06\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 235/1000, Train Loss: 0.03065, lr: 1.953125e-06\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 236/1000, Train Loss: 0.03065, lr: 1.953125e-06\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 237/1000, Train Loss: 0.03065, lr: 1.953125e-06\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 238/1000, Train Loss: 0.03065, lr: 1.953125e-06\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 239/1000, Train Loss: 0.03065, lr: 1.953125e-06\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 240/1000, Train Loss: 0.03065, lr: 1.953125e-06\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 241/1000, Train Loss: 0.03065, lr: 1.953125e-06\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 242/1000, Train Loss: 0.03064, lr: 9.765625e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 243/1000, Train Loss: 0.03064, lr: 9.765625e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 244/1000, Train Loss: 0.03064, lr: 9.765625e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 245/1000, Train Loss: 0.03064, lr: 9.765625e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 246/1000, Train Loss: 0.03064, lr: 9.765625e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 247/1000, Train Loss: 0.03064, lr: 9.765625e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 248/1000, Train Loss: 0.03064, lr: 9.765625e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 249/1000, Train Loss: 0.03064, lr: 4.8828125e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 250/1000, Train Loss: 0.03064, lr: 4.8828125e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 251/1000, Train Loss: 0.03064, lr: 4.8828125e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 252/1000, Train Loss: 0.03064, lr: 4.8828125e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 253/1000, Train Loss: 0.03064, lr: 4.8828125e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 254/1000, Train Loss: 0.03064, lr: 4.8828125e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 255/1000, Train Loss: 0.03064, lr: 4.8828125e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 256/1000, Train Loss: 0.03064, lr: 2.44140625e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 257/1000, Train Loss: 0.03064, lr: 2.44140625e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 258/1000, Train Loss: 0.03063, lr: 2.44140625e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 259/1000, Train Loss: 0.03063, lr: 2.44140625e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 260/1000, Train Loss: 0.03063, lr: 2.44140625e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 261/1000, Train Loss: 0.03063, lr: 2.44140625e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 262/1000, Train Loss: 0.03063, lr: 2.44140625e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 263/1000, Train Loss: 0.03063, lr: 1.220703125e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 264/1000, Train Loss: 0.03063, lr: 1.220703125e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 265/1000, Train Loss: 0.03063, lr: 1.220703125e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 266/1000, Train Loss: 0.03063, lr: 1.220703125e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 267/1000, Train Loss: 0.03063, lr: 1.220703125e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 268/1000, Train Loss: 0.03063, lr: 1.220703125e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 269/1000, Train Loss: 0.03063, lr: 1.220703125e-07\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 270/1000, Train Loss: 0.03063, lr: 6.103515625e-08\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 271/1000, Train Loss: 0.03063, lr: 6.103515625e-08\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 272/1000, Train Loss: 0.03063, lr: 6.103515625e-08\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 273/1000, Train Loss: 0.03063, lr: 6.103515625e-08\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 274/1000, Train Loss: 0.03063, lr: 6.103515625e-08\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Early stopping ....\n",
      "1752.8297352790833 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1461, 32, 48, 6, 1), y_hat_i: (5, 32, 48, 6, 1), y_i: (5, 32, 48, 6, 1), batch.x: torch.Size([160, 48, 5, 6]), y: (1461, 32, 48, 6, 1)\n",
      "RMSE for t2m: 1.6971831182402868; MAE for t2m: 1.2859540249534993;\n",
      "RMSE for sp: 1.6340364248319261; MAE for sp: 1.228950218484414;\n",
      "RMSE for tcc: 0.29355824319169166; MAE for tcc: 0.19479584680640386;\n",
      "RMSE for u10: 1.3023413865939992; MAE for u10: 0.9628881975194095;\n",
      "RMSE for v10: 1.2478985970687768; MAE for v10: 0.9186619536161902;\n",
      "RMSE for tp: 0.2965736961164429; MAE for tp: 0.07907047814792371;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 5, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1461, 32, 48, 6, 1), y_hat_i: (5, 32, 48, 6, 1), y_i: (5, 32, 48, 6, 1), batch.x: torch.Size([160, 48, 5, 6]), y: (1461, 32, 48, 6, 1)\n",
      "RMSE for t2m: 1.6971831182402868; MAE for t2m: 1.2859540249534993;\n",
      "RMSE for sp: 1.6340364248319261; MAE for sp: 1.228950218484414;\n",
      "RMSE for tcc: 0.2931172973538351; MAE for tcc: 0.1938707891798369;\n",
      "RMSE for u10: 1.3023413865939992; MAE for u10: 0.9628881975194095;\n",
      "RMSE for v10: 1.2478985970687768; MAE for v10: 0.9186619536161902;\n",
      "RMSE for tp: 0.2965736961164429; MAE for tp: 0.07907047814792371;\n",
      "Epoch 1/1000, Train Loss: 0.07166, lr: 0.001--------------------------| 45.5% Complete\n",
      "Val Loss: 0.06217\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.05800, lr: 0.001\n",
      "Val Loss: 0.05834\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.05216, lr: 0.001\n",
      "Val Loss: 0.04956\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.04746, lr: 0.001\n",
      "Val Loss: 0.04731\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.04580, lr: 0.001\n",
      "Val Loss: 0.04649\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.04447, lr: 0.001\n",
      "Val Loss: 0.04481\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.04233, lr: 0.001\n",
      "Val Loss: 0.04328\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.04102, lr: 0.001\n",
      "Val Loss: 0.04182\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.04023, lr: 0.001\n",
      "Val Loss: 0.04093\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.03970, lr: 0.001\n",
      "Val Loss: 0.04030\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.03933, lr: 0.001\n",
      "Val Loss: 0.03984\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.03904, lr: 0.001\n",
      "Val Loss: 0.03954\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.03882, lr: 0.001\n",
      "Val Loss: 0.03931\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.03862, lr: 0.001\n",
      "Val Loss: 0.03914\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.03846, lr: 0.001\n",
      "Val Loss: 0.03902\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.03831, lr: 0.001\n",
      "Val Loss: 0.03894\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.03816, lr: 0.001\n",
      "Val Loss: 0.03885\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.03803, lr: 0.001\n",
      "Val Loss: 0.03880\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.03793, lr: 0.001\n",
      "Val Loss: 0.03871\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.03784, lr: 0.001\n",
      "Val Loss: 0.03858\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.03776, lr: 0.001\n",
      "Val Loss: 0.03845\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.03767, lr: 0.001\n",
      "Val Loss: 0.03835\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.03760, lr: 0.001\n",
      "Val Loss: 0.03827\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.03752, lr: 0.001\n",
      "Val Loss: 0.03819\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.03744, lr: 0.001\n",
      "Val Loss: 0.03814\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.03737, lr: 0.001\n",
      "Val Loss: 0.03806\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.03730, lr: 0.001\n",
      "Val Loss: 0.03801\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.03723, lr: 0.001\n",
      "Val Loss: 0.03794\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.03717, lr: 0.001\n",
      "Val Loss: 0.03787\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.03711, lr: 0.001\n",
      "Val Loss: 0.03781\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.03705, lr: 0.001\n",
      "Val Loss: 0.03772\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.03697, lr: 0.001\n",
      "Val Loss: 0.03765\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.03691, lr: 0.001\n",
      "Val Loss: 0.03758\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.03685, lr: 0.001\n",
      "Val Loss: 0.03751\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.03680, lr: 0.001\n",
      "Val Loss: 0.03745\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.03675, lr: 0.001\n",
      "Val Loss: 0.03740\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.03670, lr: 0.001\n",
      "Val Loss: 0.03736\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.03666, lr: 0.001\n",
      "Val Loss: 0.03732\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.03661, lr: 0.001\n",
      "Val Loss: 0.03729\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.03656, lr: 0.001\n",
      "Val Loss: 0.03725\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.03651, lr: 0.001\n",
      "Val Loss: 0.03720\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.03647, lr: 0.001\n",
      "Val Loss: 0.03716\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.03642, lr: 0.001\n",
      "Val Loss: 0.03713\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.03637, lr: 0.001\n",
      "Val Loss: 0.03707\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.03632, lr: 0.001\n",
      "Val Loss: 0.03704\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.03629, lr: 0.001\n",
      "Val Loss: 0.03702\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.03625, lr: 0.001\n",
      "Val Loss: 0.03706\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.03623, lr: 0.001\n",
      "Val Loss: 0.03712\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.03620, lr: 0.001\n",
      "Val Loss: 0.03711\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.03617, lr: 0.001\n",
      "Val Loss: 0.03715\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.03613, lr: 0.001\n",
      "Val Loss: 0.03715\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.03609, lr: 0.001\n",
      "Val Loss: 0.03719\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.03605, lr: 0.001\n",
      "Val Loss: 0.03713\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 54/1000, Train Loss: 0.03534, lr: 0.0005\n",
      "Val Loss: 0.03698\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.03529, lr: 0.0005\n",
      "Val Loss: 0.03692\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.03525, lr: 0.0005\n",
      "Val Loss: 0.03688\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.03522, lr: 0.0005\n",
      "Val Loss: 0.03683\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.03519, lr: 0.0005\n",
      "Val Loss: 0.03682\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.03517, lr: 0.0005\n",
      "Val Loss: 0.03679\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.03514, lr: 0.0005\n",
      "Val Loss: 0.03677\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.03512, lr: 0.0005\n",
      "Val Loss: 0.03674\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.03510, lr: 0.0005\n",
      "Val Loss: 0.03672\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.03508, lr: 0.0005\n",
      "Val Loss: 0.03670\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.03506, lr: 0.0005\n",
      "Val Loss: 0.03669\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.03503, lr: 0.0005\n",
      "Val Loss: 0.03666\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.03502, lr: 0.0005\n",
      "Val Loss: 0.03665\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.03500, lr: 0.0005\n",
      "Val Loss: 0.03663\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.03498, lr: 0.0005\n",
      "Val Loss: 0.03661\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.03496, lr: 0.0005\n",
      "Val Loss: 0.03660\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.03494, lr: 0.0005\n",
      "Val Loss: 0.03658\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.03492, lr: 0.0005\n",
      "Val Loss: 0.03658\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.03490, lr: 0.0005\n",
      "Val Loss: 0.03656\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.03488, lr: 0.0005\n",
      "Val Loss: 0.03654\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.03486, lr: 0.0005\n",
      "Val Loss: 0.03653\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.03485, lr: 0.0005\n",
      "Val Loss: 0.03652\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.03483, lr: 0.0005\n",
      "Val Loss: 0.03651\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.03481, lr: 0.0005\n",
      "Val Loss: 0.03649\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.03480, lr: 0.0005\n",
      "Val Loss: 0.03649\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.03478, lr: 0.0005\n",
      "Val Loss: 0.03648\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.03477, lr: 0.0005\n",
      "Val Loss: 0.03648\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.03475, lr: 0.0005\n",
      "Val Loss: 0.03647\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.03474, lr: 0.0005\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.03472, lr: 0.0005\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.03471, lr: 0.0005\n",
      "Val Loss: 0.03645\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.03469, lr: 0.0005\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.03468, lr: 0.0005\n",
      "Val Loss: 0.03642\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.03466, lr: 0.0005\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.03465, lr: 0.0005\n",
      "Val Loss: 0.03641\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.03464, lr: 0.0005\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.03462, lr: 0.0005\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.03461, lr: 0.0005\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.03460, lr: 0.0005\n",
      "Val Loss: 0.03637\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.03458, lr: 0.0005\n",
      "Val Loss: 0.03635\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.03457, lr: 0.0005\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.03456, lr: 0.0005\n",
      "Val Loss: 0.03633\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.03455, lr: 0.0005\n",
      "Val Loss: 0.03631\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.03453, lr: 0.0005\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.03452, lr: 0.0005\n",
      "Val Loss: 0.03630\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.03451, lr: 0.0005\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.03450, lr: 0.0005\n",
      "Val Loss: 0.03628\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.03449, lr: 0.0005\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.03448, lr: 0.0005\n",
      "Val Loss: 0.03627\n",
      "---------\n",
      "Epoch 103/1000, Train Loss: 0.03447, lr: 0.0005\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.03446, lr: 0.0005\n",
      "Val Loss: 0.03626\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.03445, lr: 0.0005\n",
      "Val Loss: 0.03624\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.03444, lr: 0.0005\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.03443, lr: 0.0005\n",
      "Val Loss: 0.03623\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.03442, lr: 0.0005\n",
      "Val Loss: 0.03622\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.03441, lr: 0.0005\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.03440, lr: 0.0005\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.03439, lr: 0.0005\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.03438, lr: 0.0005\n",
      "Val Loss: 0.03621\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.03437, lr: 0.0005\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 114/1000, Train Loss: 0.03436, lr: 0.0005\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.03435, lr: 0.0005\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.03434, lr: 0.0005\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.03434, lr: 0.0005\n",
      "Val Loss: 0.03616\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.03433, lr: 0.0005\n",
      "Val Loss: 0.03616\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.03432, lr: 0.0005\n",
      "Val Loss: 0.03615\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.03431, lr: 0.0005\n",
      "Val Loss: 0.03615\n",
      "---------\n",
      "Epoch 121/1000, Train Loss: 0.03430, lr: 0.0005\n",
      "Val Loss: 0.03614\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.03430, lr: 0.0005\n",
      "Val Loss: 0.03613\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.03429, lr: 0.0005\n",
      "Val Loss: 0.03613\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.03428, lr: 0.0005\n",
      "Val Loss: 0.03611\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.03427, lr: 0.0005\n",
      "Val Loss: 0.03610\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.03426, lr: 0.0005\n",
      "Val Loss: 0.03610\n",
      "---------\n",
      "Epoch 127/1000, Train Loss: 0.03426, lr: 0.0005\n",
      "Val Loss: 0.03608\n",
      "---------\n",
      "Epoch 128/1000, Train Loss: 0.03425, lr: 0.0005\n",
      "Val Loss: 0.03608\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.03424, lr: 0.0005\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.03423, lr: 0.0005\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.03423, lr: 0.0005\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.03422, lr: 0.0005\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.03421, lr: 0.0005\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 134/1000, Train Loss: 0.03420, lr: 0.0005\n",
      "Val Loss: 0.03605\n",
      "---------\n",
      "Epoch 135/1000, Train Loss: 0.03420, lr: 0.0005\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 136/1000, Train Loss: 0.03419, lr: 0.0005\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.03418, lr: 0.0005\n",
      "Val Loss: 0.03603\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.03418, lr: 0.0005\n",
      "Val Loss: 0.03602\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.03417, lr: 0.0005\n",
      "Val Loss: 0.03601\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.03416, lr: 0.0005\n",
      "Val Loss: 0.03601\n",
      "---------\n",
      "Epoch 141/1000, Train Loss: 0.03416, lr: 0.0005\n",
      "Val Loss: 0.03600\n",
      "---------\n",
      "Epoch 142/1000, Train Loss: 0.03415, lr: 0.0005\n",
      "Val Loss: 0.03599\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.03414, lr: 0.0005\n",
      "Val Loss: 0.03599\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.03414, lr: 0.0005\n",
      "Val Loss: 0.03598\n",
      "---------\n",
      "Epoch 145/1000, Train Loss: 0.03413, lr: 0.0005\n",
      "Val Loss: 0.03597\n",
      "---------\n",
      "Epoch 146/1000, Train Loss: 0.03413, lr: 0.0005\n",
      "Val Loss: 0.03596\n",
      "---------\n",
      "Epoch 147/1000, Train Loss: 0.03412, lr: 0.0005\n",
      "Val Loss: 0.03595\n",
      "---------\n",
      "Epoch 148/1000, Train Loss: 0.03412, lr: 0.0005\n",
      "Val Loss: 0.03595\n",
      "---------\n",
      "Epoch 149/1000, Train Loss: 0.03411, lr: 0.0005\n",
      "Val Loss: 0.03594\n",
      "---------\n",
      "Epoch 150/1000, Train Loss: 0.03410, lr: 0.0005\n",
      "Val Loss: 0.03594\n",
      "---------\n",
      "Epoch 151/1000, Train Loss: 0.03410, lr: 0.0005\n",
      "Val Loss: 0.03593\n",
      "---------\n",
      "Epoch 152/1000, Train Loss: 0.03409, lr: 0.0005\n",
      "Val Loss: 0.03592\n",
      "---------\n",
      "Epoch 153/1000, Train Loss: 0.03408, lr: 0.0005\n",
      "Val Loss: 0.03591\n",
      "---------\n",
      "Epoch 154/1000, Train Loss: 0.03408, lr: 0.0005\n",
      "Val Loss: 0.03590\n",
      "---------\n",
      "Epoch 155/1000, Train Loss: 0.03407, lr: 0.0005\n",
      "Val Loss: 0.03589\n",
      "---------\n",
      "Epoch 156/1000, Train Loss: 0.03407, lr: 0.0005\n",
      "Val Loss: 0.03589\n",
      "---------\n",
      "Epoch 157/1000, Train Loss: 0.03406, lr: 0.0005\n",
      "Val Loss: 0.03588\n",
      "---------\n",
      "Epoch 158/1000, Train Loss: 0.03405, lr: 0.0005\n",
      "Val Loss: 0.03587\n",
      "---------\n",
      "Epoch 159/1000, Train Loss: 0.03405, lr: 0.0005\n",
      "Val Loss: 0.03587\n",
      "---------\n",
      "Epoch 160/1000, Train Loss: 0.03404, lr: 0.0005\n",
      "Val Loss: 0.03586\n",
      "---------\n",
      "Epoch 161/1000, Train Loss: 0.03403, lr: 0.0005\n",
      "Val Loss: 0.03584\n",
      "---------\n",
      "Epoch 162/1000, Train Loss: 0.03403, lr: 0.0005\n",
      "Val Loss: 0.03586\n",
      "---------\n",
      "Epoch 163/1000, Train Loss: 0.03402, lr: 0.0005\n",
      "Val Loss: 0.03583\n",
      "---------\n",
      "Epoch 164/1000, Train Loss: 0.03402, lr: 0.0005\n",
      "Val Loss: 0.03584\n",
      "---------\n",
      "Epoch 165/1000, Train Loss: 0.03401, lr: 0.0005\n",
      "Val Loss: 0.03584\n",
      "---------\n",
      "Epoch 166/1000, Train Loss: 0.03400, lr: 0.0005\n",
      "Val Loss: 0.03584\n",
      "---------\n",
      "Epoch 167/1000, Train Loss: 0.03400, lr: 0.0005\n",
      "Val Loss: 0.03583\n",
      "---------\n",
      "Epoch 168/1000, Train Loss: 0.03400, lr: 0.0005\n",
      "Val Loss: 0.03583\n",
      "---------\n",
      "Epoch 169/1000, Train Loss: 0.03399, lr: 0.0005\n",
      "Val Loss: 0.03582\n",
      "---------\n",
      "Epoch 170/1000, Train Loss: 0.03398, lr: 0.0005\n",
      "Val Loss: 0.03581\n",
      "---------\n",
      "Epoch 171/1000, Train Loss: 0.03398, lr: 0.0005\n",
      "Val Loss: 0.03581\n",
      "---------\n",
      "Epoch 172/1000, Train Loss: 0.03397, lr: 0.0005\n",
      "Val Loss: 0.03580\n",
      "---------\n",
      "Epoch 173/1000, Train Loss: 0.03397, lr: 0.0005\n",
      "Val Loss: 0.03580\n",
      "---------\n",
      "Epoch 174/1000, Train Loss: 0.03396, lr: 0.0005\n",
      "Val Loss: 0.03580\n",
      "---------\n",
      "Epoch 175/1000, Train Loss: 0.03395, lr: 0.0005\n",
      "Val Loss: 0.03579\n",
      "---------\n",
      "Epoch 176/1000, Train Loss: 0.03395, lr: 0.0005\n",
      "Val Loss: 0.03578\n",
      "---------\n",
      "Epoch 177/1000, Train Loss: 0.03394, lr: 0.0005\n",
      "Val Loss: 0.03578\n",
      "---------\n",
      "Epoch 178/1000, Train Loss: 0.03394, lr: 0.0005\n",
      "Val Loss: 0.03577\n",
      "---------\n",
      "Epoch 179/1000, Train Loss: 0.03393, lr: 0.0005\n",
      "Val Loss: 0.03577\n",
      "---------\n",
      "Epoch 180/1000, Train Loss: 0.03393, lr: 0.0005\n",
      "Val Loss: 0.03577\n",
      "---------\n",
      "Epoch 181/1000, Train Loss: 0.03392, lr: 0.0005\n",
      "Val Loss: 0.03576\n",
      "---------\n",
      "Epoch 182/1000, Train Loss: 0.03391, lr: 0.0005\n",
      "Val Loss: 0.03574\n",
      "---------\n",
      "Epoch 183/1000, Train Loss: 0.03390, lr: 0.0005\n",
      "Val Loss: 0.03575\n",
      "---------\n",
      "Epoch 184/1000, Train Loss: 0.03391, lr: 0.0005\n",
      "Val Loss: 0.03576\n",
      "---------\n",
      "Epoch 185/1000, Train Loss: 0.03390, lr: 0.0005\n",
      "Val Loss: 0.03574\n",
      "---------\n",
      "Epoch 186/1000, Train Loss: 0.03389, lr: 0.0005\n",
      "Val Loss: 0.03571\n",
      "---------\n",
      "Epoch 187/1000, Train Loss: 0.03388, lr: 0.0005\n",
      "Val Loss: 0.03571\n",
      "---------\n",
      "Epoch 188/1000, Train Loss: 0.03388, lr: 0.0005\n",
      "Val Loss: 0.03570\n",
      "---------\n",
      "Epoch 189/1000, Train Loss: 0.03387, lr: 0.0005\n",
      "Val Loss: 0.03570\n",
      "---------\n",
      "Epoch 190/1000, Train Loss: 0.03387, lr: 0.0005\n",
      "Val Loss: 0.03570\n",
      "---------\n",
      "Epoch 191/1000, Train Loss: 0.03387, lr: 0.0005\n",
      "Val Loss: 0.03573\n",
      "---------\n",
      "Epoch 192/1000, Train Loss: 0.03387, lr: 0.0005\n",
      "Val Loss: 0.03571\n",
      "---------\n",
      "Epoch 193/1000, Train Loss: 0.03386, lr: 0.0005\n",
      "Val Loss: 0.03571\n",
      "---------\n",
      "Epoch 194/1000, Train Loss: 0.03385, lr: 0.0005\n",
      "Val Loss: 0.03571\n",
      "---------\n",
      "Epoch 195/1000, Train Loss: 0.03385, lr: 0.0005\n",
      "Val Loss: 0.03571\n",
      "---------\n",
      "Epoch 196/1000, Train Loss: 0.03385, lr: 0.0005\n",
      "Val Loss: 0.03570\n",
      "---------\n",
      "Epoch 197/1000, Train Loss: 0.03384, lr: 0.0005\n",
      "Val Loss: 0.03570\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 198/1000, Train Loss: 0.03345, lr: 0.00025\n",
      "Val Loss: 0.03500\n",
      "---------\n",
      "Epoch 199/1000, Train Loss: 0.03340, lr: 0.00025\n",
      "Val Loss: 0.03500\n",
      "---------\n",
      "Epoch 200/1000, Train Loss: 0.03339, lr: 0.00025\n",
      "Val Loss: 0.03500\n",
      "---------\n",
      "Epoch 201/1000, Train Loss: 0.03338, lr: 0.00025\n",
      "Val Loss: 0.03501\n",
      "---------\n",
      "Epoch 202/1000, Train Loss: 0.03337, lr: 0.00025\n",
      "Val Loss: 0.03501\n",
      "---------\n",
      "Epoch 203/1000, Train Loss: 0.03337, lr: 0.00025\n",
      "Val Loss: 0.03501\n",
      "---------\n",
      "Epoch 204/1000, Train Loss: 0.03336, lr: 0.00025\n",
      "Val Loss: 0.03501\n",
      "---------\n",
      "Epoch 205/1000, Train Loss: 0.03336, lr: 0.00025\n",
      "Val Loss: 0.03502\n",
      "---------\n",
      "Epoch 206/1000, Train Loss: 0.03335, lr: 0.00025\n",
      "Val Loss: 0.03502\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 207/1000, Train Loss: 0.03309, lr: 0.000125\n",
      "Val Loss: 0.03460\n",
      "---------\n",
      "Epoch 208/1000, Train Loss: 0.03306, lr: 0.000125\n",
      "Val Loss: 0.03459\n",
      "---------\n",
      "Epoch 209/1000, Train Loss: 0.03305, lr: 0.000125\n",
      "Val Loss: 0.03458\n",
      "---------\n",
      "Epoch 210/1000, Train Loss: 0.03305, lr: 0.000125\n",
      "Val Loss: 0.03458\n",
      "---------\n",
      "Epoch 211/1000, Train Loss: 0.03305, lr: 0.000125\n",
      "Val Loss: 0.03458\n",
      "---------\n",
      "Epoch 212/1000, Train Loss: 0.03305, lr: 0.000125\n",
      "Val Loss: 0.03458\n",
      "---------\n",
      "Epoch 213/1000, Train Loss: 0.03304, lr: 0.000125\n",
      "Val Loss: 0.03458\n",
      "---------\n",
      "Epoch 214/1000, Train Loss: 0.03304, lr: 0.000125\n",
      "Val Loss: 0.03458\n",
      "---------\n",
      "Epoch 215/1000, Train Loss: 0.03304, lr: 0.000125\n",
      "Val Loss: 0.03457\n",
      "---------\n",
      "Epoch 216/1000, Train Loss: 0.03304, lr: 0.000125\n",
      "Val Loss: 0.03457\n",
      "---------\n",
      "Epoch 217/1000, Train Loss: 0.03303, lr: 0.000125\n",
      "Val Loss: 0.03457\n",
      "---------\n",
      "Epoch 218/1000, Train Loss: 0.03303, lr: 0.000125\n",
      "Val Loss: 0.03457\n",
      "---------\n",
      "Epoch 219/1000, Train Loss: 0.03303, lr: 0.000125\n",
      "Val Loss: 0.03457\n",
      "---------\n",
      "Epoch 220/1000, Train Loss: 0.03303, lr: 0.000125\n",
      "Val Loss: 0.03457\n",
      "---------\n",
      "Epoch 221/1000, Train Loss: 0.03302, lr: 0.000125\n",
      "Val Loss: 0.03456\n",
      "---------\n",
      "Epoch 222/1000, Train Loss: 0.03302, lr: 0.000125\n",
      "Val Loss: 0.03456\n",
      "---------\n",
      "Epoch 223/1000, Train Loss: 0.03302, lr: 0.000125\n",
      "Val Loss: 0.03456\n",
      "---------\n",
      "Epoch 224/1000, Train Loss: 0.03302, lr: 0.000125\n",
      "Val Loss: 0.03456\n",
      "---------\n",
      "Epoch 225/1000, Train Loss: 0.03301, lr: 0.000125\n",
      "Val Loss: 0.03456\n",
      "---------\n",
      "Epoch 226/1000, Train Loss: 0.03301, lr: 0.000125\n",
      "Val Loss: 0.03456\n",
      "---------\n",
      "Epoch 227/1000, Train Loss: 0.03301, lr: 0.000125\n",
      "Val Loss: 0.03456\n",
      "---------\n",
      "Epoch 228/1000, Train Loss: 0.03301, lr: 0.000125\n",
      "Val Loss: 0.03456\n",
      "---------\n",
      "Epoch 229/1000, Train Loss: 0.03300, lr: 0.000125\n",
      "Val Loss: 0.03455\n",
      "---------\n",
      "Epoch 230/1000, Train Loss: 0.03300, lr: 0.000125\n",
      "Val Loss: 0.03455\n",
      "---------\n",
      "Epoch 231/1000, Train Loss: 0.03300, lr: 0.000125\n",
      "Val Loss: 0.03455\n",
      "---------\n",
      "Epoch 232/1000, Train Loss: 0.03300, lr: 0.000125\n",
      "Val Loss: 0.03455\n",
      "---------\n",
      "Epoch 233/1000, Train Loss: 0.03299, lr: 0.000125\n",
      "Val Loss: 0.03455\n",
      "---------\n",
      "Epoch 234/1000, Train Loss: 0.03299, lr: 0.000125\n",
      "Val Loss: 0.03455\n",
      "---------\n",
      "Epoch 235/1000, Train Loss: 0.03299, lr: 0.000125\n",
      "Val Loss: 0.03455\n",
      "---------\n",
      "Epoch 236/1000, Train Loss: 0.03299, lr: 0.000125\n",
      "Val Loss: 0.03455\n",
      "---------\n",
      "Epoch 237/1000, Train Loss: 0.03298, lr: 0.000125\n",
      "Val Loss: 0.03455\n",
      "---------\n",
      "Epoch 238/1000, Train Loss: 0.03298, lr: 0.000125\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 239/1000, Train Loss: 0.03298, lr: 0.000125\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 240/1000, Train Loss: 0.03298, lr: 0.000125\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 241/1000, Train Loss: 0.03297, lr: 0.000125\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 242/1000, Train Loss: 0.03297, lr: 0.000125\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 243/1000, Train Loss: 0.03297, lr: 0.000125\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 244/1000, Train Loss: 0.03297, lr: 0.000125\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 245/1000, Train Loss: 0.03297, lr: 0.000125\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 246/1000, Train Loss: 0.03296, lr: 0.000125\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 247/1000, Train Loss: 0.03296, lr: 0.000125\n",
      "Val Loss: 0.03454\n",
      "---------\n",
      "Epoch 248/1000, Train Loss: 0.03296, lr: 0.000125\n",
      "Val Loss: 0.03453\n",
      "---------\n",
      "Epoch 249/1000, Train Loss: 0.03296, lr: 0.000125\n",
      "Val Loss: 0.03453\n",
      "---------\n",
      "Epoch 250/1000, Train Loss: 0.03296, lr: 0.000125\n",
      "Val Loss: 0.03453\n",
      "---------\n",
      "Epoch 251/1000, Train Loss: 0.03295, lr: 0.000125\n",
      "Val Loss: 0.03453\n",
      "---------\n",
      "Epoch 252/1000, Train Loss: 0.03295, lr: 0.000125\n",
      "Val Loss: 0.03453\n",
      "---------\n",
      "Epoch 253/1000, Train Loss: 0.03295, lr: 0.000125\n",
      "Val Loss: 0.03453\n",
      "---------\n",
      "Epoch 254/1000, Train Loss: 0.03295, lr: 0.000125\n",
      "Val Loss: 0.03453\n",
      "---------\n",
      "Epoch 255/1000, Train Loss: 0.03295, lr: 0.000125\n",
      "Val Loss: 0.03453\n",
      "---------\n",
      "Epoch 256/1000, Train Loss: 0.03294, lr: 0.000125\n",
      "Val Loss: 0.03452\n",
      "---------\n",
      "Epoch 257/1000, Train Loss: 0.03294, lr: 0.000125\n",
      "Val Loss: 0.03452\n",
      "---------\n",
      "Epoch 258/1000, Train Loss: 0.03294, lr: 0.000125\n",
      "Val Loss: 0.03452\n",
      "---------\n",
      "Epoch 259/1000, Train Loss: 0.03294, lr: 0.000125\n",
      "Val Loss: 0.03452\n",
      "---------\n",
      "Epoch 260/1000, Train Loss: 0.03294, lr: 0.000125\n",
      "Val Loss: 0.03452\n",
      "---------\n",
      "Epoch 261/1000, Train Loss: 0.03293, lr: 0.000125\n",
      "Val Loss: 0.03452\n",
      "---------\n",
      "Epoch 262/1000, Train Loss: 0.03293, lr: 0.000125\n",
      "Val Loss: 0.03452\n",
      "---------\n",
      "Epoch 263/1000, Train Loss: 0.03293, lr: 0.000125\n",
      "Val Loss: 0.03452\n",
      "---------\n",
      "Epoch 264/1000, Train Loss: 0.03293, lr: 0.000125\n",
      "Val Loss: 0.03452\n",
      "---------\n",
      "Epoch 265/1000, Train Loss: 0.03293, lr: 0.000125\n",
      "Val Loss: 0.03452\n",
      "---------\n",
      "Epoch 266/1000, Train Loss: 0.03292, lr: 0.000125\n",
      "Val Loss: 0.03452\n",
      "---------\n",
      "Epoch 267/1000, Train Loss: 0.03292, lr: 0.000125\n",
      "Val Loss: 0.03451\n",
      "---------\n",
      "Epoch 268/1000, Train Loss: 0.03292, lr: 0.000125\n",
      "Val Loss: 0.03451\n",
      "---------\n",
      "Epoch 269/1000, Train Loss: 0.03292, lr: 0.000125\n",
      "Val Loss: 0.03451\n",
      "---------\n",
      "Epoch 270/1000, Train Loss: 0.03292, lr: 0.000125\n",
      "Val Loss: 0.03451\n",
      "---------\n",
      "Epoch 271/1000, Train Loss: 0.03292, lr: 0.000125\n",
      "Val Loss: 0.03451\n",
      "---------\n",
      "Epoch 272/1000, Train Loss: 0.03291, lr: 0.000125\n",
      "Val Loss: 0.03451\n",
      "---------\n",
      "Epoch 273/1000, Train Loss: 0.03291, lr: 0.000125\n",
      "Val Loss: 0.03451\n",
      "---------\n",
      "Epoch 274/1000, Train Loss: 0.03291, lr: 0.000125\n",
      "Val Loss: 0.03451\n",
      "---------\n",
      "Epoch 275/1000, Train Loss: 0.03291, lr: 0.000125\n",
      "Val Loss: 0.03451\n",
      "---------\n",
      "Epoch 276/1000, Train Loss: 0.03291, lr: 0.000125\n",
      "Val Loss: 0.03451\n",
      "---------\n",
      "Epoch 277/1000, Train Loss: 0.03290, lr: 0.000125\n",
      "Val Loss: 0.03450\n",
      "---------\n",
      "Epoch 278/1000, Train Loss: 0.03290, lr: 0.000125\n",
      "Val Loss: 0.03450\n",
      "---------\n",
      "Epoch 279/1000, Train Loss: 0.03290, lr: 0.000125\n",
      "Val Loss: 0.03450\n",
      "---------\n",
      "Epoch 280/1000, Train Loss: 0.03290, lr: 0.000125\n",
      "Val Loss: 0.03450\n",
      "---------\n",
      "Epoch 281/1000, Train Loss: 0.03290, lr: 0.000125\n",
      "Val Loss: 0.03450\n",
      "---------\n",
      "Epoch 282/1000, Train Loss: 0.03290, lr: 0.000125\n",
      "Val Loss: 0.03450\n",
      "---------\n",
      "Epoch 283/1000, Train Loss: 0.03289, lr: 0.000125\n",
      "Val Loss: 0.03450\n",
      "---------\n",
      "Epoch 284/1000, Train Loss: 0.03289, lr: 0.000125\n",
      "Val Loss: 0.03450\n",
      "---------\n",
      "Epoch 285/1000, Train Loss: 0.03289, lr: 0.000125\n",
      "Val Loss: 0.03450\n",
      "---------\n",
      "Epoch 286/1000, Train Loss: 0.03289, lr: 0.000125\n",
      "Val Loss: 0.03450\n",
      "---------\n",
      "Epoch 287/1000, Train Loss: 0.03289, lr: 0.000125\n",
      "Val Loss: 0.03450\n",
      "---------\n",
      "Epoch 288/1000, Train Loss: 0.03289, lr: 0.000125\n",
      "Val Loss: 0.03450\n",
      "---------\n",
      "Epoch 289/1000, Train Loss: 0.03288, lr: 0.000125\n",
      "Val Loss: 0.03450\n",
      "---------\n",
      "Epoch 290/1000, Train Loss: 0.03288, lr: 0.000125\n",
      "Val Loss: 0.03450\n",
      "---------\n",
      "Epoch 291/1000, Train Loss: 0.03288, lr: 0.000125\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 292/1000, Train Loss: 0.03288, lr: 0.000125\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 293/1000, Train Loss: 0.03288, lr: 0.000125\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 294/1000, Train Loss: 0.03288, lr: 0.000125\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 295/1000, Train Loss: 0.03288, lr: 0.000125\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 296/1000, Train Loss: 0.03287, lr: 0.000125\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 297/1000, Train Loss: 0.03287, lr: 0.000125\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 298/1000, Train Loss: 0.03287, lr: 0.000125\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 299/1000, Train Loss: 0.03287, lr: 0.000125\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 300/1000, Train Loss: 0.03287, lr: 0.000125\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 301/1000, Train Loss: 0.03287, lr: 0.000125\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 302/1000, Train Loss: 0.03286, lr: 0.000125\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 303/1000, Train Loss: 0.03286, lr: 0.000125\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 304/1000, Train Loss: 0.03286, lr: 0.000125\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 305/1000, Train Loss: 0.03286, lr: 0.000125\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 306/1000, Train Loss: 0.03286, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 307/1000, Train Loss: 0.03286, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 308/1000, Train Loss: 0.03286, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 309/1000, Train Loss: 0.03285, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 310/1000, Train Loss: 0.03285, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 311/1000, Train Loss: 0.03285, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 312/1000, Train Loss: 0.03285, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 313/1000, Train Loss: 0.03285, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 314/1000, Train Loss: 0.03285, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 315/1000, Train Loss: 0.03284, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 316/1000, Train Loss: 0.03284, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 317/1000, Train Loss: 0.03284, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 318/1000, Train Loss: 0.03284, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 319/1000, Train Loss: 0.03284, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 320/1000, Train Loss: 0.03284, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 321/1000, Train Loss: 0.03284, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 322/1000, Train Loss: 0.03284, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 323/1000, Train Loss: 0.03283, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 324/1000, Train Loss: 0.03283, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 325/1000, Train Loss: 0.03283, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 326/1000, Train Loss: 0.03283, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 327/1000, Train Loss: 0.03283, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 328/1000, Train Loss: 0.03283, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 329/1000, Train Loss: 0.03283, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 330/1000, Train Loss: 0.03282, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 331/1000, Train Loss: 0.03282, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 332/1000, Train Loss: 0.03282, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 333/1000, Train Loss: 0.03282, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 334/1000, Train Loss: 0.03282, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 335/1000, Train Loss: 0.03282, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 336/1000, Train Loss: 0.03282, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 337/1000, Train Loss: 0.03281, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 338/1000, Train Loss: 0.03281, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 339/1000, Train Loss: 0.03281, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 340/1000, Train Loss: 0.03281, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 341/1000, Train Loss: 0.03281, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 342/1000, Train Loss: 0.03281, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 343/1000, Train Loss: 0.03281, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 344/1000, Train Loss: 0.03280, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 345/1000, Train Loss: 0.03280, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 346/1000, Train Loss: 0.03280, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 347/1000, Train Loss: 0.03280, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 348/1000, Train Loss: 0.03280, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 349/1000, Train Loss: 0.03280, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 350/1000, Train Loss: 0.03280, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 351/1000, Train Loss: 0.03280, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 352/1000, Train Loss: 0.03279, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 353/1000, Train Loss: 0.03279, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 354/1000, Train Loss: 0.03279, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 355/1000, Train Loss: 0.03279, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 356/1000, Train Loss: 0.03279, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 357/1000, Train Loss: 0.03279, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 358/1000, Train Loss: 0.03279, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 359/1000, Train Loss: 0.03279, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 360/1000, Train Loss: 0.03278, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 361/1000, Train Loss: 0.03278, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 362/1000, Train Loss: 0.03278, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 363/1000, Train Loss: 0.03278, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 364/1000, Train Loss: 0.03278, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 365/1000, Train Loss: 0.03278, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 366/1000, Train Loss: 0.03278, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 367/1000, Train Loss: 0.03278, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 368/1000, Train Loss: 0.03277, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 369/1000, Train Loss: 0.03277, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 370/1000, Train Loss: 0.03277, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 371/1000, Train Loss: 0.03277, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 372/1000, Train Loss: 0.03277, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 373/1000, Train Loss: 0.03277, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 374/1000, Train Loss: 0.03277, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 375/1000, Train Loss: 0.03277, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 376/1000, Train Loss: 0.03276, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 377/1000, Train Loss: 0.03276, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 378/1000, Train Loss: 0.03276, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 379/1000, Train Loss: 0.03276, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 380/1000, Train Loss: 0.03276, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 381/1000, Train Loss: 0.03276, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 382/1000, Train Loss: 0.03276, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 383/1000, Train Loss: 0.03276, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 384/1000, Train Loss: 0.03276, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 385/1000, Train Loss: 0.03275, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 386/1000, Train Loss: 0.03275, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 387/1000, Train Loss: 0.03275, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 388/1000, Train Loss: 0.03275, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 389/1000, Train Loss: 0.03275, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 390/1000, Train Loss: 0.03275, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 391/1000, Train Loss: 0.03275, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 392/1000, Train Loss: 0.03275, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 393/1000, Train Loss: 0.03274, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 394/1000, Train Loss: 0.03274, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 395/1000, Train Loss: 0.03274, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 396/1000, Train Loss: 0.03274, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 397/1000, Train Loss: 0.03274, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 398/1000, Train Loss: 0.03274, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 399/1000, Train Loss: 0.03274, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 400/1000, Train Loss: 0.03274, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 401/1000, Train Loss: 0.03274, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 402/1000, Train Loss: 0.03273, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 403/1000, Train Loss: 0.03273, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 404/1000, Train Loss: 0.03273, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 405/1000, Train Loss: 0.03273, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 406/1000, Train Loss: 0.03273, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 407/1000, Train Loss: 0.03273, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 408/1000, Train Loss: 0.03273, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 409/1000, Train Loss: 0.03273, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 410/1000, Train Loss: 0.03273, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 411/1000, Train Loss: 0.03272, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 412/1000, Train Loss: 0.03272, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 413/1000, Train Loss: 0.03272, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 414/1000, Train Loss: 0.03272, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 415/1000, Train Loss: 0.03272, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 416/1000, Train Loss: 0.03272, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 417/1000, Train Loss: 0.03272, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 418/1000, Train Loss: 0.03272, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 419/1000, Train Loss: 0.03272, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 420/1000, Train Loss: 0.03272, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 421/1000, Train Loss: 0.03271, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 422/1000, Train Loss: 0.03271, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 423/1000, Train Loss: 0.03271, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 424/1000, Train Loss: 0.03271, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 425/1000, Train Loss: 0.03271, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 426/1000, Train Loss: 0.03271, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 427/1000, Train Loss: 0.03271, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 428/1000, Train Loss: 0.03271, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 429/1000, Train Loss: 0.03271, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 430/1000, Train Loss: 0.03270, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 431/1000, Train Loss: 0.03270, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 432/1000, Train Loss: 0.03270, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 433/1000, Train Loss: 0.03270, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 434/1000, Train Loss: 0.03270, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 435/1000, Train Loss: 0.03270, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 436/1000, Train Loss: 0.03270, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 437/1000, Train Loss: 0.03270, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 438/1000, Train Loss: 0.03270, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 439/1000, Train Loss: 0.03270, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 440/1000, Train Loss: 0.03269, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 441/1000, Train Loss: 0.03269, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 442/1000, Train Loss: 0.03269, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 443/1000, Train Loss: 0.03269, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 444/1000, Train Loss: 0.03269, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 445/1000, Train Loss: 0.03269, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 446/1000, Train Loss: 0.03269, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 447/1000, Train Loss: 0.03269, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 448/1000, Train Loss: 0.03269, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 449/1000, Train Loss: 0.03268, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 450/1000, Train Loss: 0.03268, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 451/1000, Train Loss: 0.03268, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 452/1000, Train Loss: 0.03268, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 453/1000, Train Loss: 0.03268, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 454/1000, Train Loss: 0.03268, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 455/1000, Train Loss: 0.03268, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 456/1000, Train Loss: 0.03268, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 457/1000, Train Loss: 0.03268, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 458/1000, Train Loss: 0.03268, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 459/1000, Train Loss: 0.03267, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 460/1000, Train Loss: 0.03267, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 461/1000, Train Loss: 0.03267, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 462/1000, Train Loss: 0.03267, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 463/1000, Train Loss: 0.03267, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 464/1000, Train Loss: 0.03267, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 465/1000, Train Loss: 0.03267, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 466/1000, Train Loss: 0.03267, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 467/1000, Train Loss: 0.03267, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 468/1000, Train Loss: 0.03267, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 469/1000, Train Loss: 0.03266, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 470/1000, Train Loss: 0.03266, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 471/1000, Train Loss: 0.03266, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 472/1000, Train Loss: 0.03266, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 473/1000, Train Loss: 0.03266, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 474/1000, Train Loss: 0.03266, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 475/1000, Train Loss: 0.03266, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 476/1000, Train Loss: 0.03266, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 477/1000, Train Loss: 0.03266, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 478/1000, Train Loss: 0.03266, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 479/1000, Train Loss: 0.03265, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 480/1000, Train Loss: 0.03265, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 481/1000, Train Loss: 0.03265, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 482/1000, Train Loss: 0.03265, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 483/1000, Train Loss: 0.03265, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 484/1000, Train Loss: 0.03265, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 485/1000, Train Loss: 0.03265, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 486/1000, Train Loss: 0.03265, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 487/1000, Train Loss: 0.03265, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 488/1000, Train Loss: 0.03265, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 489/1000, Train Loss: 0.03264, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 490/1000, Train Loss: 0.03264, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 491/1000, Train Loss: 0.03264, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 492/1000, Train Loss: 0.03264, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 493/1000, Train Loss: 0.03264, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 494/1000, Train Loss: 0.03264, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 495/1000, Train Loss: 0.03264, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 496/1000, Train Loss: 0.03264, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 497/1000, Train Loss: 0.03264, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 498/1000, Train Loss: 0.03264, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 499/1000, Train Loss: 0.03264, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 500/1000, Train Loss: 0.03263, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 501/1000, Train Loss: 0.03263, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 502/1000, Train Loss: 0.03263, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 503/1000, Train Loss: 0.03263, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 504/1000, Train Loss: 0.03263, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 505/1000, Train Loss: 0.03263, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 506/1000, Train Loss: 0.03263, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 507/1000, Train Loss: 0.03263, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 508/1000, Train Loss: 0.03263, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 509/1000, Train Loss: 0.03263, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 510/1000, Train Loss: 0.03263, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 511/1000, Train Loss: 0.03262, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 512/1000, Train Loss: 0.03262, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 513/1000, Train Loss: 0.03262, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 514/1000, Train Loss: 0.03262, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 515/1000, Train Loss: 0.03262, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 516/1000, Train Loss: 0.03262, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 517/1000, Train Loss: 0.03262, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 518/1000, Train Loss: 0.03262, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 519/1000, Train Loss: 0.03262, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 520/1000, Train Loss: 0.03262, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 521/1000, Train Loss: 0.03262, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 522/1000, Train Loss: 0.03261, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 523/1000, Train Loss: 0.03261, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 524/1000, Train Loss: 0.03261, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 525/1000, Train Loss: 0.03261, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 526/1000, Train Loss: 0.03261, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 527/1000, Train Loss: 0.03261, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 528/1000, Train Loss: 0.03261, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 529/1000, Train Loss: 0.03261, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 530/1000, Train Loss: 0.03261, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 531/1000, Train Loss: 0.03261, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 532/1000, Train Loss: 0.03261, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 533/1000, Train Loss: 0.03260, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 534/1000, Train Loss: 0.03260, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 535/1000, Train Loss: 0.03260, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 536/1000, Train Loss: 0.03260, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 537/1000, Train Loss: 0.03260, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 538/1000, Train Loss: 0.03260, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 539/1000, Train Loss: 0.03260, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 540/1000, Train Loss: 0.03260, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 541/1000, Train Loss: 0.03260, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 542/1000, Train Loss: 0.03260, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 543/1000, Train Loss: 0.03260, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 544/1000, Train Loss: 0.03259, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 545/1000, Train Loss: 0.03259, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 546/1000, Train Loss: 0.03259, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 547/1000, Train Loss: 0.03259, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 548/1000, Train Loss: 0.03259, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 549/1000, Train Loss: 0.03259, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 550/1000, Train Loss: 0.03259, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 551/1000, Train Loss: 0.03259, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 552/1000, Train Loss: 0.03259, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 553/1000, Train Loss: 0.03259, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 554/1000, Train Loss: 0.03259, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 555/1000, Train Loss: 0.03258, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 556/1000, Train Loss: 0.03258, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 557/1000, Train Loss: 0.03258, lr: 0.000125\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 558/1000, Train Loss: 0.03258, lr: 0.000125\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 559/1000, Train Loss: 0.03258, lr: 0.000125\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 560/1000, Train Loss: 0.03258, lr: 0.000125\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 561/1000, Train Loss: 0.03258, lr: 0.000125\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 562/1000, Train Loss: 0.03258, lr: 0.000125\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 563/1000, Train Loss: 0.03257, lr: 0.000125\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 564/1000, Train Loss: 0.03257, lr: 0.000125\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 565/1000, Train Loss: 0.03257, lr: 0.000125\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 566/1000, Train Loss: 0.03257, lr: 0.000125\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 567/1000, Train Loss: 0.03257, lr: 0.000125\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 568/1000, Train Loss: 0.03257, lr: 0.000125\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 569/1000, Train Loss: 0.03256, lr: 0.000125\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 570/1000, Train Loss: 0.03256, lr: 0.000125\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 571/1000, Train Loss: 0.03256, lr: 0.000125\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 572/1000, Train Loss: 0.03256, lr: 0.000125\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 573/1000, Train Loss: 0.03256, lr: 0.000125\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 574/1000, Train Loss: 0.03256, lr: 0.000125\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 575/1000, Train Loss: 0.03255, lr: 0.000125\n",
      "Val Loss: 0.03435\n",
      "---------\n",
      "Epoch 576/1000, Train Loss: 0.03255, lr: 0.000125\n",
      "Val Loss: 0.03435\n",
      "---------\n",
      "Epoch 577/1000, Train Loss: 0.03255, lr: 0.000125\n",
      "Val Loss: 0.03435\n",
      "---------\n",
      "Epoch 578/1000, Train Loss: 0.03255, lr: 0.000125\n",
      "Val Loss: 0.03435\n",
      "---------\n",
      "Epoch 579/1000, Train Loss: 0.03255, lr: 0.000125\n",
      "Val Loss: 0.03435\n",
      "---------\n",
      "Epoch 580/1000, Train Loss: 0.03255, lr: 0.000125\n",
      "Val Loss: 0.03435\n",
      "---------\n",
      "Epoch 581/1000, Train Loss: 0.03255, lr: 0.000125\n",
      "Val Loss: 0.03435\n",
      "---------\n",
      "Epoch 582/1000, Train Loss: 0.03254, lr: 0.000125\n",
      "Val Loss: 0.03435\n",
      "---------\n",
      "Epoch 583/1000, Train Loss: 0.03254, lr: 0.000125\n",
      "Val Loss: 0.03435\n",
      "---------\n",
      "Epoch 584/1000, Train Loss: 0.03254, lr: 0.000125\n",
      "Val Loss: 0.03435\n",
      "---------\n",
      "Epoch 585/1000, Train Loss: 0.03254, lr: 0.000125\n",
      "Val Loss: 0.03435\n",
      "---------\n",
      "Epoch 586/1000, Train Loss: 0.03254, lr: 0.000125\n",
      "Val Loss: 0.03435\n",
      "---------\n",
      "Epoch 587/1000, Train Loss: 0.03254, lr: 0.000125\n",
      "Val Loss: 0.03435\n",
      "---------\n",
      "Epoch 588/1000, Train Loss: 0.03254, lr: 0.000125\n",
      "Val Loss: 0.03435\n",
      "---------\n",
      "Epoch 589/1000, Train Loss: 0.03254, lr: 0.000125\n",
      "Val Loss: 0.03435\n",
      "---------\n",
      "Epoch 590/1000, Train Loss: 0.03254, lr: 0.000125\n",
      "Val Loss: 0.03434\n",
      "---------\n",
      "Epoch 591/1000, Train Loss: 0.03253, lr: 0.000125\n",
      "Val Loss: 0.03434\n",
      "---------\n",
      "Epoch 592/1000, Train Loss: 0.03253, lr: 0.000125\n",
      "Val Loss: 0.03434\n",
      "---------\n",
      "Epoch 593/1000, Train Loss: 0.03253, lr: 0.000125\n",
      "Val Loss: 0.03434\n",
      "---------\n",
      "Epoch 594/1000, Train Loss: 0.03253, lr: 0.000125\n",
      "Val Loss: 0.03434\n",
      "---------\n",
      "Epoch 595/1000, Train Loss: 0.03253, lr: 0.000125\n",
      "Val Loss: 0.03434\n",
      "---------\n",
      "Epoch 596/1000, Train Loss: 0.03253, lr: 0.000125\n",
      "Val Loss: 0.03434\n",
      "---------\n",
      "Epoch 597/1000, Train Loss: 0.03253, lr: 0.000125\n",
      "Val Loss: 0.03434\n",
      "---------\n",
      "Epoch 598/1000, Train Loss: 0.03253, lr: 0.000125\n",
      "Val Loss: 0.03434\n",
      "---------\n",
      "Epoch 599/1000, Train Loss: 0.03253, lr: 0.000125\n",
      "Val Loss: 0.03434\n",
      "---------\n",
      "Epoch 600/1000, Train Loss: 0.03252, lr: 0.000125\n",
      "Val Loss: 0.03434\n",
      "---------\n",
      "Epoch 601/1000, Train Loss: 0.03252, lr: 0.000125\n",
      "Val Loss: 0.03434\n",
      "---------\n",
      "Epoch 602/1000, Train Loss: 0.03252, lr: 0.000125\n",
      "Val Loss: 0.03434\n",
      "---------\n",
      "Epoch 603/1000, Train Loss: 0.03252, lr: 0.000125\n",
      "Val Loss: 0.03434\n",
      "---------\n",
      "Epoch 604/1000, Train Loss: 0.03252, lr: 0.000125\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 605/1000, Train Loss: 0.03252, lr: 0.000125\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 606/1000, Train Loss: 0.03252, lr: 0.000125\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 607/1000, Train Loss: 0.03252, lr: 0.000125\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 608/1000, Train Loss: 0.03252, lr: 0.000125\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 609/1000, Train Loss: 0.03251, lr: 0.000125\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 610/1000, Train Loss: 0.03251, lr: 0.000125\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 611/1000, Train Loss: 0.03251, lr: 0.000125\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 612/1000, Train Loss: 0.03251, lr: 0.000125\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 613/1000, Train Loss: 0.03251, lr: 0.000125\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 614/1000, Train Loss: 0.03251, lr: 0.000125\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 615/1000, Train Loss: 0.03251, lr: 0.000125\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 616/1000, Train Loss: 0.03251, lr: 0.000125\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 617/1000, Train Loss: 0.03251, lr: 0.000125\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 618/1000, Train Loss: 0.03251, lr: 0.000125\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 619/1000, Train Loss: 0.03250, lr: 0.000125\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 620/1000, Train Loss: 0.03250, lr: 0.000125\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 621/1000, Train Loss: 0.03250, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 622/1000, Train Loss: 0.03250, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 623/1000, Train Loss: 0.03250, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 624/1000, Train Loss: 0.03250, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 625/1000, Train Loss: 0.03250, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 626/1000, Train Loss: 0.03250, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 627/1000, Train Loss: 0.03250, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 628/1000, Train Loss: 0.03250, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 629/1000, Train Loss: 0.03250, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 630/1000, Train Loss: 0.03249, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 631/1000, Train Loss: 0.03249, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 632/1000, Train Loss: 0.03249, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 633/1000, Train Loss: 0.03249, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 634/1000, Train Loss: 0.03249, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 635/1000, Train Loss: 0.03249, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 636/1000, Train Loss: 0.03249, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 637/1000, Train Loss: 0.03249, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 638/1000, Train Loss: 0.03249, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 639/1000, Train Loss: 0.03249, lr: 0.000125\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 640/1000, Train Loss: 0.03248, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 641/1000, Train Loss: 0.03248, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 642/1000, Train Loss: 0.03248, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 643/1000, Train Loss: 0.03248, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 644/1000, Train Loss: 0.03248, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 645/1000, Train Loss: 0.03248, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 646/1000, Train Loss: 0.03248, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 647/1000, Train Loss: 0.03248, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 648/1000, Train Loss: 0.03248, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 649/1000, Train Loss: 0.03248, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 650/1000, Train Loss: 0.03248, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 651/1000, Train Loss: 0.03247, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 652/1000, Train Loss: 0.03247, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 653/1000, Train Loss: 0.03247, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 654/1000, Train Loss: 0.03247, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 655/1000, Train Loss: 0.03247, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 656/1000, Train Loss: 0.03247, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 657/1000, Train Loss: 0.03247, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 658/1000, Train Loss: 0.03247, lr: 0.000125\n",
      "Val Loss: 0.03431\n",
      "---------\n",
      "Epoch 659/1000, Train Loss: 0.03247, lr: 0.000125\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 660/1000, Train Loss: 0.03247, lr: 0.000125\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 661/1000, Train Loss: 0.03247, lr: 0.000125\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 662/1000, Train Loss: 0.03246, lr: 0.000125\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 663/1000, Train Loss: 0.03246, lr: 0.000125\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 664/1000, Train Loss: 0.03246, lr: 0.000125\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 665/1000, Train Loss: 0.03246, lr: 0.000125\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 666/1000, Train Loss: 0.03246, lr: 0.000125\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 667/1000, Train Loss: 0.03246, lr: 0.000125\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 668/1000, Train Loss: 0.03246, lr: 0.000125\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 669/1000, Train Loss: 0.03246, lr: 0.000125\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 670/1000, Train Loss: 0.03246, lr: 0.000125\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 671/1000, Train Loss: 0.03246, lr: 0.000125\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 672/1000, Train Loss: 0.03246, lr: 0.000125\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 673/1000, Train Loss: 0.03245, lr: 0.000125\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 674/1000, Train Loss: 0.03245, lr: 0.000125\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 675/1000, Train Loss: 0.03245, lr: 0.000125\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 676/1000, Train Loss: 0.03245, lr: 0.000125\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 677/1000, Train Loss: 0.03245, lr: 0.000125\n",
      "Val Loss: 0.03429\n",
      "---------\n",
      "Epoch 678/1000, Train Loss: 0.03245, lr: 0.000125\n",
      "Val Loss: 0.03429\n",
      "---------\n",
      "Epoch 679/1000, Train Loss: 0.03245, lr: 0.000125\n",
      "Val Loss: 0.03429\n",
      "---------\n",
      "Epoch 680/1000, Train Loss: 0.03245, lr: 0.000125\n",
      "Val Loss: 0.03429\n",
      "---------\n",
      "Epoch 681/1000, Train Loss: 0.03245, lr: 0.000125\n",
      "Val Loss: 0.03429\n",
      "---------\n",
      "Epoch 682/1000, Train Loss: 0.03245, lr: 0.000125\n",
      "Val Loss: 0.03429\n",
      "---------\n",
      "Epoch 683/1000, Train Loss: 0.03245, lr: 0.000125\n",
      "Val Loss: 0.03429\n",
      "---------\n",
      "Epoch 684/1000, Train Loss: 0.03244, lr: 0.000125\n",
      "Val Loss: 0.03429\n",
      "---------\n",
      "Epoch 685/1000, Train Loss: 0.03244, lr: 0.000125\n",
      "Val Loss: 0.03429\n",
      "---------\n",
      "Epoch 686/1000, Train Loss: 0.03244, lr: 0.000125\n",
      "Val Loss: 0.03429\n",
      "---------\n",
      "Epoch 687/1000, Train Loss: 0.03244, lr: 0.000125\n",
      "Val Loss: 0.03429\n",
      "---------\n",
      "Epoch 688/1000, Train Loss: 0.03244, lr: 0.000125\n",
      "Val Loss: 0.03429\n",
      "---------\n",
      "Epoch 689/1000, Train Loss: 0.03244, lr: 0.000125\n",
      "Val Loss: 0.03429\n",
      "---------\n",
      "Epoch 690/1000, Train Loss: 0.03244, lr: 0.000125\n",
      "Val Loss: 0.03428\n",
      "---------\n",
      "Epoch 691/1000, Train Loss: 0.03244, lr: 0.000125\n",
      "Val Loss: 0.03428\n",
      "---------\n",
      "Epoch 692/1000, Train Loss: 0.03244, lr: 0.000125\n",
      "Val Loss: 0.03428\n",
      "---------\n",
      "Epoch 693/1000, Train Loss: 0.03244, lr: 0.000125\n",
      "Val Loss: 0.03428\n",
      "---------\n",
      "Epoch 694/1000, Train Loss: 0.03243, lr: 0.000125\n",
      "Val Loss: 0.03428\n",
      "---------\n",
      "Epoch 695/1000, Train Loss: 0.03243, lr: 0.000125\n",
      "Val Loss: 0.03428\n",
      "---------\n",
      "Epoch 696/1000, Train Loss: 0.03243, lr: 0.000125\n",
      "Val Loss: 0.03428\n",
      "---------\n",
      "Epoch 697/1000, Train Loss: 0.03243, lr: 0.000125\n",
      "Val Loss: 0.03428\n",
      "---------\n",
      "Epoch 698/1000, Train Loss: 0.03243, lr: 0.000125\n",
      "Val Loss: 0.03428\n",
      "---------\n",
      "Epoch 699/1000, Train Loss: 0.03243, lr: 0.000125\n",
      "Val Loss: 0.03428\n",
      "---------\n",
      "Epoch 700/1000, Train Loss: 0.03243, lr: 0.000125\n",
      "Val Loss: 0.03428\n",
      "---------\n",
      "Epoch 701/1000, Train Loss: 0.03243, lr: 0.000125\n",
      "Val Loss: 0.03427\n",
      "---------\n",
      "Epoch 702/1000, Train Loss: 0.03243, lr: 0.000125\n",
      "Val Loss: 0.03427\n",
      "---------\n",
      "Epoch 703/1000, Train Loss: 0.03243, lr: 0.000125\n",
      "Val Loss: 0.03427\n",
      "---------\n",
      "Epoch 704/1000, Train Loss: 0.03243, lr: 0.000125\n",
      "Val Loss: 0.03427\n",
      "---------\n",
      "Epoch 705/1000, Train Loss: 0.03242, lr: 0.000125\n",
      "Val Loss: 0.03427\n",
      "---------\n",
      "Epoch 706/1000, Train Loss: 0.03242, lr: 0.000125\n",
      "Val Loss: 0.03427\n",
      "---------\n",
      "Epoch 707/1000, Train Loss: 0.03242, lr: 0.000125\n",
      "Val Loss: 0.03427\n",
      "---------\n",
      "Epoch 708/1000, Train Loss: 0.03242, lr: 0.000125\n",
      "Val Loss: 0.03427\n",
      "---------\n",
      "Epoch 709/1000, Train Loss: 0.03242, lr: 0.000125\n",
      "Val Loss: 0.03427\n",
      "---------\n",
      "Epoch 710/1000, Train Loss: 0.03242, lr: 0.000125\n",
      "Val Loss: 0.03427\n",
      "---------\n",
      "Epoch 711/1000, Train Loss: 0.03242, lr: 0.000125\n",
      "Val Loss: 0.03427\n",
      "---------\n",
      "Epoch 712/1000, Train Loss: 0.03242, lr: 0.000125\n",
      "Val Loss: 0.03427\n",
      "---------\n",
      "Epoch 713/1000, Train Loss: 0.03242, lr: 0.000125\n",
      "Val Loss: 0.03427\n",
      "---------\n",
      "Epoch 714/1000, Train Loss: 0.03242, lr: 0.000125\n",
      "Val Loss: 0.03427\n",
      "---------\n",
      "Epoch 715/1000, Train Loss: 0.03241, lr: 0.000125\n",
      "Val Loss: 0.03427\n",
      "---------\n",
      "Epoch 716/1000, Train Loss: 0.03241, lr: 0.000125\n",
      "Val Loss: 0.03426\n",
      "---------\n",
      "Epoch 717/1000, Train Loss: 0.03241, lr: 0.000125\n",
      "Val Loss: 0.03426\n",
      "---------\n",
      "Epoch 718/1000, Train Loss: 0.03241, lr: 0.000125\n",
      "Val Loss: 0.03426\n",
      "---------\n",
      "Epoch 719/1000, Train Loss: 0.03241, lr: 0.000125\n",
      "Val Loss: 0.03426\n",
      "---------\n",
      "Epoch 720/1000, Train Loss: 0.03241, lr: 0.000125\n",
      "Val Loss: 0.03426\n",
      "---------\n",
      "Epoch 721/1000, Train Loss: 0.03241, lr: 0.000125\n",
      "Val Loss: 0.03426\n",
      "---------\n",
      "Epoch 722/1000, Train Loss: 0.03241, lr: 0.000125\n",
      "Val Loss: 0.03426\n",
      "---------\n",
      "Epoch 723/1000, Train Loss: 0.03241, lr: 0.000125\n",
      "Val Loss: 0.03426\n",
      "---------\n",
      "Epoch 724/1000, Train Loss: 0.03241, lr: 0.000125\n",
      "Val Loss: 0.03426\n",
      "---------\n",
      "Epoch 725/1000, Train Loss: 0.03241, lr: 0.000125\n",
      "Val Loss: 0.03426\n",
      "---------\n",
      "Epoch 726/1000, Train Loss: 0.03240, lr: 0.000125\n",
      "Val Loss: 0.03426\n",
      "---------\n",
      "Epoch 727/1000, Train Loss: 0.03240, lr: 0.000125\n",
      "Val Loss: 0.03426\n",
      "---------\n",
      "Epoch 728/1000, Train Loss: 0.03240, lr: 0.000125\n",
      "Val Loss: 0.03425\n",
      "---------\n",
      "Epoch 729/1000, Train Loss: 0.03240, lr: 0.000125\n",
      "Val Loss: 0.03425\n",
      "---------\n",
      "Epoch 730/1000, Train Loss: 0.03240, lr: 0.000125\n",
      "Val Loss: 0.03425\n",
      "---------\n",
      "Epoch 731/1000, Train Loss: 0.03240, lr: 0.000125\n",
      "Val Loss: 0.03425\n",
      "---------\n",
      "Epoch 732/1000, Train Loss: 0.03240, lr: 0.000125\n",
      "Val Loss: 0.03425\n",
      "---------\n",
      "Epoch 733/1000, Train Loss: 0.03240, lr: 0.000125\n",
      "Val Loss: 0.03425\n",
      "---------\n",
      "Epoch 734/1000, Train Loss: 0.03240, lr: 0.000125\n",
      "Val Loss: 0.03425\n",
      "---------\n",
      "Epoch 735/1000, Train Loss: 0.03240, lr: 0.000125\n",
      "Val Loss: 0.03425\n",
      "---------\n",
      "Epoch 736/1000, Train Loss: 0.03239, lr: 0.000125\n",
      "Val Loss: 0.03425\n",
      "---------\n",
      "Epoch 737/1000, Train Loss: 0.03239, lr: 0.000125\n",
      "Val Loss: 0.03425\n",
      "---------\n",
      "Epoch 738/1000, Train Loss: 0.03239, lr: 0.000125\n",
      "Val Loss: 0.03425\n",
      "---------\n",
      "Epoch 739/1000, Train Loss: 0.03239, lr: 0.000125\n",
      "Val Loss: 0.03425\n",
      "---------\n",
      "Epoch 740/1000, Train Loss: 0.03239, lr: 0.000125\n",
      "Val Loss: 0.03425\n",
      "---------\n",
      "Epoch 741/1000, Train Loss: 0.03239, lr: 0.000125\n",
      "Val Loss: 0.03425\n",
      "---------\n",
      "Epoch 742/1000, Train Loss: 0.03239, lr: 0.000125\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 743/1000, Train Loss: 0.03239, lr: 0.000125\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 744/1000, Train Loss: 0.03239, lr: 0.000125\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 745/1000, Train Loss: 0.03239, lr: 0.000125\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 746/1000, Train Loss: 0.03239, lr: 0.000125\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 747/1000, Train Loss: 0.03238, lr: 0.000125\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 748/1000, Train Loss: 0.03238, lr: 0.000125\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 749/1000, Train Loss: 0.03238, lr: 0.000125\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 750/1000, Train Loss: 0.03238, lr: 0.000125\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 751/1000, Train Loss: 0.03238, lr: 0.000125\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 752/1000, Train Loss: 0.03238, lr: 0.000125\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 753/1000, Train Loss: 0.03238, lr: 0.000125\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 754/1000, Train Loss: 0.03238, lr: 0.000125\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 755/1000, Train Loss: 0.03238, lr: 0.000125\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 756/1000, Train Loss: 0.03238, lr: 0.000125\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 757/1000, Train Loss: 0.03238, lr: 0.000125\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 758/1000, Train Loss: 0.03238, lr: 0.000125\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 759/1000, Train Loss: 0.03237, lr: 0.000125\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 760/1000, Train Loss: 0.03237, lr: 0.000125\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 761/1000, Train Loss: 0.03237, lr: 0.000125\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 762/1000, Train Loss: 0.03237, lr: 0.000125\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 763/1000, Train Loss: 0.03237, lr: 0.000125\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 764/1000, Train Loss: 0.03237, lr: 0.000125\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 765/1000, Train Loss: 0.03237, lr: 0.000125\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 766/1000, Train Loss: 0.03237, lr: 0.000125\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 767/1000, Train Loss: 0.03237, lr: 0.000125\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 768/1000, Train Loss: 0.03237, lr: 0.000125\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 769/1000, Train Loss: 0.03237, lr: 0.000125\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 770/1000, Train Loss: 0.03236, lr: 0.000125\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 771/1000, Train Loss: 0.03236, lr: 0.000125\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 772/1000, Train Loss: 0.03236, lr: 0.000125\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 773/1000, Train Loss: 0.03236, lr: 0.000125\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 774/1000, Train Loss: 0.03236, lr: 0.000125\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 775/1000, Train Loss: 0.03236, lr: 0.000125\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 776/1000, Train Loss: 0.03236, lr: 0.000125\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 777/1000, Train Loss: 0.03236, lr: 0.000125\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 778/1000, Train Loss: 0.03236, lr: 0.000125\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 779/1000, Train Loss: 0.03236, lr: 0.000125\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 780/1000, Train Loss: 0.03236, lr: 0.000125\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 781/1000, Train Loss: 0.03235, lr: 0.000125\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 782/1000, Train Loss: 0.03235, lr: 0.000125\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 783/1000, Train Loss: 0.03235, lr: 0.000125\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 784/1000, Train Loss: 0.03235, lr: 0.000125\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 785/1000, Train Loss: 0.03235, lr: 0.000125\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 786/1000, Train Loss: 0.03235, lr: 0.000125\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 787/1000, Train Loss: 0.03235, lr: 0.000125\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 788/1000, Train Loss: 0.03235, lr: 0.000125\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 789/1000, Train Loss: 0.03235, lr: 0.000125\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 790/1000, Train Loss: 0.03235, lr: 0.000125\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 791/1000, Train Loss: 0.03235, lr: 0.000125\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 792/1000, Train Loss: 0.03235, lr: 0.000125\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 793/1000, Train Loss: 0.03234, lr: 0.000125\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 794/1000, Train Loss: 0.03234, lr: 0.000125\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 795/1000, Train Loss: 0.03234, lr: 0.000125\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 796/1000, Train Loss: 0.03234, lr: 0.000125\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 797/1000, Train Loss: 0.03234, lr: 0.000125\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 798/1000, Train Loss: 0.03234, lr: 0.000125\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 799/1000, Train Loss: 0.03234, lr: 0.000125\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 800/1000, Train Loss: 0.03234, lr: 0.000125\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 801/1000, Train Loss: 0.03234, lr: 0.000125\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 802/1000, Train Loss: 0.03234, lr: 0.000125\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 803/1000, Train Loss: 0.03234, lr: 0.000125\n",
      "Val Loss: 0.03420\n",
      "---------\n",
      "Epoch 804/1000, Train Loss: 0.03234, lr: 0.000125\n",
      "Val Loss: 0.03420\n",
      "---------\n",
      "Epoch 805/1000, Train Loss: 0.03233, lr: 0.000125\n",
      "Val Loss: 0.03420\n",
      "---------\n",
      "Epoch 806/1000, Train Loss: 0.03233, lr: 0.000125\n",
      "Val Loss: 0.03420\n",
      "---------\n",
      "Epoch 807/1000, Train Loss: 0.03233, lr: 0.000125\n",
      "Val Loss: 0.03420\n",
      "---------\n",
      "Epoch 808/1000, Train Loss: 0.03233, lr: 0.000125\n",
      "Val Loss: 0.03420\n",
      "---------\n",
      "Epoch 809/1000, Train Loss: 0.03233, lr: 0.000125\n",
      "Val Loss: 0.03420\n",
      "---------\n",
      "Epoch 810/1000, Train Loss: 0.03233, lr: 0.000125\n",
      "Val Loss: 0.03420\n",
      "---------\n",
      "Epoch 811/1000, Train Loss: 0.03233, lr: 0.000125\n",
      "Val Loss: 0.03420\n",
      "---------\n",
      "Epoch 812/1000, Train Loss: 0.03233, lr: 0.000125\n",
      "Val Loss: 0.03420\n",
      "---------\n",
      "Epoch 813/1000, Train Loss: 0.03233, lr: 0.000125\n",
      "Val Loss: 0.03420\n",
      "---------\n",
      "Epoch 814/1000, Train Loss: 0.03233, lr: 0.000125\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 815/1000, Train Loss: 0.03233, lr: 0.000125\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 816/1000, Train Loss: 0.03233, lr: 0.000125\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 817/1000, Train Loss: 0.03232, lr: 0.000125\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 818/1000, Train Loss: 0.03232, lr: 0.000125\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 819/1000, Train Loss: 0.03232, lr: 0.000125\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 820/1000, Train Loss: 0.03232, lr: 0.000125\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 821/1000, Train Loss: 0.03232, lr: 0.000125\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 822/1000, Train Loss: 0.03232, lr: 0.000125\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 823/1000, Train Loss: 0.03232, lr: 0.000125\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 824/1000, Train Loss: 0.03232, lr: 0.000125\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 825/1000, Train Loss: 0.03232, lr: 0.000125\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 826/1000, Train Loss: 0.03232, lr: 0.000125\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 827/1000, Train Loss: 0.03232, lr: 0.000125\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 828/1000, Train Loss: 0.03232, lr: 0.000125\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 829/1000, Train Loss: 0.03231, lr: 0.000125\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 830/1000, Train Loss: 0.03231, lr: 0.000125\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 831/1000, Train Loss: 0.03231, lr: 0.000125\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 832/1000, Train Loss: 0.03231, lr: 0.000125\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 833/1000, Train Loss: 0.03231, lr: 0.000125\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 834/1000, Train Loss: 0.03231, lr: 0.000125\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 835/1000, Train Loss: 0.03231, lr: 0.000125\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 836/1000, Train Loss: 0.03231, lr: 0.000125\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 837/1000, Train Loss: 0.03231, lr: 0.000125\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 838/1000, Train Loss: 0.03231, lr: 0.000125\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 839/1000, Train Loss: 0.03231, lr: 0.000125\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 840/1000, Train Loss: 0.03231, lr: 0.000125\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 841/1000, Train Loss: 0.03230, lr: 0.000125\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 842/1000, Train Loss: 0.03230, lr: 0.000125\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 843/1000, Train Loss: 0.03230, lr: 0.000125\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 844/1000, Train Loss: 0.03230, lr: 0.000125\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 845/1000, Train Loss: 0.03230, lr: 0.000125\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 846/1000, Train Loss: 0.03230, lr: 0.000125\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 847/1000, Train Loss: 0.03230, lr: 0.000125\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 848/1000, Train Loss: 0.03230, lr: 0.000125\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 849/1000, Train Loss: 0.03230, lr: 0.000125\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 850/1000, Train Loss: 0.03230, lr: 0.000125\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 851/1000, Train Loss: 0.03230, lr: 0.000125\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 852/1000, Train Loss: 0.03230, lr: 0.000125\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 853/1000, Train Loss: 0.03230, lr: 0.000125\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 854/1000, Train Loss: 0.03229, lr: 0.000125\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 855/1000, Train Loss: 0.03229, lr: 0.000125\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 856/1000, Train Loss: 0.03229, lr: 0.000125\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 857/1000, Train Loss: 0.03229, lr: 0.000125\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 858/1000, Train Loss: 0.03229, lr: 0.000125\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 859/1000, Train Loss: 0.03229, lr: 0.000125\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 860/1000, Train Loss: 0.03229, lr: 0.000125\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 861/1000, Train Loss: 0.03229, lr: 0.000125\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 862/1000, Train Loss: 0.03229, lr: 0.000125\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 863/1000, Train Loss: 0.03229, lr: 0.000125\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 864/1000, Train Loss: 0.03229, lr: 0.000125\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 865/1000, Train Loss: 0.03229, lr: 0.000125\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 866/1000, Train Loss: 0.03228, lr: 0.000125\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 867/1000, Train Loss: 0.03228, lr: 0.000125\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 868/1000, Train Loss: 0.03228, lr: 0.000125\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 869/1000, Train Loss: 0.03228, lr: 0.000125\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 870/1000, Train Loss: 0.03228, lr: 0.000125\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 871/1000, Train Loss: 0.03228, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 872/1000, Train Loss: 0.03228, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 873/1000, Train Loss: 0.03228, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 874/1000, Train Loss: 0.03228, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 875/1000, Train Loss: 0.03228, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 876/1000, Train Loss: 0.03228, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 877/1000, Train Loss: 0.03228, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 878/1000, Train Loss: 0.03228, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 879/1000, Train Loss: 0.03227, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 880/1000, Train Loss: 0.03227, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 881/1000, Train Loss: 0.03227, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 882/1000, Train Loss: 0.03227, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 883/1000, Train Loss: 0.03227, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 884/1000, Train Loss: 0.03227, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 885/1000, Train Loss: 0.03227, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 886/1000, Train Loss: 0.03227, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 887/1000, Train Loss: 0.03227, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 888/1000, Train Loss: 0.03227, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 889/1000, Train Loss: 0.03227, lr: 0.000125\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 890/1000, Train Loss: 0.03227, lr: 0.000125\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 891/1000, Train Loss: 0.03226, lr: 0.000125\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 892/1000, Train Loss: 0.03226, lr: 0.000125\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 893/1000, Train Loss: 0.03226, lr: 0.000125\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 894/1000, Train Loss: 0.03226, lr: 0.000125\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 895/1000, Train Loss: 0.03226, lr: 0.000125\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 896/1000, Train Loss: 0.03226, lr: 0.000125\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 897/1000, Train Loss: 0.03226, lr: 0.000125\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 898/1000, Train Loss: 0.03226, lr: 0.000125\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 899/1000, Train Loss: 0.03226, lr: 0.000125\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 900/1000, Train Loss: 0.03226, lr: 0.000125\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 901/1000, Train Loss: 0.03226, lr: 0.000125\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 902/1000, Train Loss: 0.03226, lr: 0.000125\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 903/1000, Train Loss: 0.03226, lr: 0.000125\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 904/1000, Train Loss: 0.03225, lr: 0.000125\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 905/1000, Train Loss: 0.03225, lr: 0.000125\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 906/1000, Train Loss: 0.03225, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 907/1000, Train Loss: 0.03225, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 908/1000, Train Loss: 0.03225, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 909/1000, Train Loss: 0.03225, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 910/1000, Train Loss: 0.03225, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 911/1000, Train Loss: 0.03225, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 912/1000, Train Loss: 0.03225, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 913/1000, Train Loss: 0.03225, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 914/1000, Train Loss: 0.03225, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 915/1000, Train Loss: 0.03225, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 916/1000, Train Loss: 0.03225, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 917/1000, Train Loss: 0.03225, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 918/1000, Train Loss: 0.03224, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 919/1000, Train Loss: 0.03224, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 920/1000, Train Loss: 0.03224, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 921/1000, Train Loss: 0.03224, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 922/1000, Train Loss: 0.03224, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 923/1000, Train Loss: 0.03224, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 924/1000, Train Loss: 0.03224, lr: 0.000125\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 925/1000, Train Loss: 0.03224, lr: 0.000125\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 926/1000, Train Loss: 0.03224, lr: 0.000125\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 927/1000, Train Loss: 0.03224, lr: 0.000125\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 928/1000, Train Loss: 0.03224, lr: 0.000125\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 929/1000, Train Loss: 0.03224, lr: 0.000125\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 930/1000, Train Loss: 0.03224, lr: 0.000125\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 931/1000, Train Loss: 0.03224, lr: 0.000125\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 932/1000, Train Loss: 0.03223, lr: 0.000125\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 933/1000, Train Loss: 0.03223, lr: 0.000125\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 934/1000, Train Loss: 0.03223, lr: 0.000125\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 935/1000, Train Loss: 0.03223, lr: 0.000125\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 936/1000, Train Loss: 0.03223, lr: 0.000125\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 937/1000, Train Loss: 0.03223, lr: 0.000125\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 938/1000, Train Loss: 0.03223, lr: 0.000125\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 939/1000, Train Loss: 0.03223, lr: 0.000125\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 940/1000, Train Loss: 0.03223, lr: 0.000125\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 941/1000, Train Loss: 0.03223, lr: 0.000125\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 942/1000, Train Loss: 0.03223, lr: 0.000125\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 943/1000, Train Loss: 0.03223, lr: 0.000125\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 944/1000, Train Loss: 0.03223, lr: 0.000125\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 945/1000, Train Loss: 0.03223, lr: 0.000125\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 946/1000, Train Loss: 0.03223, lr: 0.000125\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 947/1000, Train Loss: 0.03222, lr: 0.000125\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 948/1000, Train Loss: 0.03222, lr: 0.000125\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 949/1000, Train Loss: 0.03222, lr: 0.000125\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 950/1000, Train Loss: 0.03222, lr: 0.000125\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 951/1000, Train Loss: 0.03222, lr: 0.000125\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 952/1000, Train Loss: 0.03222, lr: 0.000125\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 953/1000, Train Loss: 0.03222, lr: 0.000125\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 954/1000, Train Loss: 0.03222, lr: 0.000125\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 955/1000, Train Loss: 0.03222, lr: 0.000125\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 956/1000, Train Loss: 0.03222, lr: 0.000125\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 957/1000, Train Loss: 0.03222, lr: 0.000125\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 958/1000, Train Loss: 0.03222, lr: 0.000125\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 959/1000, Train Loss: 0.03222, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 960/1000, Train Loss: 0.03221, lr: 0.000125\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 961/1000, Train Loss: 0.03221, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 962/1000, Train Loss: 0.03221, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 963/1000, Train Loss: 0.03221, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 964/1000, Train Loss: 0.03221, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 965/1000, Train Loss: 0.03221, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 966/1000, Train Loss: 0.03221, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 967/1000, Train Loss: 0.03221, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 968/1000, Train Loss: 0.03221, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 969/1000, Train Loss: 0.03221, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 970/1000, Train Loss: 0.03221, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 971/1000, Train Loss: 0.03221, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 972/1000, Train Loss: 0.03221, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 973/1000, Train Loss: 0.03221, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 974/1000, Train Loss: 0.03221, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 975/1000, Train Loss: 0.03221, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 976/1000, Train Loss: 0.03220, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 977/1000, Train Loss: 0.03220, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 978/1000, Train Loss: 0.03220, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 979/1000, Train Loss: 0.03220, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 980/1000, Train Loss: 0.03220, lr: 0.000125\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 981/1000, Train Loss: 0.03220, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 982/1000, Train Loss: 0.03220, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 983/1000, Train Loss: 0.03220, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 984/1000, Train Loss: 0.03220, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 985/1000, Train Loss: 0.03220, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 986/1000, Train Loss: 0.03220, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 987/1000, Train Loss: 0.03220, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 988/1000, Train Loss: 0.03220, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 989/1000, Train Loss: 0.03220, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 990/1000, Train Loss: 0.03220, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 991/1000, Train Loss: 0.03219, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 992/1000, Train Loss: 0.03219, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 993/1000, Train Loss: 0.03219, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 994/1000, Train Loss: 0.03219, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 995/1000, Train Loss: 0.03219, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 996/1000, Train Loss: 0.03219, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 997/1000, Train Loss: 0.03219, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 998/1000, Train Loss: 0.03219, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 999/1000, Train Loss: 0.03219, lr: 0.000125\n",
      "Val Loss: 0.03409\n",
      "---------\n",
      "Epoch 1000/1000, Train Loss: 0.03219, lr: 0.000125\n",
      "Val Loss: 0.03408\n",
      "---------\n",
      "6559.78499007225 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1460, 32, 48, 6, 1), y_hat_i: (4, 32, 48, 6, 1), y_i: (4, 32, 48, 6, 1), batch.x: torch.Size([128, 48, 6, 6]), y: (1460, 32, 48, 6, 1)\n",
      "RMSE for t2m: 1.6365369413103699; MAE for t2m: 1.2347697555721118;\n",
      "RMSE for sp: 1.3469031717486248; MAE for sp: 1.0011758922094987;\n",
      "RMSE for tcc: 0.2882938362014774; MAE for tcc: 0.19658766167662012;\n",
      "RMSE for u10: 1.2882589221340313; MAE for u10: 0.9525732894721486;\n",
      "RMSE for v10: 1.2937306830296469; MAE for v10: 0.9443348800613615;\n",
      "RMSE for tp: 0.2954103775379109; MAE for tp: 0.08010829353779521;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1460, 32, 48, 6, 1), y_hat_i: (4, 32, 48, 6, 1), y_i: (4, 32, 48, 6, 1), batch.x: torch.Size([128, 48, 6, 6]), y: (1460, 32, 48, 6, 1)\n",
      "RMSE for t2m: 1.6365369413103699; MAE for t2m: 1.2347697555721118;\n",
      "RMSE for sp: 1.3469031717486248; MAE for sp: 1.0011758922094987;\n",
      "RMSE for tcc: 0.2879348691632412; MAE for tcc: 0.19552819459559648;\n",
      "RMSE for u10: 1.2882589221340313; MAE for u10: 0.9525732894721486;\n",
      "RMSE for v10: 1.2937306830296469; MAE for v10: 0.9443348800613615;\n",
      "RMSE for tp: 0.2954103775379109; MAE for tp: 0.08010829353779521;\n",
      "Epoch 1/1000, Train Loss: 0.07188, lr: 0.001-----------------------| 54.5% Complete\n",
      "Val Loss: 0.06457\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.06085, lr: 0.001\n",
      "Val Loss: 0.05735\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.05624, lr: 0.001\n",
      "Val Loss: 0.05450\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.05324, lr: 0.001\n",
      "Val Loss: 0.05213\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.05011, lr: 0.001\n",
      "Val Loss: 0.04925\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.04702, lr: 0.001\n",
      "Val Loss: 0.04672\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.04487, lr: 0.001\n",
      "Val Loss: 0.04643\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.04383, lr: 0.001\n",
      "Val Loss: 0.04530\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.04323, lr: 0.001\n",
      "Val Loss: 0.04461\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.04289, lr: 0.001\n",
      "Val Loss: 0.04397\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.04260, lr: 0.001\n",
      "Val Loss: 0.04364\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.04234, lr: 0.001\n",
      "Val Loss: 0.04334\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.04206, lr: 0.001\n",
      "Val Loss: 0.04311\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.04184, lr: 0.001\n",
      "Val Loss: 0.04282\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.04153, lr: 0.001\n",
      "Val Loss: 0.04251\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.04011, lr: 0.001\n",
      "Val Loss: 0.04119\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.03874, lr: 0.001\n",
      "Val Loss: 0.04092\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.03813, lr: 0.001\n",
      "Val Loss: 0.04034\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.03783, lr: 0.001\n",
      "Val Loss: 0.03992\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.03753, lr: 0.001\n",
      "Val Loss: 0.03970\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.03729, lr: 0.001\n",
      "Val Loss: 0.03959\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.03708, lr: 0.001\n",
      "Val Loss: 0.03937\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.03689, lr: 0.001\n",
      "Val Loss: 0.03936\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.03673, lr: 0.001\n",
      "Val Loss: 0.03921\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.03660, lr: 0.001\n",
      "Val Loss: 0.03912\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.03652, lr: 0.001\n",
      "Val Loss: 0.03903\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.03641, lr: 0.001\n",
      "Val Loss: 0.03905\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.03628, lr: 0.001\n",
      "Val Loss: 0.03902\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.03619, lr: 0.001\n",
      "Val Loss: 0.03885\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.03612, lr: 0.001\n",
      "Val Loss: 0.03879\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.03602, lr: 0.001\n",
      "Val Loss: 0.03878\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.03595, lr: 0.001\n",
      "Val Loss: 0.03867\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.03588, lr: 0.001\n",
      "Val Loss: 0.03858\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.03579, lr: 0.001\n",
      "Val Loss: 0.03848\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.03569, lr: 0.001\n",
      "Val Loss: 0.03819\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.03560, lr: 0.001\n",
      "Val Loss: 0.03815\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.03549, lr: 0.001\n",
      "Val Loss: 0.03813\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.03540, lr: 0.001\n",
      "Val Loss: 0.03810\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.03533, lr: 0.001\n",
      "Val Loss: 0.03808\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.03528, lr: 0.001\n",
      "Val Loss: 0.03825\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.03520, lr: 0.001\n",
      "Val Loss: 0.03807\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.03513, lr: 0.001\n",
      "Val Loss: 0.03804\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.03506, lr: 0.001\n",
      "Val Loss: 0.03806\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.03500, lr: 0.001\n",
      "Val Loss: 0.03800\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.03494, lr: 0.001\n",
      "Val Loss: 0.03798\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.03486, lr: 0.001\n",
      "Val Loss: 0.03796\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.03483, lr: 0.001\n",
      "Val Loss: 0.03787\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.03479, lr: 0.001\n",
      "Val Loss: 0.03786\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.03472, lr: 0.001\n",
      "Val Loss: 0.03788\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.03466, lr: 0.001\n",
      "Val Loss: 0.03759\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.03460, lr: 0.001\n",
      "Val Loss: 0.03754\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.03453, lr: 0.001\n",
      "Val Loss: 0.03755\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.03445, lr: 0.001\n",
      "Val Loss: 0.03726\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.03444, lr: 0.001\n",
      "Val Loss: 0.03728\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.03434, lr: 0.001\n",
      "Val Loss: 0.03731\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.03428, lr: 0.001\n",
      "Val Loss: 0.03711\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.03423, lr: 0.001\n",
      "Val Loss: 0.03739\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.03418, lr: 0.001\n",
      "Val Loss: 0.03730\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.03411, lr: 0.001\n",
      "Val Loss: 0.03739\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.03406, lr: 0.001\n",
      "Val Loss: 0.03716\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.03404, lr: 0.001\n",
      "Val Loss: 0.03723\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.03396, lr: 0.001\n",
      "Val Loss: 0.03731\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.03396, lr: 0.001\n",
      "Val Loss: 0.03713\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 64/1000, Train Loss: 0.03314, lr: 0.0005\n",
      "Val Loss: 0.03562\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.03302, lr: 0.0005\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.03297, lr: 0.0005\n",
      "Val Loss: 0.03563\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.03293, lr: 0.0005\n",
      "Val Loss: 0.03568\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.03289, lr: 0.0005\n",
      "Val Loss: 0.03569\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.03285, lr: 0.0005\n",
      "Val Loss: 0.03567\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.03281, lr: 0.0005\n",
      "Val Loss: 0.03567\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.03278, lr: 0.0005\n",
      "Val Loss: 0.03569\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.03274, lr: 0.0005\n",
      "Val Loss: 0.03572\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 73/1000, Train Loss: 0.03238, lr: 0.00025\n",
      "Val Loss: 0.03497\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.03232, lr: 0.00025\n",
      "Val Loss: 0.03494\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.03227, lr: 0.00025\n",
      "Val Loss: 0.03493\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.03224, lr: 0.00025\n",
      "Val Loss: 0.03495\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.03221, lr: 0.00025\n",
      "Val Loss: 0.03496\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.03218, lr: 0.00025\n",
      "Val Loss: 0.03497\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.03216, lr: 0.00025\n",
      "Val Loss: 0.03497\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.03213, lr: 0.00025\n",
      "Val Loss: 0.03497\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.03211, lr: 0.00025\n",
      "Val Loss: 0.03497\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.03209, lr: 0.00025\n",
      "Val Loss: 0.03496\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 83/1000, Train Loss: 0.03188, lr: 0.000125\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.03183, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.03181, lr: 0.000125\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.03179, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.03177, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.03176, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.03175, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.03174, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.03173, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.03171, lr: 0.000125\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.03170, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.03169, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.03168, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.03167, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.03166, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.03165, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.03164, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.03163, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.03162, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.03161, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 103/1000, Train Loss: 0.03160, lr: 0.000125\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.03159, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.03158, lr: 0.000125\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.03157, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.03156, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.03155, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.03154, lr: 0.000125\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.03153, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.03152, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.03151, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.03151, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 114/1000, Train Loss: 0.03150, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.03149, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.03148, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.03147, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.03146, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.03145, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.03144, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 121/1000, Train Loss: 0.03144, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.03143, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.03142, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.03141, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.03140, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.03140, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 127/1000, Train Loss: 0.03139, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 128/1000, Train Loss: 0.03138, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.03137, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.03137, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.03136, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.03135, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.03134, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 134/1000, Train Loss: 0.03134, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 135/1000, Train Loss: 0.03133, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 136/1000, Train Loss: 0.03132, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.03131, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.03131, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.03130, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.03129, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 141/1000, Train Loss: 0.03128, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 142/1000, Train Loss: 0.03128, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.03127, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.03126, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 145/1000, Train Loss: 0.03126, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 146/1000, Train Loss: 0.03125, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 147/1000, Train Loss: 0.03124, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 148/1000, Train Loss: 0.03123, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 149/1000, Train Loss: 0.03123, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 150/1000, Train Loss: 0.03122, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 151/1000, Train Loss: 0.03121, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 152/1000, Train Loss: 0.03121, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 153/1000, Train Loss: 0.03120, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 154/1000, Train Loss: 0.03119, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 155/1000, Train Loss: 0.03119, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 156/1000, Train Loss: 0.03118, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 157/1000, Train Loss: 0.03117, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 158/1000, Train Loss: 0.03117, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 159/1000, Train Loss: 0.03116, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 160/1000, Train Loss: 0.03116, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 161/1000, Train Loss: 0.03115, lr: 0.000125\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 162/1000, Train Loss: 0.03105, lr: 6.25e-05\n",
      "Val Loss: 0.03406\n",
      "---------\n",
      "Epoch 163/1000, Train Loss: 0.03103, lr: 6.25e-05\n",
      "Val Loss: 0.03406\n",
      "---------\n",
      "Epoch 164/1000, Train Loss: 0.03102, lr: 6.25e-05\n",
      "Val Loss: 0.03405\n",
      "---------\n",
      "Epoch 165/1000, Train Loss: 0.03101, lr: 6.25e-05\n",
      "Val Loss: 0.03405\n",
      "---------\n",
      "Epoch 166/1000, Train Loss: 0.03101, lr: 6.25e-05\n",
      "Val Loss: 0.03405\n",
      "---------\n",
      "Epoch 167/1000, Train Loss: 0.03100, lr: 6.25e-05\n",
      "Val Loss: 0.03405\n",
      "---------\n",
      "Epoch 168/1000, Train Loss: 0.03100, lr: 6.25e-05\n",
      "Val Loss: 0.03405\n",
      "---------\n",
      "Epoch 169/1000, Train Loss: 0.03099, lr: 6.25e-05\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "Epoch 170/1000, Train Loss: 0.03099, lr: 6.25e-05\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "Epoch 171/1000, Train Loss: 0.03098, lr: 6.25e-05\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "Epoch 172/1000, Train Loss: 0.03098, lr: 6.25e-05\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "Epoch 173/1000, Train Loss: 0.03097, lr: 6.25e-05\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "Epoch 174/1000, Train Loss: 0.03097, lr: 6.25e-05\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "Epoch 175/1000, Train Loss: 0.03096, lr: 6.25e-05\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "Epoch 176/1000, Train Loss: 0.03096, lr: 6.25e-05\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "Epoch 177/1000, Train Loss: 0.03095, lr: 6.25e-05\n",
      "Val Loss: 0.03403\n",
      "---------\n",
      "Epoch 178/1000, Train Loss: 0.03095, lr: 6.25e-05\n",
      "Val Loss: 0.03403\n",
      "---------\n",
      "Epoch 179/1000, Train Loss: 0.03095, lr: 6.25e-05\n",
      "Val Loss: 0.03403\n",
      "---------\n",
      "Epoch 180/1000, Train Loss: 0.03094, lr: 6.25e-05\n",
      "Val Loss: 0.03403\n",
      "---------\n",
      "Epoch 181/1000, Train Loss: 0.03094, lr: 6.25e-05\n",
      "Val Loss: 0.03403\n",
      "---------\n",
      "Epoch 182/1000, Train Loss: 0.03093, lr: 6.25e-05\n",
      "Val Loss: 0.03403\n",
      "---------\n",
      "Epoch 183/1000, Train Loss: 0.03093, lr: 6.25e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 184/1000, Train Loss: 0.03093, lr: 6.25e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 185/1000, Train Loss: 0.03092, lr: 6.25e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 186/1000, Train Loss: 0.03092, lr: 6.25e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 187/1000, Train Loss: 0.03091, lr: 6.25e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 188/1000, Train Loss: 0.03091, lr: 6.25e-05\n",
      "Val Loss: 0.03402\n",
      "---------\n",
      "Epoch 189/1000, Train Loss: 0.03091, lr: 6.25e-05\n",
      "Val Loss: 0.03401\n",
      "---------\n",
      "Epoch 190/1000, Train Loss: 0.03090, lr: 6.25e-05\n",
      "Val Loss: 0.03401\n",
      "---------\n",
      "Epoch 191/1000, Train Loss: 0.03090, lr: 6.25e-05\n",
      "Val Loss: 0.03401\n",
      "---------\n",
      "Epoch 192/1000, Train Loss: 0.03089, lr: 6.25e-05\n",
      "Val Loss: 0.03401\n",
      "---------\n",
      "Epoch 193/1000, Train Loss: 0.03089, lr: 6.25e-05\n",
      "Val Loss: 0.03401\n",
      "---------\n",
      "Epoch 194/1000, Train Loss: 0.03089, lr: 6.25e-05\n",
      "Val Loss: 0.03401\n",
      "---------\n",
      "Epoch 195/1000, Train Loss: 0.03088, lr: 6.25e-05\n",
      "Val Loss: 0.03401\n",
      "---------\n",
      "Epoch 196/1000, Train Loss: 0.03088, lr: 6.25e-05\n",
      "Val Loss: 0.03401\n",
      "---------\n",
      "Epoch 197/1000, Train Loss: 0.03088, lr: 6.25e-05\n",
      "Val Loss: 0.03401\n",
      "---------\n",
      "Epoch 198/1000, Train Loss: 0.03087, lr: 6.25e-05\n",
      "Val Loss: 0.03401\n",
      "---------\n",
      "Epoch 199/1000, Train Loss: 0.03087, lr: 6.25e-05\n",
      "Val Loss: 0.03400\n",
      "---------\n",
      "Epoch 200/1000, Train Loss: 0.03087, lr: 6.25e-05\n",
      "Val Loss: 0.03400\n",
      "---------\n",
      "Epoch 201/1000, Train Loss: 0.03086, lr: 6.25e-05\n",
      "Val Loss: 0.03400\n",
      "---------\n",
      "Epoch 202/1000, Train Loss: 0.03086, lr: 6.25e-05\n",
      "Val Loss: 0.03400\n",
      "---------\n",
      "Epoch 203/1000, Train Loss: 0.03085, lr: 6.25e-05\n",
      "Val Loss: 0.03400\n",
      "---------\n",
      "Epoch 204/1000, Train Loss: 0.03085, lr: 6.25e-05\n",
      "Val Loss: 0.03400\n",
      "---------\n",
      "Epoch 205/1000, Train Loss: 0.03085, lr: 6.25e-05\n",
      "Val Loss: 0.03400\n",
      "---------\n",
      "Epoch 206/1000, Train Loss: 0.03084, lr: 6.25e-05\n",
      "Val Loss: 0.03400\n",
      "---------\n",
      "Epoch 207/1000, Train Loss: 0.03084, lr: 6.25e-05\n",
      "Val Loss: 0.03400\n",
      "---------\n",
      "Epoch 208/1000, Train Loss: 0.03084, lr: 6.25e-05\n",
      "Val Loss: 0.03400\n",
      "---------\n",
      "Epoch 209/1000, Train Loss: 0.03083, lr: 6.25e-05\n",
      "Val Loss: 0.03400\n",
      "---------\n",
      "Epoch 210/1000, Train Loss: 0.03083, lr: 6.25e-05\n",
      "Val Loss: 0.03399\n",
      "---------\n",
      "Epoch 211/1000, Train Loss: 0.03083, lr: 6.25e-05\n",
      "Val Loss: 0.03399\n",
      "---------\n",
      "Epoch 212/1000, Train Loss: 0.03082, lr: 6.25e-05\n",
      "Val Loss: 0.03399\n",
      "---------\n",
      "Epoch 213/1000, Train Loss: 0.03082, lr: 6.25e-05\n",
      "Val Loss: 0.03399\n",
      "---------\n",
      "Epoch 214/1000, Train Loss: 0.03082, lr: 6.25e-05\n",
      "Val Loss: 0.03399\n",
      "---------\n",
      "Epoch 215/1000, Train Loss: 0.03081, lr: 6.25e-05\n",
      "Val Loss: 0.03399\n",
      "---------\n",
      "Epoch 216/1000, Train Loss: 0.03081, lr: 6.25e-05\n",
      "Val Loss: 0.03399\n",
      "---------\n",
      "Epoch 217/1000, Train Loss: 0.03080, lr: 6.25e-05\n",
      "Val Loss: 0.03399\n",
      "---------\n",
      "Epoch 218/1000, Train Loss: 0.03080, lr: 6.25e-05\n",
      "Val Loss: 0.03399\n",
      "---------\n",
      "Epoch 219/1000, Train Loss: 0.03080, lr: 6.25e-05\n",
      "Val Loss: 0.03399\n",
      "---------\n",
      "Epoch 220/1000, Train Loss: 0.03079, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 221/1000, Train Loss: 0.03079, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 222/1000, Train Loss: 0.03079, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 223/1000, Train Loss: 0.03078, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 224/1000, Train Loss: 0.03078, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 225/1000, Train Loss: 0.03078, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 226/1000, Train Loss: 0.03077, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 227/1000, Train Loss: 0.03077, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 228/1000, Train Loss: 0.03077, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 229/1000, Train Loss: 0.03076, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 230/1000, Train Loss: 0.03076, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 231/1000, Train Loss: 0.03076, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 232/1000, Train Loss: 0.03075, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 233/1000, Train Loss: 0.03075, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 234/1000, Train Loss: 0.03075, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 235/1000, Train Loss: 0.03074, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 236/1000, Train Loss: 0.03074, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 237/1000, Train Loss: 0.03074, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 238/1000, Train Loss: 0.03073, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 239/1000, Train Loss: 0.03073, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 240/1000, Train Loss: 0.03073, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 241/1000, Train Loss: 0.03073, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 242/1000, Train Loss: 0.03072, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 243/1000, Train Loss: 0.03072, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 244/1000, Train Loss: 0.03072, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 245/1000, Train Loss: 0.03071, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 246/1000, Train Loss: 0.03071, lr: 6.25e-05\n",
      "Val Loss: 0.03398\n",
      "---------\n",
      "Epoch 247/1000, Train Loss: 0.03071, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 248/1000, Train Loss: 0.03070, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 249/1000, Train Loss: 0.03070, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 250/1000, Train Loss: 0.03070, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 251/1000, Train Loss: 0.03069, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 252/1000, Train Loss: 0.03069, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 253/1000, Train Loss: 0.03069, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 254/1000, Train Loss: 0.03069, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 255/1000, Train Loss: 0.03068, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 256/1000, Train Loss: 0.03068, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 257/1000, Train Loss: 0.03068, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 258/1000, Train Loss: 0.03067, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 259/1000, Train Loss: 0.03067, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 260/1000, Train Loss: 0.03067, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 261/1000, Train Loss: 0.03067, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 262/1000, Train Loss: 0.03066, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 263/1000, Train Loss: 0.03066, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 264/1000, Train Loss: 0.03066, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 265/1000, Train Loss: 0.03065, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 266/1000, Train Loss: 0.03065, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 267/1000, Train Loss: 0.03065, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 268/1000, Train Loss: 0.03064, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 269/1000, Train Loss: 0.03064, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 270/1000, Train Loss: 0.03064, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 271/1000, Train Loss: 0.03063, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 272/1000, Train Loss: 0.03063, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 273/1000, Train Loss: 0.03063, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 274/1000, Train Loss: 0.03063, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 275/1000, Train Loss: 0.03062, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 276/1000, Train Loss: 0.03062, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 277/1000, Train Loss: 0.03062, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 278/1000, Train Loss: 0.03061, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 279/1000, Train Loss: 0.03061, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 280/1000, Train Loss: 0.03061, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 281/1000, Train Loss: 0.03061, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 282/1000, Train Loss: 0.03060, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 283/1000, Train Loss: 0.03060, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 284/1000, Train Loss: 0.03060, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 285/1000, Train Loss: 0.03059, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 286/1000, Train Loss: 0.03059, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 287/1000, Train Loss: 0.03059, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 288/1000, Train Loss: 0.03059, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 289/1000, Train Loss: 0.03058, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 290/1000, Train Loss: 0.03058, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 291/1000, Train Loss: 0.03058, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 292/1000, Train Loss: 0.03057, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 293/1000, Train Loss: 0.03057, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 294/1000, Train Loss: 0.03057, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 295/1000, Train Loss: 0.03057, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 296/1000, Train Loss: 0.03056, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 297/1000, Train Loss: 0.03056, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 298/1000, Train Loss: 0.03056, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 299/1000, Train Loss: 0.03056, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 300/1000, Train Loss: 0.03055, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 301/1000, Train Loss: 0.03055, lr: 6.25e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 302/1000, Train Loss: 0.03055, lr: 6.25e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 303/1000, Train Loss: 0.03055, lr: 6.25e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 304/1000, Train Loss: 0.03054, lr: 6.25e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 305/1000, Train Loss: 0.03054, lr: 6.25e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 306/1000, Train Loss: 0.03054, lr: 6.25e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 307/1000, Train Loss: 0.03054, lr: 6.25e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 308/1000, Train Loss: 0.03053, lr: 6.25e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 309/1000, Train Loss: 0.03053, lr: 6.25e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 310/1000, Train Loss: 0.03053, lr: 6.25e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 311/1000, Train Loss: 0.03052, lr: 6.25e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 312/1000, Train Loss: 0.03052, lr: 6.25e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 313/1000, Train Loss: 0.03052, lr: 6.25e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 314/1000, Train Loss: 0.03052, lr: 6.25e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 315/1000, Train Loss: 0.03051, lr: 6.25e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 316/1000, Train Loss: 0.03051, lr: 6.25e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 317/1000, Train Loss: 0.03046, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 318/1000, Train Loss: 0.03045, lr: 3.125e-05\n",
      "Val Loss: 0.03389\n",
      "---------\n",
      "Epoch 319/1000, Train Loss: 0.03044, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 320/1000, Train Loss: 0.03044, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 321/1000, Train Loss: 0.03044, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 322/1000, Train Loss: 0.03044, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 323/1000, Train Loss: 0.03043, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 324/1000, Train Loss: 0.03043, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 325/1000, Train Loss: 0.03043, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 326/1000, Train Loss: 0.03043, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 327/1000, Train Loss: 0.03043, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 328/1000, Train Loss: 0.03042, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 329/1000, Train Loss: 0.03042, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 330/1000, Train Loss: 0.03042, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 331/1000, Train Loss: 0.03042, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 332/1000, Train Loss: 0.03042, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 333/1000, Train Loss: 0.03042, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 334/1000, Train Loss: 0.03041, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 335/1000, Train Loss: 0.03041, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 336/1000, Train Loss: 0.03041, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 337/1000, Train Loss: 0.03041, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 338/1000, Train Loss: 0.03041, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 339/1000, Train Loss: 0.03040, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 340/1000, Train Loss: 0.03040, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 341/1000, Train Loss: 0.03040, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 342/1000, Train Loss: 0.03040, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 343/1000, Train Loss: 0.03040, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 344/1000, Train Loss: 0.03040, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 345/1000, Train Loss: 0.03039, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 346/1000, Train Loss: 0.03039, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 347/1000, Train Loss: 0.03039, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 348/1000, Train Loss: 0.03039, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 349/1000, Train Loss: 0.03039, lr: 3.125e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 350/1000, Train Loss: 0.03039, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 351/1000, Train Loss: 0.03038, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 352/1000, Train Loss: 0.03038, lr: 3.125e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 353/1000, Train Loss: 0.03038, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 354/1000, Train Loss: 0.03038, lr: 3.125e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 355/1000, Train Loss: 0.03038, lr: 3.125e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 356/1000, Train Loss: 0.03038, lr: 3.125e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 357/1000, Train Loss: 0.03037, lr: 3.125e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 358/1000, Train Loss: 0.03037, lr: 3.125e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 359/1000, Train Loss: 0.03037, lr: 3.125e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 360/1000, Train Loss: 0.03037, lr: 3.125e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 361/1000, Train Loss: 0.03037, lr: 3.125e-05\n",
      "Val Loss: 0.03388\n",
      "---------\n",
      "Epoch 362/1000, Train Loss: 0.03037, lr: 3.125e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 363/1000, Train Loss: 0.03036, lr: 3.125e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 364/1000, Train Loss: 0.03034, lr: 1.5625e-05\n",
      "Val Loss: 0.03385\n",
      "---------\n",
      "Epoch 365/1000, Train Loss: 0.03033, lr: 1.5625e-05\n",
      "Val Loss: 0.03385\n",
      "---------\n",
      "Epoch 366/1000, Train Loss: 0.03033, lr: 1.5625e-05\n",
      "Val Loss: 0.03385\n",
      "---------\n",
      "Epoch 367/1000, Train Loss: 0.03033, lr: 1.5625e-05\n",
      "Val Loss: 0.03385\n",
      "---------\n",
      "Epoch 368/1000, Train Loss: 0.03033, lr: 1.5625e-05\n",
      "Val Loss: 0.03385\n",
      "---------\n",
      "Epoch 369/1000, Train Loss: 0.03032, lr: 1.5625e-05\n",
      "Val Loss: 0.03385\n",
      "---------\n",
      "Epoch 370/1000, Train Loss: 0.03032, lr: 1.5625e-05\n",
      "Val Loss: 0.03385\n",
      "---------\n",
      "Epoch 371/1000, Train Loss: 0.03032, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 372/1000, Train Loss: 0.03032, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 373/1000, Train Loss: 0.03032, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 374/1000, Train Loss: 0.03032, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 375/1000, Train Loss: 0.03032, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 376/1000, Train Loss: 0.03032, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 377/1000, Train Loss: 0.03031, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 378/1000, Train Loss: 0.03031, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 379/1000, Train Loss: 0.03031, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 380/1000, Train Loss: 0.03031, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 381/1000, Train Loss: 0.03031, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 382/1000, Train Loss: 0.03031, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 383/1000, Train Loss: 0.03031, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 384/1000, Train Loss: 0.03031, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 385/1000, Train Loss: 0.03031, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 386/1000, Train Loss: 0.03030, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 387/1000, Train Loss: 0.03030, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 388/1000, Train Loss: 0.03030, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 389/1000, Train Loss: 0.03030, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 390/1000, Train Loss: 0.03030, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 391/1000, Train Loss: 0.03030, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 392/1000, Train Loss: 0.03030, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 393/1000, Train Loss: 0.03030, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 394/1000, Train Loss: 0.03030, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 395/1000, Train Loss: 0.03030, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 396/1000, Train Loss: 0.03030, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 397/1000, Train Loss: 0.03029, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 398/1000, Train Loss: 0.03029, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 399/1000, Train Loss: 0.03029, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 400/1000, Train Loss: 0.03029, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 401/1000, Train Loss: 0.03029, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 402/1000, Train Loss: 0.03029, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 403/1000, Train Loss: 0.03029, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 404/1000, Train Loss: 0.03029, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 405/1000, Train Loss: 0.03029, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 406/1000, Train Loss: 0.03029, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 407/1000, Train Loss: 0.03029, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 408/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 409/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 410/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 411/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 412/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 413/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 414/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 415/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 416/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 417/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 418/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 419/1000, Train Loss: 0.03026, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 420/1000, Train Loss: 0.03026, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 421/1000, Train Loss: 0.03025, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 422/1000, Train Loss: 0.03025, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 423/1000, Train Loss: 0.03025, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 424/1000, Train Loss: 0.03025, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 425/1000, Train Loss: 0.03025, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 426/1000, Train Loss: 0.03025, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 427/1000, Train Loss: 0.03025, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 428/1000, Train Loss: 0.03025, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 429/1000, Train Loss: 0.03025, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 430/1000, Train Loss: 0.03025, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 431/1000, Train Loss: 0.03025, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 432/1000, Train Loss: 0.03025, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 433/1000, Train Loss: 0.03025, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 434/1000, Train Loss: 0.03025, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 435/1000, Train Loss: 0.03025, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 436/1000, Train Loss: 0.03025, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 437/1000, Train Loss: 0.03025, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 438/1000, Train Loss: 0.03024, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 439/1000, Train Loss: 0.03024, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 440/1000, Train Loss: 0.03024, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 441/1000, Train Loss: 0.03024, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 442/1000, Train Loss: 0.03024, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 443/1000, Train Loss: 0.03024, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 444/1000, Train Loss: 0.03023, lr: 3.90625e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 445/1000, Train Loss: 0.03023, lr: 3.90625e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 446/1000, Train Loss: 0.03023, lr: 3.90625e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 447/1000, Train Loss: 0.03023, lr: 3.90625e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 448/1000, Train Loss: 0.03023, lr: 3.90625e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 449/1000, Train Loss: 0.03023, lr: 3.90625e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 450/1000, Train Loss: 0.03023, lr: 3.90625e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 451/1000, Train Loss: 0.03023, lr: 3.90625e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 452/1000, Train Loss: 0.03022, lr: 1.953125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 453/1000, Train Loss: 0.03022, lr: 1.953125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 454/1000, Train Loss: 0.03022, lr: 1.953125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 455/1000, Train Loss: 0.03022, lr: 1.953125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 456/1000, Train Loss: 0.03022, lr: 1.953125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 457/1000, Train Loss: 0.03022, lr: 1.953125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 458/1000, Train Loss: 0.03022, lr: 1.953125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 459/1000, Train Loss: 0.03022, lr: 1.953125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 460/1000, Train Loss: 0.03022, lr: 9.765625e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 461/1000, Train Loss: 0.03021, lr: 9.765625e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 462/1000, Train Loss: 0.03021, lr: 9.765625e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 463/1000, Train Loss: 0.03021, lr: 9.765625e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 464/1000, Train Loss: 0.03021, lr: 9.765625e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 465/1000, Train Loss: 0.03021, lr: 9.765625e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 466/1000, Train Loss: 0.03021, lr: 9.765625e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 467/1000, Train Loss: 0.03021, lr: 4.8828125e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 468/1000, Train Loss: 0.03021, lr: 4.8828125e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 469/1000, Train Loss: 0.03021, lr: 4.8828125e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 470/1000, Train Loss: 0.03021, lr: 4.8828125e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 471/1000, Train Loss: 0.03021, lr: 4.8828125e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 472/1000, Train Loss: 0.03021, lr: 4.8828125e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 473/1000, Train Loss: 0.03021, lr: 4.8828125e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 474/1000, Train Loss: 0.03021, lr: 2.44140625e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 475/1000, Train Loss: 0.03021, lr: 2.44140625e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 476/1000, Train Loss: 0.03021, lr: 2.44140625e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 477/1000, Train Loss: 0.03021, lr: 2.44140625e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 478/1000, Train Loss: 0.03021, lr: 2.44140625e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 479/1000, Train Loss: 0.03021, lr: 2.44140625e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 480/1000, Train Loss: 0.03021, lr: 2.44140625e-07\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 481/1000, Train Loss: 0.03021, lr: 1.220703125e-07\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 482/1000, Train Loss: 0.03021, lr: 1.220703125e-07\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 483/1000, Train Loss: 0.03021, lr: 1.220703125e-07\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 484/1000, Train Loss: 0.03021, lr: 1.220703125e-07\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 485/1000, Train Loss: 0.03021, lr: 1.220703125e-07\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 486/1000, Train Loss: 0.03021, lr: 1.220703125e-07\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 487/1000, Train Loss: 0.03021, lr: 1.220703125e-07\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 488/1000, Train Loss: 0.03021, lr: 6.103515625e-08\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 489/1000, Train Loss: 0.03021, lr: 6.103515625e-08\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 490/1000, Train Loss: 0.03021, lr: 6.103515625e-08\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 491/1000, Train Loss: 0.03021, lr: 6.103515625e-08\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 492/1000, Train Loss: 0.03021, lr: 6.103515625e-08\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Early stopping ....\n",
      "3202.704474210739 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1459, 32, 48, 6, 1), y_hat_i: (3, 32, 48, 6, 1), y_i: (3, 32, 48, 6, 1), batch.x: torch.Size([96, 48, 7, 6]), y: (1459, 32, 48, 6, 1)\n",
      "RMSE for t2m: 1.6704659655130838; MAE for t2m: 1.2623316222253018;\n",
      "RMSE for sp: 1.5208451658263382; MAE for sp: 1.1446596929573267;\n",
      "RMSE for tcc: 0.2907224223109444; MAE for tcc: 0.19627802981295622;\n",
      "RMSE for u10: 1.239791570394688; MAE for u10: 0.9236860045905183;\n",
      "RMSE for v10: 1.183239513985983; MAE for v10: 0.8803754119190318;\n",
      "RMSE for tp: 0.29146913949964126; MAE for tp: 0.07959529972607791;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 7, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1459, 32, 48, 6, 1), y_hat_i: (3, 32, 48, 6, 1), y_i: (3, 32, 48, 6, 1), batch.x: torch.Size([96, 48, 7, 6]), y: (1459, 32, 48, 6, 1)\n",
      "RMSE for t2m: 1.6704659655130838; MAE for t2m: 1.2623316222253018;\n",
      "RMSE for sp: 1.5208451658263382; MAE for sp: 1.1446596929573267;\n",
      "RMSE for tcc: 0.28986339933039645; MAE for tcc: 0.19487972670332648;\n",
      "RMSE for u10: 1.239791570394688; MAE for u10: 0.9236860045905183;\n",
      "RMSE for v10: 1.183239513985983; MAE for v10: 0.8803754119190318;\n",
      "RMSE for tp: 0.29146913949964126; MAE for tp: 0.07959529972607791;\n",
      "Epoch 1/1000, Train Loss: 0.07441, lr: 0.001-------------------| 63.6% Complete\n",
      "Val Loss: 0.06021\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.05513, lr: 0.001\n",
      "Val Loss: 0.05241\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.04935, lr: 0.001\n",
      "Val Loss: 0.04882\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.04656, lr: 0.001\n",
      "Val Loss: 0.04637\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.04365, lr: 0.001\n",
      "Val Loss: 0.04337\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.04158, lr: 0.001\n",
      "Val Loss: 0.04194\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.04054, lr: 0.001\n",
      "Val Loss: 0.04126\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.04002, lr: 0.001\n",
      "Val Loss: 0.04090\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.03949, lr: 0.001\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.03899, lr: 0.001\n",
      "Val Loss: 0.04037\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.03858, lr: 0.001\n",
      "Val Loss: 0.04013\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.03831, lr: 0.001\n",
      "Val Loss: 0.03992\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.03807, lr: 0.001\n",
      "Val Loss: 0.03958\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.03788, lr: 0.001\n",
      "Val Loss: 0.03926\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.03769, lr: 0.001\n",
      "Val Loss: 0.03896\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.03749, lr: 0.001\n",
      "Val Loss: 0.03896\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.03730, lr: 0.001\n",
      "Val Loss: 0.03883\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.03712, lr: 0.001\n",
      "Val Loss: 0.03880\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.03697, lr: 0.001\n",
      "Val Loss: 0.03865\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.03688, lr: 0.001\n",
      "Val Loss: 0.03858\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.03676, lr: 0.001\n",
      "Val Loss: 0.03853\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.03666, lr: 0.001\n",
      "Val Loss: 0.03843\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.03658, lr: 0.001\n",
      "Val Loss: 0.03831\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.03647, lr: 0.001\n",
      "Val Loss: 0.03832\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.03640, lr: 0.001\n",
      "Val Loss: 0.03818\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.03632, lr: 0.001\n",
      "Val Loss: 0.03809\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.03626, lr: 0.001\n",
      "Val Loss: 0.03796\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.03620, lr: 0.001\n",
      "Val Loss: 0.03792\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.03615, lr: 0.001\n",
      "Val Loss: 0.03783\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.03610, lr: 0.001\n",
      "Val Loss: 0.03782\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.03606, lr: 0.001\n",
      "Val Loss: 0.03781\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.03604, lr: 0.001\n",
      "Val Loss: 0.03774\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.03602, lr: 0.001\n",
      "Val Loss: 0.03770\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.03598, lr: 0.001\n",
      "Val Loss: 0.03750\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.03595, lr: 0.001\n",
      "Val Loss: 0.03743\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.03590, lr: 0.001\n",
      "Val Loss: 0.03743\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.03585, lr: 0.001\n",
      "Val Loss: 0.03745\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.03583, lr: 0.001\n",
      "Val Loss: 0.03737\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.03578, lr: 0.001\n",
      "Val Loss: 0.03737\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.03574, lr: 0.001\n",
      "Val Loss: 0.03731\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.03570, lr: 0.001\n",
      "Val Loss: 0.03732\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.03565, lr: 0.001\n",
      "Val Loss: 0.03731\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.03562, lr: 0.001\n",
      "Val Loss: 0.03732\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.03555, lr: 0.001\n",
      "Val Loss: 0.03733\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.03552, lr: 0.001\n",
      "Val Loss: 0.03730\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.03548, lr: 0.001\n",
      "Val Loss: 0.03724\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.03545, lr: 0.001\n",
      "Val Loss: 0.03721\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.03539, lr: 0.001\n",
      "Val Loss: 0.03725\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.03537, lr: 0.001\n",
      "Val Loss: 0.03721\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.03534, lr: 0.001\n",
      "Val Loss: 0.03719\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.03532, lr: 0.001\n",
      "Val Loss: 0.03711\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.03528, lr: 0.001\n",
      "Val Loss: 0.03708\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.03525, lr: 0.001\n",
      "Val Loss: 0.03702\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.03524, lr: 0.001\n",
      "Val Loss: 0.03691\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.03524, lr: 0.001\n",
      "Val Loss: 0.03692\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.03523, lr: 0.001\n",
      "Val Loss: 0.03691\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.03522, lr: 0.001\n",
      "Val Loss: 0.03692\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.03518, lr: 0.001\n",
      "Val Loss: 0.03688\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.03517, lr: 0.001\n",
      "Val Loss: 0.03681\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.03512, lr: 0.001\n",
      "Val Loss: 0.03682\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.03509, lr: 0.001\n",
      "Val Loss: 0.03684\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.03505, lr: 0.001\n",
      "Val Loss: 0.03680\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.03502, lr: 0.001\n",
      "Val Loss: 0.03681\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.03500, lr: 0.001\n",
      "Val Loss: 0.03671\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.03497, lr: 0.001\n",
      "Val Loss: 0.03671\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.03493, lr: 0.001\n",
      "Val Loss: 0.03668\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.03493, lr: 0.001\n",
      "Val Loss: 0.03665\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.03489, lr: 0.001\n",
      "Val Loss: 0.03672\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.03485, lr: 0.001\n",
      "Val Loss: 0.03663\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.03485, lr: 0.001\n",
      "Val Loss: 0.03663\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.03484, lr: 0.001\n",
      "Val Loss: 0.03662\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.03468, lr: 0.001\n",
      "Val Loss: 0.03665\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.03481, lr: 0.001\n",
      "Val Loss: 0.03653\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.03477, lr: 0.001\n",
      "Val Loss: 0.03656\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.03474, lr: 0.001\n",
      "Val Loss: 0.03658\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.03470, lr: 0.001\n",
      "Val Loss: 0.03655\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.03467, lr: 0.001\n",
      "Val Loss: 0.03666\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.03465, lr: 0.001\n",
      "Val Loss: 0.03656\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.03463, lr: 0.001\n",
      "Val Loss: 0.03662\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.03461, lr: 0.001\n",
      "Val Loss: 0.03660\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 81/1000, Train Loss: 0.03381, lr: 0.0005\n",
      "Val Loss: 0.03576\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.03365, lr: 0.0005\n",
      "Val Loss: 0.03569\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.03361, lr: 0.0005\n",
      "Val Loss: 0.03566\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.03359, lr: 0.0005\n",
      "Val Loss: 0.03565\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.03356, lr: 0.0005\n",
      "Val Loss: 0.03564\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.03354, lr: 0.0005\n",
      "Val Loss: 0.03564\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.03352, lr: 0.0005\n",
      "Val Loss: 0.03562\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.03350, lr: 0.0005\n",
      "Val Loss: 0.03561\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.03348, lr: 0.0005\n",
      "Val Loss: 0.03561\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.03347, lr: 0.0005\n",
      "Val Loss: 0.03560\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.03345, lr: 0.0005\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.03343, lr: 0.0005\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.03342, lr: 0.0005\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.03340, lr: 0.0005\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.03338, lr: 0.0005\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.03337, lr: 0.0005\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.03335, lr: 0.0005\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.03333, lr: 0.0005\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.03332, lr: 0.0005\n",
      "Val Loss: 0.03555\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.03331, lr: 0.0005\n",
      "Val Loss: 0.03554\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.03329, lr: 0.0005\n",
      "Val Loss: 0.03554\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.03328, lr: 0.0005\n",
      "Val Loss: 0.03553\n",
      "---------\n",
      "Epoch 103/1000, Train Loss: 0.03326, lr: 0.0005\n",
      "Val Loss: 0.03553\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.03325, lr: 0.0005\n",
      "Val Loss: 0.03554\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.03323, lr: 0.0005\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.03321, lr: 0.0005\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.03319, lr: 0.0005\n",
      "Val Loss: 0.03548\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.03317, lr: 0.0005\n",
      "Val Loss: 0.03548\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.03314, lr: 0.0005\n",
      "Val Loss: 0.03549\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.03312, lr: 0.0005\n",
      "Val Loss: 0.03543\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.03309, lr: 0.0005\n",
      "Val Loss: 0.03541\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.03308, lr: 0.0005\n",
      "Val Loss: 0.03539\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.03306, lr: 0.0005\n",
      "Val Loss: 0.03539\n",
      "---------\n",
      "Epoch 114/1000, Train Loss: 0.03304, lr: 0.0005\n",
      "Val Loss: 0.03540\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.03303, lr: 0.0005\n",
      "Val Loss: 0.03538\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.03301, lr: 0.0005\n",
      "Val Loss: 0.03538\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.03300, lr: 0.0005\n",
      "Val Loss: 0.03538\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.03298, lr: 0.0005\n",
      "Val Loss: 0.03538\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.03296, lr: 0.0005\n",
      "Val Loss: 0.03537\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.03295, lr: 0.0005\n",
      "Val Loss: 0.03535\n",
      "---------\n",
      "Epoch 121/1000, Train Loss: 0.03294, lr: 0.0005\n",
      "Val Loss: 0.03533\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.03292, lr: 0.0005\n",
      "Val Loss: 0.03533\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.03291, lr: 0.0005\n",
      "Val Loss: 0.03533\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.03289, lr: 0.0005\n",
      "Val Loss: 0.03533\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.03288, lr: 0.0005\n",
      "Val Loss: 0.03532\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.03286, lr: 0.0005\n",
      "Val Loss: 0.03533\n",
      "---------\n",
      "Epoch 127/1000, Train Loss: 0.03285, lr: 0.0005\n",
      "Val Loss: 0.03533\n",
      "---------\n",
      "Epoch 128/1000, Train Loss: 0.03283, lr: 0.0005\n",
      "Val Loss: 0.03533\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.03282, lr: 0.0005\n",
      "Val Loss: 0.03535\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.03281, lr: 0.0005\n",
      "Val Loss: 0.03533\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.03280, lr: 0.0005\n",
      "Val Loss: 0.03531\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.03278, lr: 0.0005\n",
      "Val Loss: 0.03529\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.03277, lr: 0.0005\n",
      "Val Loss: 0.03529\n",
      "---------\n",
      "Epoch 134/1000, Train Loss: 0.03276, lr: 0.0005\n",
      "Val Loss: 0.03529\n",
      "---------\n",
      "Epoch 135/1000, Train Loss: 0.03274, lr: 0.0005\n",
      "Val Loss: 0.03528\n",
      "---------\n",
      "Epoch 136/1000, Train Loss: 0.03273, lr: 0.0005\n",
      "Val Loss: 0.03528\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.03272, lr: 0.0005\n",
      "Val Loss: 0.03529\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.03271, lr: 0.0005\n",
      "Val Loss: 0.03528\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.03270, lr: 0.0005\n",
      "Val Loss: 0.03527\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.03269, lr: 0.0005\n",
      "Val Loss: 0.03526\n",
      "---------\n",
      "Epoch 141/1000, Train Loss: 0.03268, lr: 0.0005\n",
      "Val Loss: 0.03525\n",
      "---------\n",
      "Epoch 142/1000, Train Loss: 0.03267, lr: 0.0005\n",
      "Val Loss: 0.03527\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.03266, lr: 0.0005\n",
      "Val Loss: 0.03526\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.03265, lr: 0.0005\n",
      "Val Loss: 0.03526\n",
      "---------\n",
      "Epoch 145/1000, Train Loss: 0.03264, lr: 0.0005\n",
      "Val Loss: 0.03527\n",
      "---------\n",
      "Epoch 146/1000, Train Loss: 0.03262, lr: 0.0005\n",
      "Val Loss: 0.03528\n",
      "---------\n",
      "Epoch 147/1000, Train Loss: 0.03262, lr: 0.0005\n",
      "Val Loss: 0.03527\n",
      "---------\n",
      "Epoch 148/1000, Train Loss: 0.03260, lr: 0.0005\n",
      "Val Loss: 0.03524\n",
      "---------\n",
      "Epoch 149/1000, Train Loss: 0.03259, lr: 0.0005\n",
      "Val Loss: 0.03523\n",
      "---------\n",
      "Epoch 150/1000, Train Loss: 0.03257, lr: 0.0005\n",
      "Val Loss: 0.03524\n",
      "---------\n",
      "Epoch 151/1000, Train Loss: 0.03256, lr: 0.0005\n",
      "Val Loss: 0.03529\n",
      "---------\n",
      "Epoch 152/1000, Train Loss: 0.03254, lr: 0.0005\n",
      "Val Loss: 0.03525\n",
      "---------\n",
      "Epoch 153/1000, Train Loss: 0.03253, lr: 0.0005\n",
      "Val Loss: 0.03525\n",
      "---------\n",
      "Epoch 154/1000, Train Loss: 0.03252, lr: 0.0005\n",
      "Val Loss: 0.03527\n",
      "---------\n",
      "Epoch 155/1000, Train Loss: 0.03252, lr: 0.0005\n",
      "Val Loss: 0.03528\n",
      "---------\n",
      "Epoch 156/1000, Train Loss: 0.03250, lr: 0.0005\n",
      "Val Loss: 0.03528\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 157/1000, Train Loss: 0.03234, lr: 0.00025\n",
      "Val Loss: 0.03473\n",
      "---------\n",
      "Epoch 158/1000, Train Loss: 0.03230, lr: 0.00025\n",
      "Val Loss: 0.03470\n",
      "---------\n",
      "Epoch 159/1000, Train Loss: 0.03226, lr: 0.00025\n",
      "Val Loss: 0.03467\n",
      "---------\n",
      "Epoch 160/1000, Train Loss: 0.03224, lr: 0.00025\n",
      "Val Loss: 0.03467\n",
      "---------\n",
      "Epoch 161/1000, Train Loss: 0.03223, lr: 0.00025\n",
      "Val Loss: 0.03466\n",
      "---------\n",
      "Epoch 162/1000, Train Loss: 0.03221, lr: 0.00025\n",
      "Val Loss: 0.03466\n",
      "---------\n",
      "Epoch 163/1000, Train Loss: 0.03219, lr: 0.00025\n",
      "Val Loss: 0.03466\n",
      "---------\n",
      "Epoch 164/1000, Train Loss: 0.03218, lr: 0.00025\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 165/1000, Train Loss: 0.03217, lr: 0.00025\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 166/1000, Train Loss: 0.03216, lr: 0.00025\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 167/1000, Train Loss: 0.03214, lr: 0.00025\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 168/1000, Train Loss: 0.03213, lr: 0.00025\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 169/1000, Train Loss: 0.03212, lr: 0.00025\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 170/1000, Train Loss: 0.03212, lr: 0.00025\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 171/1000, Train Loss: 0.03211, lr: 0.00025\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 172/1000, Train Loss: 0.03210, lr: 0.00025\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 173/1000, Train Loss: 0.03209, lr: 0.00025\n",
      "Val Loss: 0.03463\n",
      "---------\n",
      "Epoch 174/1000, Train Loss: 0.03208, lr: 0.00025\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 175/1000, Train Loss: 0.03207, lr: 0.00025\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 176/1000, Train Loss: 0.03207, lr: 0.00025\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 177/1000, Train Loss: 0.03206, lr: 0.00025\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 178/1000, Train Loss: 0.03205, lr: 0.00025\n",
      "Val Loss: 0.03466\n",
      "---------\n",
      "Epoch 179/1000, Train Loss: 0.03204, lr: 0.00025\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 180/1000, Train Loss: 0.03203, lr: 0.00025\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 181/1000, Train Loss: 0.03187, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 182/1000, Train Loss: 0.03183, lr: 0.000125\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 183/1000, Train Loss: 0.03182, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 184/1000, Train Loss: 0.03181, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 185/1000, Train Loss: 0.03180, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 186/1000, Train Loss: 0.03179, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 187/1000, Train Loss: 0.03178, lr: 0.000125\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 188/1000, Train Loss: 0.03178, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 189/1000, Train Loss: 0.03177, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 190/1000, Train Loss: 0.03177, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 191/1000, Train Loss: 0.03176, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 192/1000, Train Loss: 0.03175, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 193/1000, Train Loss: 0.03175, lr: 0.000125\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 194/1000, Train Loss: 0.03174, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 195/1000, Train Loss: 0.03174, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 196/1000, Train Loss: 0.03173, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 197/1000, Train Loss: 0.03173, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 198/1000, Train Loss: 0.03172, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 199/1000, Train Loss: 0.03172, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 200/1000, Train Loss: 0.03171, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 201/1000, Train Loss: 0.03170, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 202/1000, Train Loss: 0.03170, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 203/1000, Train Loss: 0.03169, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 204/1000, Train Loss: 0.03169, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 205/1000, Train Loss: 0.03169, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 206/1000, Train Loss: 0.03168, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 207/1000, Train Loss: 0.03168, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 208/1000, Train Loss: 0.03167, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 209/1000, Train Loss: 0.03167, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 210/1000, Train Loss: 0.03166, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 211/1000, Train Loss: 0.03166, lr: 0.000125\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 212/1000, Train Loss: 0.03166, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 213/1000, Train Loss: 0.03165, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 214/1000, Train Loss: 0.03165, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 215/1000, Train Loss: 0.03164, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 216/1000, Train Loss: 0.03164, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 217/1000, Train Loss: 0.03163, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 218/1000, Train Loss: 0.03163, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 219/1000, Train Loss: 0.03162, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 220/1000, Train Loss: 0.03162, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 221/1000, Train Loss: 0.03162, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 222/1000, Train Loss: 0.03161, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 223/1000, Train Loss: 0.03161, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 224/1000, Train Loss: 0.03161, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 225/1000, Train Loss: 0.03160, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 226/1000, Train Loss: 0.03160, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 227/1000, Train Loss: 0.03159, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 228/1000, Train Loss: 0.03159, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 229/1000, Train Loss: 0.03159, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 230/1000, Train Loss: 0.03158, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 231/1000, Train Loss: 0.03158, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 232/1000, Train Loss: 0.03158, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 233/1000, Train Loss: 0.03157, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 234/1000, Train Loss: 0.03157, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 235/1000, Train Loss: 0.03156, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 236/1000, Train Loss: 0.03156, lr: 0.000125\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 237/1000, Train Loss: 0.03146, lr: 6.25e-05\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 238/1000, Train Loss: 0.03144, lr: 6.25e-05\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 239/1000, Train Loss: 0.03143, lr: 6.25e-05\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 240/1000, Train Loss: 0.03143, lr: 6.25e-05\n",
      "Val Loss: 0.03420\n",
      "---------\n",
      "Epoch 241/1000, Train Loss: 0.03142, lr: 6.25e-05\n",
      "Val Loss: 0.03420\n",
      "---------\n",
      "Epoch 242/1000, Train Loss: 0.03142, lr: 6.25e-05\n",
      "Val Loss: 0.03420\n",
      "---------\n",
      "Epoch 243/1000, Train Loss: 0.03142, lr: 6.25e-05\n",
      "Val Loss: 0.03420\n",
      "---------\n",
      "Epoch 244/1000, Train Loss: 0.03141, lr: 6.25e-05\n",
      "Val Loss: 0.03420\n",
      "---------\n",
      "Epoch 245/1000, Train Loss: 0.03141, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 246/1000, Train Loss: 0.03141, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 247/1000, Train Loss: 0.03140, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 248/1000, Train Loss: 0.03140, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 249/1000, Train Loss: 0.03140, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 250/1000, Train Loss: 0.03140, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 251/1000, Train Loss: 0.03139, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 252/1000, Train Loss: 0.03139, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 253/1000, Train Loss: 0.03139, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 254/1000, Train Loss: 0.03139, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 255/1000, Train Loss: 0.03138, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 256/1000, Train Loss: 0.03138, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 257/1000, Train Loss: 0.03138, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 258/1000, Train Loss: 0.03138, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 259/1000, Train Loss: 0.03137, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 260/1000, Train Loss: 0.03137, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 261/1000, Train Loss: 0.03137, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 262/1000, Train Loss: 0.03137, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 263/1000, Train Loss: 0.03136, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 264/1000, Train Loss: 0.03136, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 265/1000, Train Loss: 0.03136, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 266/1000, Train Loss: 0.03136, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 267/1000, Train Loss: 0.03136, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 268/1000, Train Loss: 0.03135, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 269/1000, Train Loss: 0.03135, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 270/1000, Train Loss: 0.03135, lr: 6.25e-05\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 271/1000, Train Loss: 0.03135, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 272/1000, Train Loss: 0.03135, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 273/1000, Train Loss: 0.03134, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 274/1000, Train Loss: 0.03134, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 275/1000, Train Loss: 0.03134, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 276/1000, Train Loss: 0.03134, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 277/1000, Train Loss: 0.03133, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 278/1000, Train Loss: 0.03133, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 279/1000, Train Loss: 0.03133, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 280/1000, Train Loss: 0.03133, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 281/1000, Train Loss: 0.03133, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 282/1000, Train Loss: 0.03132, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 283/1000, Train Loss: 0.03132, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 284/1000, Train Loss: 0.03132, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 285/1000, Train Loss: 0.03132, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 286/1000, Train Loss: 0.03131, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 287/1000, Train Loss: 0.03131, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 288/1000, Train Loss: 0.03131, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 289/1000, Train Loss: 0.03131, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 290/1000, Train Loss: 0.03131, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 291/1000, Train Loss: 0.03130, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 292/1000, Train Loss: 0.03130, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 293/1000, Train Loss: 0.03130, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 294/1000, Train Loss: 0.03130, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 295/1000, Train Loss: 0.03130, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 296/1000, Train Loss: 0.03130, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 297/1000, Train Loss: 0.03129, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 298/1000, Train Loss: 0.03129, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 299/1000, Train Loss: 0.03129, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 300/1000, Train Loss: 0.03129, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 301/1000, Train Loss: 0.03128, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 302/1000, Train Loss: 0.03128, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 303/1000, Train Loss: 0.03128, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 304/1000, Train Loss: 0.03128, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 305/1000, Train Loss: 0.03128, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 306/1000, Train Loss: 0.03128, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 307/1000, Train Loss: 0.03127, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 308/1000, Train Loss: 0.03127, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 309/1000, Train Loss: 0.03127, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 310/1000, Train Loss: 0.03127, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 311/1000, Train Loss: 0.03127, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 312/1000, Train Loss: 0.03126, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 313/1000, Train Loss: 0.03126, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 314/1000, Train Loss: 0.03126, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 315/1000, Train Loss: 0.03126, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 316/1000, Train Loss: 0.03126, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 317/1000, Train Loss: 0.03126, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 318/1000, Train Loss: 0.03125, lr: 6.25e-05\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 319/1000, Train Loss: 0.03125, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 320/1000, Train Loss: 0.03125, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 321/1000, Train Loss: 0.03125, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 322/1000, Train Loss: 0.03125, lr: 6.25e-05\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 323/1000, Train Loss: 0.03124, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 324/1000, Train Loss: 0.03124, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 325/1000, Train Loss: 0.03124, lr: 6.25e-05\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 326/1000, Train Loss: 0.03118, lr: 3.125e-05\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 327/1000, Train Loss: 0.03117, lr: 3.125e-05\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 328/1000, Train Loss: 0.03117, lr: 3.125e-05\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 329/1000, Train Loss: 0.03117, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 330/1000, Train Loss: 0.03117, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 331/1000, Train Loss: 0.03116, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 332/1000, Train Loss: 0.03116, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 333/1000, Train Loss: 0.03116, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 334/1000, Train Loss: 0.03116, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 335/1000, Train Loss: 0.03116, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 336/1000, Train Loss: 0.03116, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 337/1000, Train Loss: 0.03115, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 338/1000, Train Loss: 0.03115, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 339/1000, Train Loss: 0.03115, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 340/1000, Train Loss: 0.03115, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 341/1000, Train Loss: 0.03115, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 342/1000, Train Loss: 0.03115, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 343/1000, Train Loss: 0.03115, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 344/1000, Train Loss: 0.03115, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 345/1000, Train Loss: 0.03114, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 346/1000, Train Loss: 0.03114, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 347/1000, Train Loss: 0.03114, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 348/1000, Train Loss: 0.03114, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 349/1000, Train Loss: 0.03114, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 350/1000, Train Loss: 0.03114, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 351/1000, Train Loss: 0.03114, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 352/1000, Train Loss: 0.03114, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 353/1000, Train Loss: 0.03113, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 354/1000, Train Loss: 0.03113, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 355/1000, Train Loss: 0.03113, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 356/1000, Train Loss: 0.03113, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 357/1000, Train Loss: 0.03113, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 358/1000, Train Loss: 0.03113, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 359/1000, Train Loss: 0.03113, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 360/1000, Train Loss: 0.03113, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 361/1000, Train Loss: 0.03112, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 362/1000, Train Loss: 0.03112, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 363/1000, Train Loss: 0.03112, lr: 3.125e-05\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 364/1000, Train Loss: 0.03109, lr: 1.5625e-05\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 365/1000, Train Loss: 0.03108, lr: 1.5625e-05\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 366/1000, Train Loss: 0.03108, lr: 1.5625e-05\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 367/1000, Train Loss: 0.03108, lr: 1.5625e-05\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 368/1000, Train Loss: 0.03108, lr: 1.5625e-05\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 369/1000, Train Loss: 0.03108, lr: 1.5625e-05\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 370/1000, Train Loss: 0.03108, lr: 1.5625e-05\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 371/1000, Train Loss: 0.03108, lr: 1.5625e-05\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 372/1000, Train Loss: 0.03106, lr: 7.8125e-06\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 373/1000, Train Loss: 0.03106, lr: 7.8125e-06\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 374/1000, Train Loss: 0.03106, lr: 7.8125e-06\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 375/1000, Train Loss: 0.03106, lr: 7.8125e-06\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 376/1000, Train Loss: 0.03105, lr: 7.8125e-06\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 377/1000, Train Loss: 0.03105, lr: 7.8125e-06\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 378/1000, Train Loss: 0.03105, lr: 7.8125e-06\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 379/1000, Train Loss: 0.03105, lr: 7.8125e-06\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 380/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 381/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 382/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 383/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 384/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 385/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 386/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 387/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 388/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 389/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 390/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 391/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 392/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 393/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 394/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 395/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 396/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 397/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 398/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 399/1000, Train Loss: 0.03104, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 400/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 401/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 402/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 403/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 404/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 405/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 406/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 407/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 408/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 409/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 410/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 411/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 412/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 413/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 414/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 415/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 416/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 417/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 418/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 419/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 420/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 421/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 422/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 423/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 424/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 425/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 426/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 427/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 428/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 429/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 430/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 431/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 432/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 433/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 434/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 435/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 436/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 437/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 438/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 439/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 440/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 441/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 442/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 443/1000, Train Loss: 0.03103, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 444/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 445/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 446/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 447/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 448/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 449/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 450/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 451/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 452/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 453/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 454/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 455/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 456/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 457/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 458/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 459/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 460/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 461/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 462/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 463/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 464/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 465/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 466/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 467/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 468/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 469/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 470/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 471/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 472/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 473/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 474/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 475/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 476/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 477/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 478/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 479/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 480/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 481/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 482/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 483/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 484/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 485/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 486/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 487/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 488/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 489/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 490/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 491/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 492/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 493/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 494/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 495/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 496/1000, Train Loss: 0.03102, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 497/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 498/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 499/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 500/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 501/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 502/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 503/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 504/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 505/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 506/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 507/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 508/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 509/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 510/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 511/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 512/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 513/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 514/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 515/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 516/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 517/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 518/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 519/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 520/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 521/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 522/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 523/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 524/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 525/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 526/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 527/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 528/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 529/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 530/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 531/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 532/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 533/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 534/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 535/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 536/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 537/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 538/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 539/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 540/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 541/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 542/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 543/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 544/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 545/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 546/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 547/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 548/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 549/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 550/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 551/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 552/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 553/1000, Train Loss: 0.03101, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 554/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 555/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 556/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 557/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 558/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 559/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 560/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 561/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 562/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 563/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 564/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 565/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 566/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 567/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 568/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 569/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 570/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 571/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 572/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 573/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 574/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 575/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 576/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 577/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 578/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 579/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 580/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 581/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 582/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 583/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 584/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 585/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 586/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 587/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 588/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 589/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 590/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 591/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 592/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 593/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 594/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 595/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 596/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 597/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 598/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 599/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 600/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 601/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 602/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 603/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 604/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 605/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 606/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 607/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 608/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 609/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 610/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 611/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 612/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 613/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 614/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 615/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 616/1000, Train Loss: 0.03100, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 617/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 618/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 619/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 620/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 621/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 622/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 623/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 624/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 625/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 626/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 627/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 628/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 629/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 630/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 631/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 632/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 633/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 634/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 635/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 636/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 637/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 638/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 639/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 640/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 641/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 642/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 643/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 644/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 645/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 646/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 647/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 648/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 649/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 650/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 651/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 652/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 653/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 654/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 655/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 656/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 657/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 658/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 659/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 660/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 661/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 662/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 663/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 664/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 665/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 666/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 667/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 668/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 669/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 670/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 671/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 672/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 673/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 674/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 675/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 676/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 677/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 678/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 679/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 680/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 681/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 682/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 683/1000, Train Loss: 0.03099, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 684/1000, Train Loss: 0.03098, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 685/1000, Train Loss: 0.03098, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 686/1000, Train Loss: 0.03098, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 687/1000, Train Loss: 0.03098, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 688/1000, Train Loss: 0.03098, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 689/1000, Train Loss: 0.03098, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 690/1000, Train Loss: 0.03098, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 691/1000, Train Loss: 0.03098, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 692/1000, Train Loss: 0.03098, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 693/1000, Train Loss: 0.03098, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 694/1000, Train Loss: 0.03098, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 695/1000, Train Loss: 0.03098, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 696/1000, Train Loss: 0.03098, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 697/1000, Train Loss: 0.03098, lr: 3.90625e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 698/1000, Train Loss: 0.03098, lr: 1.953125e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 699/1000, Train Loss: 0.03098, lr: 1.953125e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 700/1000, Train Loss: 0.03098, lr: 1.953125e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 701/1000, Train Loss: 0.03098, lr: 1.953125e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 702/1000, Train Loss: 0.03098, lr: 1.953125e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 703/1000, Train Loss: 0.03098, lr: 1.953125e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 704/1000, Train Loss: 0.03098, lr: 1.953125e-06\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 705/1000, Train Loss: 0.03097, lr: 9.765625e-07\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 706/1000, Train Loss: 0.03097, lr: 9.765625e-07\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 707/1000, Train Loss: 0.03097, lr: 9.765625e-07\n",
      "Val Loss: 0.03410\n",
      "---------\n",
      "Epoch 708/1000, Train Loss: 0.03097, lr: 9.765625e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 709/1000, Train Loss: 0.03097, lr: 9.765625e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 710/1000, Train Loss: 0.03097, lr: 9.765625e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 711/1000, Train Loss: 0.03097, lr: 9.765625e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 712/1000, Train Loss: 0.03097, lr: 4.8828125e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 713/1000, Train Loss: 0.03097, lr: 4.8828125e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 714/1000, Train Loss: 0.03097, lr: 4.8828125e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 715/1000, Train Loss: 0.03097, lr: 4.8828125e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 716/1000, Train Loss: 0.03097, lr: 4.8828125e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 717/1000, Train Loss: 0.03097, lr: 4.8828125e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 718/1000, Train Loss: 0.03097, lr: 4.8828125e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 719/1000, Train Loss: 0.03097, lr: 2.44140625e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 720/1000, Train Loss: 0.03097, lr: 2.44140625e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 721/1000, Train Loss: 0.03097, lr: 2.44140625e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 722/1000, Train Loss: 0.03097, lr: 2.44140625e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 723/1000, Train Loss: 0.03097, lr: 2.44140625e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 724/1000, Train Loss: 0.03097, lr: 2.44140625e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 725/1000, Train Loss: 0.03097, lr: 2.44140625e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 726/1000, Train Loss: 0.03097, lr: 1.220703125e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 727/1000, Train Loss: 0.03097, lr: 1.220703125e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 728/1000, Train Loss: 0.03097, lr: 1.220703125e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 729/1000, Train Loss: 0.03097, lr: 1.220703125e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 730/1000, Train Loss: 0.03097, lr: 1.220703125e-07\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Early stopping ....\n",
      "4867.857780218124 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1460, 32, 48, 6, 1), y_hat_i: (4, 32, 48, 6, 1), y_i: (4, 32, 48, 6, 1), batch.x: torch.Size([128, 48, 8, 6]), y: (1460, 32, 48, 6, 1)\n",
      "RMSE for t2m: 1.750073994893572; MAE for t2m: 1.3280518181676817;\n",
      "RMSE for sp: 1.4680411640494473; MAE for sp: 1.112939663710224;\n",
      "RMSE for tcc: 0.29386633601294426; MAE for tcc: 0.20156463827901322;\n",
      "RMSE for u10: 1.224268613018142; MAE for u10: 0.9135147775390906;\n",
      "RMSE for v10: 1.187695869370097; MAE for v10: 0.8799184231110464;\n",
      "RMSE for tp: 0.3020000877644687; MAE for tp: 0.08093949858495261;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 8, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1460, 32, 48, 6, 1), y_hat_i: (4, 32, 48, 6, 1), y_i: (4, 32, 48, 6, 1), batch.x: torch.Size([128, 48, 8, 6]), y: (1460, 32, 48, 6, 1)\n",
      "RMSE for t2m: 1.750073994893572; MAE for t2m: 1.3280518181676817;\n",
      "RMSE for sp: 1.4680411640494473; MAE for sp: 1.112939663710224;\n",
      "RMSE for tcc: 0.29345337700902685; MAE for tcc: 0.20053933774288754;\n",
      "RMSE for u10: 1.224268613018142; MAE for u10: 0.9135147775390906;\n",
      "RMSE for v10: 1.187695869370097; MAE for v10: 0.8799184231110464;\n",
      "RMSE for tp: 0.3020000877644687; MAE for tp: 0.08093949858495261;\n",
      "Epoch 1/1000, Train Loss: 0.07053, lr: 0.001--------------| 72.7% Complete\n",
      "Val Loss: 0.06321\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.05703, lr: 0.001\n",
      "Val Loss: 0.05300\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.04943, lr: 0.001\n",
      "Val Loss: 0.05019\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.04693, lr: 0.001\n",
      "Val Loss: 0.04810\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.04556, lr: 0.001\n",
      "Val Loss: 0.04687\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.04448, lr: 0.001\n",
      "Val Loss: 0.04611\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.04374, lr: 0.001\n",
      "Val Loss: 0.04598\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.04309, lr: 0.001\n",
      "Val Loss: 0.04594\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.04266, lr: 0.001\n",
      "Val Loss: 0.04612\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.04227, lr: 0.001\n",
      "Val Loss: 0.04560\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.04172, lr: 0.001\n",
      "Val Loss: 0.04493\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.04051, lr: 0.001\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.03852, lr: 0.001\n",
      "Val Loss: 0.03990\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.03784, lr: 0.001\n",
      "Val Loss: 0.04002\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.03747, lr: 0.001\n",
      "Val Loss: 0.04047\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.03721, lr: 0.001\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.03696, lr: 0.001\n",
      "Val Loss: 0.04037\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.03663, lr: 0.001\n",
      "Val Loss: 0.03984\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.03645, lr: 0.001\n",
      "Val Loss: 0.03947\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.03629, lr: 0.001\n",
      "Val Loss: 0.03897\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.03606, lr: 0.001\n",
      "Val Loss: 0.03886\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.03622, lr: 0.001\n",
      "Val Loss: 0.03876\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.03590, lr: 0.001\n",
      "Val Loss: 0.03871\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.03577, lr: 0.001\n",
      "Val Loss: 0.03818\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.03556, lr: 0.001\n",
      "Val Loss: 0.03843\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.03539, lr: 0.001\n",
      "Val Loss: 0.03852\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.03543, lr: 0.001\n",
      "Val Loss: 0.03831\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.03524, lr: 0.001\n",
      "Val Loss: 0.03845\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.03512, lr: 0.001\n",
      "Val Loss: 0.03851\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.03504, lr: 0.001\n",
      "Val Loss: 0.03865\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.03498, lr: 0.001\n",
      "Val Loss: 0.03837\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 32/1000, Train Loss: 0.03447, lr: 0.0005\n",
      "Val Loss: 0.03658\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.03404, lr: 0.0005\n",
      "Val Loss: 0.03658\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.03385, lr: 0.0005\n",
      "Val Loss: 0.03660\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.03369, lr: 0.0005\n",
      "Val Loss: 0.03652\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.03358, lr: 0.0005\n",
      "Val Loss: 0.03653\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.03343, lr: 0.0005\n",
      "Val Loss: 0.03653\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.03324, lr: 0.0005\n",
      "Val Loss: 0.03659\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.03317, lr: 0.0005\n",
      "Val Loss: 0.03647\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.03302, lr: 0.0005\n",
      "Val Loss: 0.03652\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.03295, lr: 0.0005\n",
      "Val Loss: 0.03666\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.03276, lr: 0.0005\n",
      "Val Loss: 0.03662\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.03268, lr: 0.0005\n",
      "Val Loss: 0.03653\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.03262, lr: 0.0005\n",
      "Val Loss: 0.03648\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.03249, lr: 0.0005\n",
      "Val Loss: 0.03644\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.03246, lr: 0.0005\n",
      "Val Loss: 0.03647\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.03228, lr: 0.0005\n",
      "Val Loss: 0.03659\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.03228, lr: 0.0005\n",
      "Val Loss: 0.03672\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.03212, lr: 0.0005\n",
      "Val Loss: 0.03660\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.03202, lr: 0.0005\n",
      "Val Loss: 0.03683\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.03204, lr: 0.0005\n",
      "Val Loss: 0.03697\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.03199, lr: 0.0005\n",
      "Val Loss: 0.03694\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 53/1000, Train Loss: 0.03165, lr: 0.00025\n",
      "Val Loss: 0.03600\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.03142, lr: 0.00025\n",
      "Val Loss: 0.03586\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.03126, lr: 0.00025\n",
      "Val Loss: 0.03588\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.03115, lr: 0.00025\n",
      "Val Loss: 0.03593\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.03106, lr: 0.00025\n",
      "Val Loss: 0.03588\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.03093, lr: 0.00025\n",
      "Val Loss: 0.03593\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.03085, lr: 0.00025\n",
      "Val Loss: 0.03601\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.03080, lr: 0.00025\n",
      "Val Loss: 0.03595\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.03069, lr: 0.00025\n",
      "Val Loss: 0.03590\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 62/1000, Train Loss: 0.03078, lr: 0.000125\n",
      "Val Loss: 0.03568\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.03063, lr: 0.000125\n",
      "Val Loss: 0.03564\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.03053, lr: 0.000125\n",
      "Val Loss: 0.03564\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.03045, lr: 0.000125\n",
      "Val Loss: 0.03564\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.03038, lr: 0.000125\n",
      "Val Loss: 0.03565\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.03031, lr: 0.000125\n",
      "Val Loss: 0.03567\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.03025, lr: 0.000125\n",
      "Val Loss: 0.03569\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.03017, lr: 0.000125\n",
      "Val Loss: 0.03569\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.03013, lr: 0.000125\n",
      "Val Loss: 0.03569\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.03006, lr: 0.000125\n",
      "Val Loss: 0.03570\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.03002, lr: 0.000125\n",
      "Val Loss: 0.03574\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 73/1000, Train Loss: 0.03010, lr: 6.25e-05\n",
      "Val Loss: 0.03562\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.03003, lr: 6.25e-05\n",
      "Val Loss: 0.03562\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.02996, lr: 6.25e-05\n",
      "Val Loss: 0.03563\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.02991, lr: 6.25e-05\n",
      "Val Loss: 0.03562\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.02986, lr: 6.25e-05\n",
      "Val Loss: 0.03563\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.02981, lr: 6.25e-05\n",
      "Val Loss: 0.03563\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.02977, lr: 6.25e-05\n",
      "Val Loss: 0.03564\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.02973, lr: 6.25e-05\n",
      "Val Loss: 0.03563\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.02969, lr: 6.25e-05\n",
      "Val Loss: 0.03564\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 82/1000, Train Loss: 0.02975, lr: 3.125e-05\n",
      "Val Loss: 0.03556\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.02971, lr: 3.125e-05\n",
      "Val Loss: 0.03554\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.02968, lr: 3.125e-05\n",
      "Val Loss: 0.03553\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.02965, lr: 3.125e-05\n",
      "Val Loss: 0.03553\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.02962, lr: 3.125e-05\n",
      "Val Loss: 0.03553\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.02960, lr: 3.125e-05\n",
      "Val Loss: 0.03553\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.02957, lr: 3.125e-05\n",
      "Val Loss: 0.03553\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.02955, lr: 3.125e-05\n",
      "Val Loss: 0.03554\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.02953, lr: 3.125e-05\n",
      "Val Loss: 0.03554\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.02951, lr: 3.125e-05\n",
      "Val Loss: 0.03554\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.02949, lr: 3.125e-05\n",
      "Val Loss: 0.03554\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.02947, lr: 3.125e-05\n",
      "Val Loss: 0.03554\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 94/1000, Train Loss: 0.02955, lr: 1.5625e-05\n",
      "Val Loss: 0.03547\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.02951, lr: 1.5625e-05\n",
      "Val Loss: 0.03546\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.02949, lr: 1.5625e-05\n",
      "Val Loss: 0.03546\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.02947, lr: 1.5625e-05\n",
      "Val Loss: 0.03547\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.02946, lr: 1.5625e-05\n",
      "Val Loss: 0.03547\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.02944, lr: 1.5625e-05\n",
      "Val Loss: 0.03547\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.02943, lr: 1.5625e-05\n",
      "Val Loss: 0.03547\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.02941, lr: 1.5625e-05\n",
      "Val Loss: 0.03547\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.02940, lr: 1.5625e-05\n",
      "Val Loss: 0.03547\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 103/1000, Train Loss: 0.02942, lr: 7.8125e-06\n",
      "Val Loss: 0.03546\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.02939, lr: 7.8125e-06\n",
      "Val Loss: 0.03546\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.02938, lr: 7.8125e-06\n",
      "Val Loss: 0.03546\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.02937, lr: 7.8125e-06\n",
      "Val Loss: 0.03546\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.02936, lr: 7.8125e-06\n",
      "Val Loss: 0.03546\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.02935, lr: 7.8125e-06\n",
      "Val Loss: 0.03546\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.02935, lr: 7.8125e-06\n",
      "Val Loss: 0.03546\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.02934, lr: 7.8125e-06\n",
      "Val Loss: 0.03546\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.02933, lr: 7.8125e-06\n",
      "Val Loss: 0.03546\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.02932, lr: 7.8125e-06\n",
      "Val Loss: 0.03546\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.02932, lr: 7.8125e-06\n",
      "Val Loss: 0.03546\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 114/1000, Train Loss: 0.02931, lr: 3.90625e-06\n",
      "Val Loss: 0.03546\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.02930, lr: 3.90625e-06\n",
      "Val Loss: 0.03547\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.02930, lr: 3.90625e-06\n",
      "Val Loss: 0.03547\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.02929, lr: 3.90625e-06\n",
      "Val Loss: 0.03547\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.02929, lr: 3.90625e-06\n",
      "Val Loss: 0.03547\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.02928, lr: 3.90625e-06\n",
      "Val Loss: 0.03547\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.02928, lr: 3.90625e-06\n",
      "Val Loss: 0.03547\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 121/1000, Train Loss: 0.02927, lr: 1.953125e-06\n",
      "Val Loss: 0.03548\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.02927, lr: 1.953125e-06\n",
      "Val Loss: 0.03548\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.02926, lr: 1.953125e-06\n",
      "Val Loss: 0.03548\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.02926, lr: 1.953125e-06\n",
      "Val Loss: 0.03548\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.02926, lr: 1.953125e-06\n",
      "Val Loss: 0.03548\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.02926, lr: 1.953125e-06\n",
      "Val Loss: 0.03548\n",
      "---------\n",
      "Epoch 127/1000, Train Loss: 0.02925, lr: 1.953125e-06\n",
      "Val Loss: 0.03548\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 128/1000, Train Loss: 0.02925, lr: 9.765625e-07\n",
      "Val Loss: 0.03548\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.02924, lr: 9.765625e-07\n",
      "Val Loss: 0.03548\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.02924, lr: 9.765625e-07\n",
      "Val Loss: 0.03548\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.02924, lr: 9.765625e-07\n",
      "Val Loss: 0.03549\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.02924, lr: 9.765625e-07\n",
      "Val Loss: 0.03549\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.02924, lr: 9.765625e-07\n",
      "Val Loss: 0.03549\n",
      "---------\n",
      "Epoch 134/1000, Train Loss: 0.02924, lr: 9.765625e-07\n",
      "Val Loss: 0.03549\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 135/1000, Train Loss: 0.02923, lr: 4.8828125e-07\n",
      "Val Loss: 0.03549\n",
      "---------\n",
      "Epoch 136/1000, Train Loss: 0.02923, lr: 4.8828125e-07\n",
      "Val Loss: 0.03549\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.02923, lr: 4.8828125e-07\n",
      "Val Loss: 0.03549\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.02923, lr: 4.8828125e-07\n",
      "Val Loss: 0.03549\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.02923, lr: 4.8828125e-07\n",
      "Val Loss: 0.03549\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.02923, lr: 4.8828125e-07\n",
      "Val Loss: 0.03549\n",
      "---------\n",
      "Epoch 141/1000, Train Loss: 0.02923, lr: 4.8828125e-07\n",
      "Val Loss: 0.03549\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 142/1000, Train Loss: 0.02923, lr: 2.44140625e-07\n",
      "Val Loss: 0.03549\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.02923, lr: 2.44140625e-07\n",
      "Val Loss: 0.03549\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.02923, lr: 2.44140625e-07\n",
      "Val Loss: 0.03549\n",
      "---------\n",
      "Epoch 145/1000, Train Loss: 0.02923, lr: 2.44140625e-07\n",
      "Val Loss: 0.03549\n",
      "---------\n",
      "Epoch 146/1000, Train Loss: 0.02923, lr: 2.44140625e-07\n",
      "Val Loss: 0.03549\n",
      "---------\n",
      "Early stopping ....\n",
      "946.8926360607147 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1459, 32, 48, 6, 1), y_hat_i: (3, 32, 48, 6, 1), y_i: (3, 32, 48, 6, 1), batch.x: torch.Size([96, 48, 9, 6]), y: (1459, 32, 48, 6, 1)\n",
      "RMSE for t2m: 1.7459404771195475; MAE for t2m: 1.3240221616577177;\n",
      "RMSE for sp: 1.8537667204345962; MAE for sp: 1.4017921461721665;\n",
      "RMSE for tcc: 0.30948239764652186; MAE for tcc: 0.21173785945122206;\n",
      "RMSE for u10: 1.3132021277493393; MAE for u10: 0.9734514242507414;\n",
      "RMSE for v10: 1.2338938748158432; MAE for v10: 0.9100089765773793;\n",
      "RMSE for tp: 0.31632038165009385; MAE for tp: 0.0829393147159999;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 9, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1459, 32, 48, 6, 1), y_hat_i: (3, 32, 48, 6, 1), y_i: (3, 32, 48, 6, 1), batch.x: torch.Size([96, 48, 9, 6]), y: (1459, 32, 48, 6, 1)\n",
      "RMSE for t2m: 1.7459404771195475; MAE for t2m: 1.3240221616577177;\n",
      "RMSE for sp: 1.8537667204345962; MAE for sp: 1.4017921461721665;\n",
      "RMSE for tcc: 0.3086971480044221; MAE for tcc: 0.21047596666144402;\n",
      "RMSE for u10: 1.3132021277493393; MAE for u10: 0.9734514242507414;\n",
      "RMSE for v10: 1.2338938748158432; MAE for v10: 0.9100089765773793;\n",
      "RMSE for tp: 0.31632038165009385; MAE for tp: 0.0829393147159999;\n",
      "Epoch 1/1000, Train Loss: 0.06901, lr: 0.001----------| 81.8% Complete\n",
      "Val Loss: 0.06088\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.05717, lr: 0.001\n",
      "Val Loss: 0.05742\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.05425, lr: 0.001\n",
      "Val Loss: 0.05552\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.05278, lr: 0.001\n",
      "Val Loss: 0.05421\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.05024, lr: 0.001\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.04612, lr: 0.001\n",
      "Val Loss: 0.04707\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.04388, lr: 0.001\n",
      "Val Loss: 0.04428\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.04127, lr: 0.001\n",
      "Val Loss: 0.04230\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.04010, lr: 0.001\n",
      "Val Loss: 0.04128\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.03931, lr: 0.001\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.03884, lr: 0.001\n",
      "Val Loss: 0.04022\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.03835, lr: 0.001\n",
      "Val Loss: 0.03972\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.03804, lr: 0.001\n",
      "Val Loss: 0.03914\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.03772, lr: 0.001\n",
      "Val Loss: 0.03910\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.03749, lr: 0.001\n",
      "Val Loss: 0.03880\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.03713, lr: 0.001\n",
      "Val Loss: 0.03870\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.03702, lr: 0.001\n",
      "Val Loss: 0.03851\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.03673, lr: 0.001\n",
      "Val Loss: 0.03844\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.03657, lr: 0.001\n",
      "Val Loss: 0.03804\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.03631, lr: 0.001\n",
      "Val Loss: 0.03810\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.03625, lr: 0.001\n",
      "Val Loss: 0.03803\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.03600, lr: 0.001\n",
      "Val Loss: 0.03767\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.03581, lr: 0.001\n",
      "Val Loss: 0.03746\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.03576, lr: 0.001\n",
      "Val Loss: 0.03738\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.03557, lr: 0.001\n",
      "Val Loss: 0.03727\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.03552, lr: 0.001\n",
      "Val Loss: 0.03761\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.03533, lr: 0.001\n",
      "Val Loss: 0.03725\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.03511, lr: 0.001\n",
      "Val Loss: 0.03728\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.03510, lr: 0.001\n",
      "Val Loss: 0.03723\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.03500, lr: 0.001\n",
      "Val Loss: 0.03720\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.03486, lr: 0.001\n",
      "Val Loss: 0.03714\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.03473, lr: 0.001\n",
      "Val Loss: 0.03677\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.03462, lr: 0.001\n",
      "Val Loss: 0.03670\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.03444, lr: 0.001\n",
      "Val Loss: 0.03661\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.03428, lr: 0.001\n",
      "Val Loss: 0.03661\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.03419, lr: 0.001\n",
      "Val Loss: 0.03692\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.03415, lr: 0.001\n",
      "Val Loss: 0.03677\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.03407, lr: 0.001\n",
      "Val Loss: 0.03629\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.03397, lr: 0.001\n",
      "Val Loss: 0.03679\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.03392, lr: 0.001\n",
      "Val Loss: 0.03693\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.03392, lr: 0.001\n",
      "Val Loss: 0.03684\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.03384, lr: 0.001\n",
      "Val Loss: 0.03690\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.03383, lr: 0.001\n",
      "Val Loss: 0.03675\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.03374, lr: 0.001\n",
      "Val Loss: 0.03732\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.03361, lr: 0.001\n",
      "Val Loss: 0.03708\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 46/1000, Train Loss: 0.03281, lr: 0.0005\n",
      "Val Loss: 0.03594\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.03231, lr: 0.0005\n",
      "Val Loss: 0.03592\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.03210, lr: 0.0005\n",
      "Val Loss: 0.03584\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.03192, lr: 0.0005\n",
      "Val Loss: 0.03596\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.03176, lr: 0.0005\n",
      "Val Loss: 0.03598\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.03164, lr: 0.0005\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.03150, lr: 0.0005\n",
      "Val Loss: 0.03609\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.03136, lr: 0.0005\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.03121, lr: 0.0005\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.03114, lr: 0.0005\n",
      "Val Loss: 0.03639\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 56/1000, Train Loss: 0.03095, lr: 0.00025\n",
      "Val Loss: 0.03592\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.03065, lr: 0.00025\n",
      "Val Loss: 0.03589\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.03049, lr: 0.00025\n",
      "Val Loss: 0.03597\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.03034, lr: 0.00025\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.03022, lr: 0.00025\n",
      "Val Loss: 0.03604\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.03007, lr: 0.00025\n",
      "Val Loss: 0.03606\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.02998, lr: 0.00025\n",
      "Val Loss: 0.03617\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 63/1000, Train Loss: 0.03006, lr: 0.000125\n",
      "Val Loss: 0.03602\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.02994, lr: 0.000125\n",
      "Val Loss: 0.03596\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.02981, lr: 0.000125\n",
      "Val Loss: 0.03597\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.02971, lr: 0.000125\n",
      "Val Loss: 0.03596\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.02963, lr: 0.000125\n",
      "Val Loss: 0.03597\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.02953, lr: 0.000125\n",
      "Val Loss: 0.03599\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.02946, lr: 0.000125\n",
      "Val Loss: 0.03599\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 70/1000, Train Loss: 0.02958, lr: 6.25e-05\n",
      "Val Loss: 0.03567\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.02951, lr: 6.25e-05\n",
      "Val Loss: 0.03567\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.02943, lr: 6.25e-05\n",
      "Val Loss: 0.03567\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.02936, lr: 6.25e-05\n",
      "Val Loss: 0.03567\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.02930, lr: 6.25e-05\n",
      "Val Loss: 0.03568\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.02925, lr: 6.25e-05\n",
      "Val Loss: 0.03569\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.02920, lr: 6.25e-05\n",
      "Val Loss: 0.03569\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.02915, lr: 6.25e-05\n",
      "Val Loss: 0.03569\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.02911, lr: 6.25e-05\n",
      "Val Loss: 0.03569\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.02906, lr: 6.25e-05\n",
      "Val Loss: 0.03570\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 80/1000, Train Loss: 0.02914, lr: 3.125e-05\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.02910, lr: 3.125e-05\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.02906, lr: 3.125e-05\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.02903, lr: 3.125e-05\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.02900, lr: 3.125e-05\n",
      "Val Loss: 0.03560\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.02897, lr: 3.125e-05\n",
      "Val Loss: 0.03560\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.02894, lr: 3.125e-05\n",
      "Val Loss: 0.03561\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.02892, lr: 3.125e-05\n",
      "Val Loss: 0.03561\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.02889, lr: 3.125e-05\n",
      "Val Loss: 0.03562\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 89/1000, Train Loss: 0.02894, lr: 1.5625e-05\n",
      "Val Loss: 0.03561\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.02890, lr: 1.5625e-05\n",
      "Val Loss: 0.03561\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.02888, lr: 1.5625e-05\n",
      "Val Loss: 0.03561\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.02886, lr: 1.5625e-05\n",
      "Val Loss: 0.03561\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.02884, lr: 1.5625e-05\n",
      "Val Loss: 0.03562\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.02882, lr: 1.5625e-05\n",
      "Val Loss: 0.03562\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.02881, lr: 1.5625e-05\n",
      "Val Loss: 0.03562\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 96/1000, Train Loss: 0.02886, lr: 7.8125e-06\n",
      "Val Loss: 0.03560\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.02883, lr: 7.8125e-06\n",
      "Val Loss: 0.03560\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.02882, lr: 7.8125e-06\n",
      "Val Loss: 0.03560\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.02881, lr: 7.8125e-06\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.02880, lr: 7.8125e-06\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.02879, lr: 7.8125e-06\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.02878, lr: 7.8125e-06\n",
      "Val Loss: 0.03560\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 103/1000, Train Loss: 0.02879, lr: 3.90625e-06\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.02878, lr: 3.90625e-06\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.02877, lr: 3.90625e-06\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.02876, lr: 3.90625e-06\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.02876, lr: 3.90625e-06\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.02875, lr: 3.90625e-06\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.02875, lr: 3.90625e-06\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.02874, lr: 3.90625e-06\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.02874, lr: 3.90625e-06\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.02873, lr: 3.90625e-06\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.02873, lr: 3.90625e-06\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 114/1000, Train Loss: 0.02872, lr: 1.953125e-06\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.02872, lr: 1.953125e-06\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.02871, lr: 1.953125e-06\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.02871, lr: 1.953125e-06\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.02871, lr: 1.953125e-06\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.02870, lr: 1.953125e-06\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.02870, lr: 1.953125e-06\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 121/1000, Train Loss: 0.02869, lr: 9.765625e-07\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.02869, lr: 9.765625e-07\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.02869, lr: 9.765625e-07\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.02869, lr: 9.765625e-07\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.02868, lr: 9.765625e-07\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.02868, lr: 9.765625e-07\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "Epoch 127/1000, Train Loss: 0.02868, lr: 9.765625e-07\n",
      "Val Loss: 0.03558\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 128/1000, Train Loss: 0.02868, lr: 4.8828125e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.02867, lr: 4.8828125e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.02867, lr: 4.8828125e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.02867, lr: 4.8828125e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.02867, lr: 4.8828125e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.02867, lr: 4.8828125e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 134/1000, Train Loss: 0.02867, lr: 4.8828125e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 135/1000, Train Loss: 0.02867, lr: 2.44140625e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 136/1000, Train Loss: 0.02867, lr: 2.44140625e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.02867, lr: 2.44140625e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.02867, lr: 2.44140625e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.02867, lr: 2.44140625e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.02867, lr: 2.44140625e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 141/1000, Train Loss: 0.02866, lr: 2.44140625e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 142/1000, Train Loss: 0.02866, lr: 1.220703125e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.02866, lr: 1.220703125e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.02866, lr: 1.220703125e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 145/1000, Train Loss: 0.02866, lr: 1.220703125e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 146/1000, Train Loss: 0.02866, lr: 1.220703125e-07\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Early stopping ....\n",
      "947.1930949687958 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1458, 32, 48, 6, 1), y_hat_i: (2, 32, 48, 6, 1), y_i: (2, 32, 48, 6, 1), batch.x: torch.Size([64, 48, 10, 6]), y: (1458, 32, 48, 6, 1)\n",
      "RMSE for t2m: 1.715621975747492; MAE for t2m: 1.2996553670390374;\n",
      "RMSE for sp: 1.8853370880093179; MAE for sp: 1.4415023714869946;\n",
      "RMSE for tcc: 0.3088200996017392; MAE for tcc: 0.20711551312627743;\n",
      "RMSE for u10: 1.2980407032095895; MAE for u10: 0.9603357844116799;\n",
      "RMSE for v10: 1.2512851393207645; MAE for v10: 0.9251908968167187;\n",
      "RMSE for tp: 0.3064928718545804; MAE for tp: 0.08170604230748517;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 10, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1458, 32, 48, 6, 1), y_hat_i: (2, 32, 48, 6, 1), y_i: (2, 32, 48, 6, 1), batch.x: torch.Size([64, 48, 10, 6]), y: (1458, 32, 48, 6, 1)\n",
      "RMSE for t2m: 1.715621975747492; MAE for t2m: 1.2996553670390374;\n",
      "RMSE for sp: 1.8853370880093179; MAE for sp: 1.4415023714869946;\n",
      "RMSE for tcc: 0.3083340993896888; MAE for tcc: 0.2062965074130709;\n",
      "RMSE for u10: 1.2980407032095895; MAE for u10: 0.9603357844116799;\n",
      "RMSE for v10: 1.2512851393207645; MAE for v10: 0.9251908968167187;\n",
      "RMSE for tp: 0.3064928718545804; MAE for tp: 0.08170604230748517;\n",
      "Sequence Progress: |-----| 90.9% Complete\r"
     ]
    }
   ],
   "source": [
    "hpo.determine_best_s()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo.write_plots_to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 0.07251, lr: 0.001-------------------------------------| 0.0% Complete\n",
      "Val Loss: 0.05977\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.05812, lr: 0.001\n",
      "Val Loss: 0.05581\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.05515, lr: 0.001\n",
      "Val Loss: 0.05439\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.04876, lr: 0.001\n",
      "Val Loss: 0.04796\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.04546, lr: 0.001\n",
      "Val Loss: 0.04725\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.04295, lr: 0.001\n",
      "Val Loss: 0.04274\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.04071, lr: 0.001\n",
      "Val Loss: 0.04159\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.03962, lr: 0.001\n",
      "Val Loss: 0.04110\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.03900, lr: 0.001\n",
      "Val Loss: 0.04033\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.03862, lr: 0.001\n",
      "Val Loss: 0.03932\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.03829, lr: 0.001\n",
      "Val Loss: 0.03895\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.03798, lr: 0.001\n",
      "Val Loss: 0.03859\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.03771, lr: 0.001\n",
      "Val Loss: 0.03834\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.03754, lr: 0.001\n",
      "Val Loss: 0.03817\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.03728, lr: 0.001\n",
      "Val Loss: 0.03797\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.03713, lr: 0.001\n",
      "Val Loss: 0.03777\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.03700, lr: 0.001\n",
      "Val Loss: 0.03770\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.03686, lr: 0.001\n",
      "Val Loss: 0.03757\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.03673, lr: 0.001\n",
      "Val Loss: 0.03748\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.03660, lr: 0.001\n",
      "Val Loss: 0.03735\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.03648, lr: 0.001\n",
      "Val Loss: 0.03722\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.03636, lr: 0.001\n",
      "Val Loss: 0.03708\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.03625, lr: 0.001\n",
      "Val Loss: 0.03702\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.03614, lr: 0.001\n",
      "Val Loss: 0.03695\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.03604, lr: 0.001\n",
      "Val Loss: 0.03691\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.03592, lr: 0.001\n",
      "Val Loss: 0.03677\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.03582, lr: 0.001\n",
      "Val Loss: 0.03667\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.03578, lr: 0.001\n",
      "Val Loss: 0.03672\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.03567, lr: 0.001\n",
      "Val Loss: 0.03665\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.03559, lr: 0.001\n",
      "Val Loss: 0.03667\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.03550, lr: 0.001\n",
      "Val Loss: 0.03651\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.03542, lr: 0.001\n",
      "Val Loss: 0.03646\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.03536, lr: 0.001\n",
      "Val Loss: 0.03661\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.03539, lr: 0.001\n",
      "Val Loss: 0.03679\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.03528, lr: 0.001\n",
      "Val Loss: 0.03656\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.03517, lr: 0.001\n",
      "Val Loss: 0.03636\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.03508, lr: 0.001\n",
      "Val Loss: 0.03620\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.03501, lr: 0.001\n",
      "Val Loss: 0.03619\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.03493, lr: 0.001\n",
      "Val Loss: 0.03618\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.03486, lr: 0.001\n",
      "Val Loss: 0.03602\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.03479, lr: 0.001\n",
      "Val Loss: 0.03594\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.03472, lr: 0.001\n",
      "Val Loss: 0.03585\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.03466, lr: 0.001\n",
      "Val Loss: 0.03583\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.03460, lr: 0.001\n",
      "Val Loss: 0.03582\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.03453, lr: 0.001\n",
      "Val Loss: 0.03585\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.03448, lr: 0.001\n",
      "Val Loss: 0.03584\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.03442, lr: 0.001\n",
      "Val Loss: 0.03588\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.03436, lr: 0.001\n",
      "Val Loss: 0.03585\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.03433, lr: 0.001\n",
      "Val Loss: 0.03593\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.03426, lr: 0.001\n",
      "Val Loss: 0.03585\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.03423, lr: 0.001\n",
      "Val Loss: 0.03588\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 52/1000, Train Loss: 0.03351, lr: 0.0005\n",
      "Val Loss: 0.03504\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.03336, lr: 0.0005\n",
      "Val Loss: 0.03499\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.03329, lr: 0.0005\n",
      "Val Loss: 0.03498\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.03324, lr: 0.0005\n",
      "Val Loss: 0.03495\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.03320, lr: 0.0005\n",
      "Val Loss: 0.03493\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.03315, lr: 0.0005\n",
      "Val Loss: 0.03492\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.03312, lr: 0.0005\n",
      "Val Loss: 0.03490\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.03308, lr: 0.0005\n",
      "Val Loss: 0.03490\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.03305, lr: 0.0005\n",
      "Val Loss: 0.03488\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.03302, lr: 0.0005\n",
      "Val Loss: 0.03486\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.03297, lr: 0.0005\n",
      "Val Loss: 0.03485\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.03294, lr: 0.0005\n",
      "Val Loss: 0.03483\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.03291, lr: 0.0005\n",
      "Val Loss: 0.03482\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.03288, lr: 0.0005\n",
      "Val Loss: 0.03481\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.03285, lr: 0.0005\n",
      "Val Loss: 0.03480\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.03282, lr: 0.0005\n",
      "Val Loss: 0.03478\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.03279, lr: 0.0005\n",
      "Val Loss: 0.03477\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.03276, lr: 0.0005\n",
      "Val Loss: 0.03476\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.03274, lr: 0.0005\n",
      "Val Loss: 0.03476\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.03271, lr: 0.0005\n",
      "Val Loss: 0.03473\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.03268, lr: 0.0005\n",
      "Val Loss: 0.03473\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.03266, lr: 0.0005\n",
      "Val Loss: 0.03471\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.03263, lr: 0.0005\n",
      "Val Loss: 0.03471\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.03260, lr: 0.0005\n",
      "Val Loss: 0.03471\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.03258, lr: 0.0005\n",
      "Val Loss: 0.03471\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.03256, lr: 0.0005\n",
      "Val Loss: 0.03471\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.03254, lr: 0.0005\n",
      "Val Loss: 0.03469\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.03251, lr: 0.0005\n",
      "Val Loss: 0.03468\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.03249, lr: 0.0005\n",
      "Val Loss: 0.03469\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.03247, lr: 0.0005\n",
      "Val Loss: 0.03467\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.03245, lr: 0.0005\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.03243, lr: 0.0005\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.03240, lr: 0.0005\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.03239, lr: 0.0005\n",
      "Val Loss: 0.03465\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.03237, lr: 0.0005\n",
      "Val Loss: 0.03464\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.03235, lr: 0.0005\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.03232, lr: 0.0005\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.03230, lr: 0.0005\n",
      "Val Loss: 0.03461\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.03228, lr: 0.0005\n",
      "Val Loss: 0.03461\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.03226, lr: 0.0005\n",
      "Val Loss: 0.03461\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.03225, lr: 0.0005\n",
      "Val Loss: 0.03460\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.03222, lr: 0.0005\n",
      "Val Loss: 0.03460\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.03221, lr: 0.0005\n",
      "Val Loss: 0.03459\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.03219, lr: 0.0005\n",
      "Val Loss: 0.03458\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.03217, lr: 0.0005\n",
      "Val Loss: 0.03460\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.03216, lr: 0.0005\n",
      "Val Loss: 0.03460\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.03214, lr: 0.0005\n",
      "Val Loss: 0.03460\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.03214, lr: 0.0005\n",
      "Val Loss: 0.03460\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.03211, lr: 0.0005\n",
      "Val Loss: 0.03461\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.03210, lr: 0.0005\n",
      "Val Loss: 0.03461\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.03208, lr: 0.0005\n",
      "Val Loss: 0.03462\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 103/1000, Train Loss: 0.03168, lr: 0.00025\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.03159, lr: 0.00025\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.03156, lr: 0.00025\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.03154, lr: 0.00025\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.03152, lr: 0.00025\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.03151, lr: 0.00025\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.03149, lr: 0.00025\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.03148, lr: 0.00025\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.03147, lr: 0.00025\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.03145, lr: 0.00025\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.03144, lr: 0.00025\n",
      "Val Loss: 0.03437\n",
      "---------\n",
      "Epoch 114/1000, Train Loss: 0.03143, lr: 0.00025\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.03142, lr: 0.00025\n",
      "Val Loss: 0.03436\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.03141, lr: 0.00025\n",
      "Val Loss: 0.03434\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.03139, lr: 0.00025\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.03138, lr: 0.00025\n",
      "Val Loss: 0.03433\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.03137, lr: 0.00025\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.03136, lr: 0.00025\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 121/1000, Train Loss: 0.03134, lr: 0.00025\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.03133, lr: 0.00025\n",
      "Val Loss: 0.03432\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.03132, lr: 0.00025\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.03131, lr: 0.00025\n",
      "Val Loss: 0.03430\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.03130, lr: 0.00025\n",
      "Val Loss: 0.03428\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.03129, lr: 0.00025\n",
      "Val Loss: 0.03429\n",
      "---------\n",
      "Epoch 127/1000, Train Loss: 0.03128, lr: 0.00025\n",
      "Val Loss: 0.03428\n",
      "---------\n",
      "Epoch 128/1000, Train Loss: 0.03127, lr: 0.00025\n",
      "Val Loss: 0.03429\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.03126, lr: 0.00025\n",
      "Val Loss: 0.03429\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.03125, lr: 0.00025\n",
      "Val Loss: 0.03428\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.03124, lr: 0.00025\n",
      "Val Loss: 0.03427\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.03123, lr: 0.00025\n",
      "Val Loss: 0.03427\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.03122, lr: 0.00025\n",
      "Val Loss: 0.03427\n",
      "---------\n",
      "Epoch 134/1000, Train Loss: 0.03121, lr: 0.00025\n",
      "Val Loss: 0.03425\n",
      "---------\n",
      "Epoch 135/1000, Train Loss: 0.03120, lr: 0.00025\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 136/1000, Train Loss: 0.03119, lr: 0.00025\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.03118, lr: 0.00025\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.03117, lr: 0.00025\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.03117, lr: 0.00025\n",
      "Val Loss: 0.03424\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.03116, lr: 0.00025\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 141/1000, Train Loss: 0.03115, lr: 0.00025\n",
      "Val Loss: 0.03423\n",
      "---------\n",
      "Epoch 142/1000, Train Loss: 0.03114, lr: 0.00025\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.03113, lr: 0.00025\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.03111, lr: 0.00025\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 145/1000, Train Loss: 0.03111, lr: 0.00025\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 146/1000, Train Loss: 0.03110, lr: 0.00025\n",
      "Val Loss: 0.03422\n",
      "---------\n",
      "Epoch 147/1000, Train Loss: 0.03109, lr: 0.00025\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 148/1000, Train Loss: 0.03108, lr: 0.00025\n",
      "Val Loss: 0.03421\n",
      "---------\n",
      "Epoch 149/1000, Train Loss: 0.03107, lr: 0.00025\n",
      "Val Loss: 0.03420\n",
      "---------\n",
      "Epoch 150/1000, Train Loss: 0.03106, lr: 0.00025\n",
      "Val Loss: 0.03420\n",
      "---------\n",
      "Epoch 151/1000, Train Loss: 0.03105, lr: 0.00025\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 152/1000, Train Loss: 0.03105, lr: 0.00025\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 153/1000, Train Loss: 0.03104, lr: 0.00025\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 154/1000, Train Loss: 0.03103, lr: 0.00025\n",
      "Val Loss: 0.03419\n",
      "---------\n",
      "Epoch 155/1000, Train Loss: 0.03102, lr: 0.00025\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 156/1000, Train Loss: 0.03101, lr: 0.00025\n",
      "Val Loss: 0.03418\n",
      "---------\n",
      "Epoch 157/1000, Train Loss: 0.03101, lr: 0.00025\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 158/1000, Train Loss: 0.03100, lr: 0.00025\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 159/1000, Train Loss: 0.03099, lr: 0.00025\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 160/1000, Train Loss: 0.03098, lr: 0.00025\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 161/1000, Train Loss: 0.03097, lr: 0.00025\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 162/1000, Train Loss: 0.03097, lr: 0.00025\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 163/1000, Train Loss: 0.03096, lr: 0.00025\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 164/1000, Train Loss: 0.03095, lr: 0.00025\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 165/1000, Train Loss: 0.03095, lr: 0.00025\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 166/1000, Train Loss: 0.03094, lr: 0.00025\n",
      "Val Loss: 0.03417\n",
      "---------\n",
      "Epoch 167/1000, Train Loss: 0.03093, lr: 0.00025\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 168/1000, Train Loss: 0.03092, lr: 0.00025\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 169/1000, Train Loss: 0.03092, lr: 0.00025\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 170/1000, Train Loss: 0.03091, lr: 0.00025\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 171/1000, Train Loss: 0.03090, lr: 0.00025\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 172/1000, Train Loss: 0.03089, lr: 0.00025\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 173/1000, Train Loss: 0.03089, lr: 0.00025\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 174/1000, Train Loss: 0.03088, lr: 0.00025\n",
      "Val Loss: 0.03416\n",
      "---------\n",
      "Epoch 175/1000, Train Loss: 0.03087, lr: 0.00025\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 176/1000, Train Loss: 0.03087, lr: 0.00025\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 177/1000, Train Loss: 0.03086, lr: 0.00025\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 178/1000, Train Loss: 0.03085, lr: 0.00025\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 179/1000, Train Loss: 0.03084, lr: 0.00025\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 180/1000, Train Loss: 0.03084, lr: 0.00025\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 181/1000, Train Loss: 0.03083, lr: 0.00025\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 182/1000, Train Loss: 0.03082, lr: 0.00025\n",
      "Val Loss: 0.03415\n",
      "---------\n",
      "Epoch 183/1000, Train Loss: 0.03081, lr: 0.00025\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 184/1000, Train Loss: 0.03080, lr: 0.00025\n",
      "Val Loss: 0.03414\n",
      "---------\n",
      "Epoch 185/1000, Train Loss: 0.03079, lr: 0.00025\n",
      "Val Loss: 0.03413\n",
      "---------\n",
      "Epoch 186/1000, Train Loss: 0.03079, lr: 0.00025\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 187/1000, Train Loss: 0.03078, lr: 0.00025\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 188/1000, Train Loss: 0.03077, lr: 0.00025\n",
      "Val Loss: 0.03412\n",
      "---------\n",
      "Epoch 189/1000, Train Loss: 0.03077, lr: 0.00025\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 190/1000, Train Loss: 0.03076, lr: 0.00025\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 191/1000, Train Loss: 0.03076, lr: 0.00025\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 192/1000, Train Loss: 0.03075, lr: 0.00025\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 193/1000, Train Loss: 0.03074, lr: 0.00025\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 194/1000, Train Loss: 0.03074, lr: 0.00025\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 195/1000, Train Loss: 0.03073, lr: 0.00025\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 196/1000, Train Loss: 0.03072, lr: 0.00025\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 197/1000, Train Loss: 0.03072, lr: 0.00025\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 198/1000, Train Loss: 0.03071, lr: 0.00025\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 199/1000, Train Loss: 0.03071, lr: 0.00025\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 200/1000, Train Loss: 0.03070, lr: 0.00025\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "Epoch 201/1000, Train Loss: 0.03069, lr: 0.00025\n",
      "Val Loss: 0.03411\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 202/1000, Train Loss: 0.03051, lr: 0.000125\n",
      "Val Loss: 0.03373\n",
      "---------\n",
      "Epoch 203/1000, Train Loss: 0.03051, lr: 0.000125\n",
      "Val Loss: 0.03372\n",
      "---------\n",
      "Epoch 204/1000, Train Loss: 0.03049, lr: 0.000125\n",
      "Val Loss: 0.03373\n",
      "---------\n",
      "Epoch 205/1000, Train Loss: 0.03047, lr: 0.000125\n",
      "Val Loss: 0.03373\n",
      "---------\n",
      "Epoch 206/1000, Train Loss: 0.03046, lr: 0.000125\n",
      "Val Loss: 0.03372\n",
      "---------\n",
      "Epoch 207/1000, Train Loss: 0.03046, lr: 0.000125\n",
      "Val Loss: 0.03372\n",
      "---------\n",
      "Epoch 208/1000, Train Loss: 0.03045, lr: 0.000125\n",
      "Val Loss: 0.03372\n",
      "---------\n",
      "Epoch 209/1000, Train Loss: 0.03044, lr: 0.000125\n",
      "Val Loss: 0.03372\n",
      "---------\n",
      "Epoch 210/1000, Train Loss: 0.03044, lr: 0.000125\n",
      "Val Loss: 0.03372\n",
      "---------\n",
      "Epoch 211/1000, Train Loss: 0.03043, lr: 0.000125\n",
      "Val Loss: 0.03372\n",
      "---------\n",
      "Epoch 212/1000, Train Loss: 0.03042, lr: 0.000125\n",
      "Val Loss: 0.03372\n",
      "---------\n",
      "Epoch 213/1000, Train Loss: 0.03042, lr: 0.000125\n",
      "Val Loss: 0.03372\n",
      "---------\n",
      "Epoch 214/1000, Train Loss: 0.03041, lr: 0.000125\n",
      "Val Loss: 0.03372\n",
      "---------\n",
      "Epoch 215/1000, Train Loss: 0.03041, lr: 0.000125\n",
      "Val Loss: 0.03372\n",
      "---------\n",
      "Epoch 216/1000, Train Loss: 0.03040, lr: 0.000125\n",
      "Val Loss: 0.03372\n",
      "---------\n",
      "Epoch 217/1000, Train Loss: 0.03040, lr: 0.000125\n",
      "Val Loss: 0.03372\n",
      "---------\n",
      "Epoch 218/1000, Train Loss: 0.03039, lr: 0.000125\n",
      "Val Loss: 0.03372\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 219/1000, Train Loss: 0.03027, lr: 6.25e-05\n",
      "Val Loss: 0.03359\n",
      "---------\n",
      "Epoch 220/1000, Train Loss: 0.03025, lr: 6.25e-05\n",
      "Val Loss: 0.03358\n",
      "---------\n",
      "Epoch 221/1000, Train Loss: 0.03024, lr: 6.25e-05\n",
      "Val Loss: 0.03358\n",
      "---------\n",
      "Epoch 222/1000, Train Loss: 0.03023, lr: 6.25e-05\n",
      "Val Loss: 0.03358\n",
      "---------\n",
      "Epoch 223/1000, Train Loss: 0.03022, lr: 6.25e-05\n",
      "Val Loss: 0.03358\n",
      "---------\n",
      "Epoch 224/1000, Train Loss: 0.03022, lr: 6.25e-05\n",
      "Val Loss: 0.03357\n",
      "---------\n",
      "Epoch 225/1000, Train Loss: 0.03021, lr: 6.25e-05\n",
      "Val Loss: 0.03357\n",
      "---------\n",
      "Epoch 226/1000, Train Loss: 0.03021, lr: 6.25e-05\n",
      "Val Loss: 0.03357\n",
      "---------\n",
      "Epoch 227/1000, Train Loss: 0.03020, lr: 6.25e-05\n",
      "Val Loss: 0.03357\n",
      "---------\n",
      "Epoch 228/1000, Train Loss: 0.03020, lr: 6.25e-05\n",
      "Val Loss: 0.03357\n",
      "---------\n",
      "Epoch 229/1000, Train Loss: 0.03020, lr: 6.25e-05\n",
      "Val Loss: 0.03357\n",
      "---------\n",
      "Epoch 230/1000, Train Loss: 0.03019, lr: 6.25e-05\n",
      "Val Loss: 0.03357\n",
      "---------\n",
      "Epoch 231/1000, Train Loss: 0.03019, lr: 6.25e-05\n",
      "Val Loss: 0.03356\n",
      "---------\n",
      "Epoch 232/1000, Train Loss: 0.03018, lr: 6.25e-05\n",
      "Val Loss: 0.03356\n",
      "---------\n",
      "Epoch 233/1000, Train Loss: 0.03018, lr: 6.25e-05\n",
      "Val Loss: 0.03356\n",
      "---------\n",
      "Epoch 234/1000, Train Loss: 0.03018, lr: 6.25e-05\n",
      "Val Loss: 0.03356\n",
      "---------\n",
      "Epoch 235/1000, Train Loss: 0.03017, lr: 6.25e-05\n",
      "Val Loss: 0.03356\n",
      "---------\n",
      "Epoch 236/1000, Train Loss: 0.03017, lr: 6.25e-05\n",
      "Val Loss: 0.03356\n",
      "---------\n",
      "Epoch 237/1000, Train Loss: 0.03016, lr: 6.25e-05\n",
      "Val Loss: 0.03356\n",
      "---------\n",
      "Epoch 238/1000, Train Loss: 0.03016, lr: 6.25e-05\n",
      "Val Loss: 0.03356\n",
      "---------\n",
      "Epoch 239/1000, Train Loss: 0.03016, lr: 6.25e-05\n",
      "Val Loss: 0.03356\n",
      "---------\n",
      "Epoch 240/1000, Train Loss: 0.03015, lr: 6.25e-05\n",
      "Val Loss: 0.03356\n",
      "---------\n",
      "Epoch 241/1000, Train Loss: 0.03015, lr: 6.25e-05\n",
      "Val Loss: 0.03356\n",
      "---------\n",
      "Epoch 242/1000, Train Loss: 0.03015, lr: 6.25e-05\n",
      "Val Loss: 0.03355\n",
      "---------\n",
      "Epoch 243/1000, Train Loss: 0.03014, lr: 6.25e-05\n",
      "Val Loss: 0.03355\n",
      "---------\n",
      "Epoch 244/1000, Train Loss: 0.03014, lr: 6.25e-05\n",
      "Val Loss: 0.03355\n",
      "---------\n",
      "Epoch 245/1000, Train Loss: 0.03014, lr: 6.25e-05\n",
      "Val Loss: 0.03355\n",
      "---------\n",
      "Epoch 246/1000, Train Loss: 0.03013, lr: 6.25e-05\n",
      "Val Loss: 0.03355\n",
      "---------\n",
      "Epoch 247/1000, Train Loss: 0.03013, lr: 6.25e-05\n",
      "Val Loss: 0.03355\n",
      "---------\n",
      "Epoch 248/1000, Train Loss: 0.03013, lr: 6.25e-05\n",
      "Val Loss: 0.03355\n",
      "---------\n",
      "Epoch 249/1000, Train Loss: 0.03012, lr: 6.25e-05\n",
      "Val Loss: 0.03355\n",
      "---------\n",
      "Epoch 250/1000, Train Loss: 0.03012, lr: 6.25e-05\n",
      "Val Loss: 0.03355\n",
      "---------\n",
      "Epoch 251/1000, Train Loss: 0.03012, lr: 6.25e-05\n",
      "Val Loss: 0.03355\n",
      "---------\n",
      "Epoch 252/1000, Train Loss: 0.03012, lr: 6.25e-05\n",
      "Val Loss: 0.03355\n",
      "---------\n",
      "Epoch 253/1000, Train Loss: 0.03011, lr: 6.25e-05\n",
      "Val Loss: 0.03355\n",
      "---------\n",
      "Epoch 254/1000, Train Loss: 0.03011, lr: 6.25e-05\n",
      "Val Loss: 0.03355\n",
      "---------\n",
      "Epoch 255/1000, Train Loss: 0.03011, lr: 6.25e-05\n",
      "Val Loss: 0.03355\n",
      "---------\n",
      "Epoch 256/1000, Train Loss: 0.03010, lr: 6.25e-05\n",
      "Val Loss: 0.03355\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 257/1000, Train Loss: 0.03005, lr: 3.125e-05\n",
      "Val Loss: 0.03355\n",
      "---------\n",
      "Epoch 258/1000, Train Loss: 0.03003, lr: 3.125e-05\n",
      "Val Loss: 0.03354\n",
      "---------\n",
      "Epoch 259/1000, Train Loss: 0.03003, lr: 3.125e-05\n",
      "Val Loss: 0.03354\n",
      "---------\n",
      "Epoch 260/1000, Train Loss: 0.03002, lr: 3.125e-05\n",
      "Val Loss: 0.03354\n",
      "---------\n",
      "Epoch 261/1000, Train Loss: 0.03002, lr: 3.125e-05\n",
      "Val Loss: 0.03354\n",
      "---------\n",
      "Epoch 262/1000, Train Loss: 0.03002, lr: 3.125e-05\n",
      "Val Loss: 0.03354\n",
      "---------\n",
      "Epoch 263/1000, Train Loss: 0.03002, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 264/1000, Train Loss: 0.03002, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 265/1000, Train Loss: 0.03001, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 266/1000, Train Loss: 0.03001, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 267/1000, Train Loss: 0.03001, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 268/1000, Train Loss: 0.03001, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 269/1000, Train Loss: 0.03001, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 270/1000, Train Loss: 0.03000, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 271/1000, Train Loss: 0.03000, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 272/1000, Train Loss: 0.03000, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 273/1000, Train Loss: 0.03000, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 274/1000, Train Loss: 0.03000, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 275/1000, Train Loss: 0.02999, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 276/1000, Train Loss: 0.02999, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 277/1000, Train Loss: 0.02999, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 278/1000, Train Loss: 0.02999, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 279/1000, Train Loss: 0.02999, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 280/1000, Train Loss: 0.02999, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 281/1000, Train Loss: 0.02998, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 282/1000, Train Loss: 0.02998, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 283/1000, Train Loss: 0.02998, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 284/1000, Train Loss: 0.02998, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 285/1000, Train Loss: 0.02998, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 286/1000, Train Loss: 0.02998, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 287/1000, Train Loss: 0.02997, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 288/1000, Train Loss: 0.02997, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 289/1000, Train Loss: 0.02997, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 290/1000, Train Loss: 0.02997, lr: 3.125e-05\n",
      "Val Loss: 0.03353\n",
      "---------\n",
      "Epoch 291/1000, Train Loss: 0.02997, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 292/1000, Train Loss: 0.02997, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 293/1000, Train Loss: 0.02996, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 294/1000, Train Loss: 0.02996, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 295/1000, Train Loss: 0.02996, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 296/1000, Train Loss: 0.02996, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 297/1000, Train Loss: 0.02996, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 298/1000, Train Loss: 0.02996, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 299/1000, Train Loss: 0.02996, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 300/1000, Train Loss: 0.02995, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 301/1000, Train Loss: 0.02995, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 302/1000, Train Loss: 0.02995, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 303/1000, Train Loss: 0.02995, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 304/1000, Train Loss: 0.02995, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 305/1000, Train Loss: 0.02995, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 306/1000, Train Loss: 0.02995, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 307/1000, Train Loss: 0.02994, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 308/1000, Train Loss: 0.02994, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 309/1000, Train Loss: 0.02994, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 310/1000, Train Loss: 0.02994, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 311/1000, Train Loss: 0.02994, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 312/1000, Train Loss: 0.02994, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 313/1000, Train Loss: 0.02994, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 314/1000, Train Loss: 0.02993, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 315/1000, Train Loss: 0.02993, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 316/1000, Train Loss: 0.02993, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 317/1000, Train Loss: 0.02993, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 318/1000, Train Loss: 0.02993, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 319/1000, Train Loss: 0.02993, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 320/1000, Train Loss: 0.02993, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 321/1000, Train Loss: 0.02992, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 322/1000, Train Loss: 0.02992, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 323/1000, Train Loss: 0.02992, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 324/1000, Train Loss: 0.02992, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 325/1000, Train Loss: 0.02992, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 326/1000, Train Loss: 0.02992, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 327/1000, Train Loss: 0.02992, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 328/1000, Train Loss: 0.02992, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 329/1000, Train Loss: 0.02991, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 330/1000, Train Loss: 0.02991, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 331/1000, Train Loss: 0.02991, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 332/1000, Train Loss: 0.02991, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 333/1000, Train Loss: 0.02991, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 334/1000, Train Loss: 0.02991, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 335/1000, Train Loss: 0.02991, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 336/1000, Train Loss: 0.02991, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 337/1000, Train Loss: 0.02990, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 338/1000, Train Loss: 0.02990, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 339/1000, Train Loss: 0.02990, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 340/1000, Train Loss: 0.02990, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 341/1000, Train Loss: 0.02990, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 342/1000, Train Loss: 0.02990, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 343/1000, Train Loss: 0.02990, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 344/1000, Train Loss: 0.02990, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 345/1000, Train Loss: 0.02989, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 346/1000, Train Loss: 0.02989, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 347/1000, Train Loss: 0.02989, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 348/1000, Train Loss: 0.02989, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 349/1000, Train Loss: 0.02989, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 350/1000, Train Loss: 0.02989, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 351/1000, Train Loss: 0.02989, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 352/1000, Train Loss: 0.02989, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 353/1000, Train Loss: 0.02988, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 354/1000, Train Loss: 0.02988, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 355/1000, Train Loss: 0.02988, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 356/1000, Train Loss: 0.02988, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 357/1000, Train Loss: 0.02988, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 358/1000, Train Loss: 0.02988, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 359/1000, Train Loss: 0.02988, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 360/1000, Train Loss: 0.02988, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 361/1000, Train Loss: 0.02987, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 362/1000, Train Loss: 0.02987, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 363/1000, Train Loss: 0.02987, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 364/1000, Train Loss: 0.02987, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 365/1000, Train Loss: 0.02987, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 366/1000, Train Loss: 0.02987, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 367/1000, Train Loss: 0.02987, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 368/1000, Train Loss: 0.02987, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 369/1000, Train Loss: 0.02987, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 370/1000, Train Loss: 0.02986, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 371/1000, Train Loss: 0.02986, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 372/1000, Train Loss: 0.02986, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 373/1000, Train Loss: 0.02986, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 374/1000, Train Loss: 0.02986, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 375/1000, Train Loss: 0.02986, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 376/1000, Train Loss: 0.02986, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 377/1000, Train Loss: 0.02986, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 378/1000, Train Loss: 0.02986, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 379/1000, Train Loss: 0.02985, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 380/1000, Train Loss: 0.02985, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 381/1000, Train Loss: 0.02985, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "Epoch 382/1000, Train Loss: 0.02985, lr: 3.125e-05\n",
      "Val Loss: 0.03352\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 383/1000, Train Loss: 0.02983, lr: 1.5625e-05\n",
      "Val Loss: 0.03351\n",
      "---------\n",
      "Epoch 384/1000, Train Loss: 0.02983, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 385/1000, Train Loss: 0.02982, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 386/1000, Train Loss: 0.02982, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 387/1000, Train Loss: 0.02982, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 388/1000, Train Loss: 0.02982, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 389/1000, Train Loss: 0.02982, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 390/1000, Train Loss: 0.02982, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 391/1000, Train Loss: 0.02982, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 392/1000, Train Loss: 0.02982, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 393/1000, Train Loss: 0.02981, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 394/1000, Train Loss: 0.02981, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 395/1000, Train Loss: 0.02981, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 396/1000, Train Loss: 0.02981, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 397/1000, Train Loss: 0.02981, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 398/1000, Train Loss: 0.02981, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 399/1000, Train Loss: 0.02981, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 400/1000, Train Loss: 0.02981, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 401/1000, Train Loss: 0.02981, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 402/1000, Train Loss: 0.02981, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 403/1000, Train Loss: 0.02981, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 404/1000, Train Loss: 0.02981, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 405/1000, Train Loss: 0.02980, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 406/1000, Train Loss: 0.02980, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 407/1000, Train Loss: 0.02980, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 408/1000, Train Loss: 0.02980, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 409/1000, Train Loss: 0.02980, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 410/1000, Train Loss: 0.02980, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 411/1000, Train Loss: 0.02980, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 412/1000, Train Loss: 0.02980, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 413/1000, Train Loss: 0.02980, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 414/1000, Train Loss: 0.02980, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 415/1000, Train Loss: 0.02980, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 416/1000, Train Loss: 0.02980, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 417/1000, Train Loss: 0.02980, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 418/1000, Train Loss: 0.02980, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 419/1000, Train Loss: 0.02979, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 420/1000, Train Loss: 0.02979, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 421/1000, Train Loss: 0.02979, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 422/1000, Train Loss: 0.02979, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 423/1000, Train Loss: 0.02979, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 424/1000, Train Loss: 0.02979, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 425/1000, Train Loss: 0.02979, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 426/1000, Train Loss: 0.02979, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 427/1000, Train Loss: 0.02979, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 428/1000, Train Loss: 0.02979, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 429/1000, Train Loss: 0.02979, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 430/1000, Train Loss: 0.02979, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 431/1000, Train Loss: 0.02979, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 432/1000, Train Loss: 0.02979, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 433/1000, Train Loss: 0.02979, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 434/1000, Train Loss: 0.02978, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 435/1000, Train Loss: 0.02978, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 436/1000, Train Loss: 0.02978, lr: 1.5625e-05\n",
      "Val Loss: 0.03350\n",
      "---------\n",
      "Epoch 437/1000, Train Loss: 0.02978, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 438/1000, Train Loss: 0.02978, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 439/1000, Train Loss: 0.02978, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 440/1000, Train Loss: 0.02978, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 441/1000, Train Loss: 0.02978, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 442/1000, Train Loss: 0.02978, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 443/1000, Train Loss: 0.02978, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 444/1000, Train Loss: 0.02978, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 445/1000, Train Loss: 0.02978, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 446/1000, Train Loss: 0.02978, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 447/1000, Train Loss: 0.02978, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 448/1000, Train Loss: 0.02978, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 449/1000, Train Loss: 0.02978, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 450/1000, Train Loss: 0.02977, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 451/1000, Train Loss: 0.02977, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 452/1000, Train Loss: 0.02977, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 453/1000, Train Loss: 0.02977, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 454/1000, Train Loss: 0.02977, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 455/1000, Train Loss: 0.02977, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 456/1000, Train Loss: 0.02977, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 457/1000, Train Loss: 0.02977, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 458/1000, Train Loss: 0.02977, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 459/1000, Train Loss: 0.02977, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 460/1000, Train Loss: 0.02977, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 461/1000, Train Loss: 0.02977, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 462/1000, Train Loss: 0.02977, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 463/1000, Train Loss: 0.02977, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 464/1000, Train Loss: 0.02977, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 465/1000, Train Loss: 0.02976, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 466/1000, Train Loss: 0.02976, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 467/1000, Train Loss: 0.02976, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 468/1000, Train Loss: 0.02976, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 469/1000, Train Loss: 0.02976, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 470/1000, Train Loss: 0.02976, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 471/1000, Train Loss: 0.02976, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 472/1000, Train Loss: 0.02976, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 473/1000, Train Loss: 0.02976, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 474/1000, Train Loss: 0.02976, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 475/1000, Train Loss: 0.02976, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 476/1000, Train Loss: 0.02976, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 477/1000, Train Loss: 0.02976, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 478/1000, Train Loss: 0.02976, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 479/1000, Train Loss: 0.02976, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 480/1000, Train Loss: 0.02976, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 481/1000, Train Loss: 0.02975, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 482/1000, Train Loss: 0.02975, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 483/1000, Train Loss: 0.02975, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 484/1000, Train Loss: 0.02975, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 485/1000, Train Loss: 0.02975, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 486/1000, Train Loss: 0.02975, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 487/1000, Train Loss: 0.02975, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 488/1000, Train Loss: 0.02975, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 489/1000, Train Loss: 0.02975, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 490/1000, Train Loss: 0.02975, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 491/1000, Train Loss: 0.02975, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 492/1000, Train Loss: 0.02975, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 493/1000, Train Loss: 0.02975, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 494/1000, Train Loss: 0.02975, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 495/1000, Train Loss: 0.02975, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 496/1000, Train Loss: 0.02975, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 497/1000, Train Loss: 0.02975, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 498/1000, Train Loss: 0.02974, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 499/1000, Train Loss: 0.02974, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 500/1000, Train Loss: 0.02974, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 501/1000, Train Loss: 0.02974, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 502/1000, Train Loss: 0.02974, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 503/1000, Train Loss: 0.02974, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 504/1000, Train Loss: 0.02974, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 505/1000, Train Loss: 0.02974, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 506/1000, Train Loss: 0.02974, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 507/1000, Train Loss: 0.02974, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 508/1000, Train Loss: 0.02974, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 509/1000, Train Loss: 0.02974, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 510/1000, Train Loss: 0.02974, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 511/1000, Train Loss: 0.02974, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 512/1000, Train Loss: 0.02974, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "Epoch 513/1000, Train Loss: 0.02974, lr: 1.5625e-05\n",
      "Val Loss: 0.03349\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 514/1000, Train Loss: 0.02972, lr: 7.8125e-06\n",
      "Val Loss: 0.03348\n",
      "---------\n",
      "Epoch 515/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 516/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 517/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 518/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 519/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 520/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 521/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 522/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 523/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 524/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 525/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 526/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 527/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 528/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 529/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 530/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 531/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 532/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 533/1000, Train Loss: 0.02971, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 534/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 535/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 536/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 537/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 538/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 539/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 540/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 541/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 542/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 543/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 544/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 545/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 546/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 547/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 548/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 549/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 550/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 551/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 552/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 553/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 554/1000, Train Loss: 0.02970, lr: 7.8125e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 555/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 556/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 557/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 558/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 559/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 560/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 561/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 562/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 563/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 564/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 565/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 566/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 567/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 568/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 569/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 570/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 571/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03347\n",
      "---------\n",
      "Epoch 572/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 573/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 574/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 575/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 576/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 577/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 578/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 579/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 580/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 581/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 582/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 583/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 584/1000, Train Loss: 0.02968, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 585/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 586/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 587/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 588/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 589/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 590/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 591/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 592/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 593/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 594/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 595/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 596/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 597/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 598/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 599/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 600/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 601/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 602/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 603/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 604/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 605/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 606/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 607/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 608/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 609/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 610/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 611/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 612/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 613/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 614/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 615/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 616/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 617/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 618/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 619/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 620/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 621/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 622/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 623/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 624/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 625/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 626/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 627/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 628/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 629/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 630/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 631/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 632/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 633/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 634/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 635/1000, Train Loss: 0.02967, lr: 3.90625e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 636/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 637/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 638/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 639/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 640/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 641/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 642/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 643/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 644/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 645/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 646/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 647/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 648/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 649/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 650/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 651/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 652/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 653/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 654/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 655/1000, Train Loss: 0.02966, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 656/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 657/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 658/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 659/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 660/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 661/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 662/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 663/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 664/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 665/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 666/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 667/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 668/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 669/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 670/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 671/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 672/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 673/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 674/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 675/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 676/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 677/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 678/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 679/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 680/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 681/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 682/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 683/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 684/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 685/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 686/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 687/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 688/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 689/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 690/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 691/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 692/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 693/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 694/1000, Train Loss: 0.02965, lr: 1.953125e-06\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 695/1000, Train Loss: 0.02965, lr: 9.765625e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 696/1000, Train Loss: 0.02965, lr: 9.765625e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 697/1000, Train Loss: 0.02965, lr: 9.765625e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 698/1000, Train Loss: 0.02965, lr: 9.765625e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 699/1000, Train Loss: 0.02965, lr: 9.765625e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 700/1000, Train Loss: 0.02965, lr: 9.765625e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 701/1000, Train Loss: 0.02965, lr: 9.765625e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 702/1000, Train Loss: 0.02965, lr: 9.765625e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 703/1000, Train Loss: 0.02964, lr: 4.8828125e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 704/1000, Train Loss: 0.02964, lr: 4.8828125e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 705/1000, Train Loss: 0.02964, lr: 4.8828125e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 706/1000, Train Loss: 0.02964, lr: 4.8828125e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 707/1000, Train Loss: 0.02964, lr: 4.8828125e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 708/1000, Train Loss: 0.02964, lr: 4.8828125e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 709/1000, Train Loss: 0.02964, lr: 4.8828125e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 710/1000, Train Loss: 0.02964, lr: 2.44140625e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 711/1000, Train Loss: 0.02964, lr: 2.44140625e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 712/1000, Train Loss: 0.02964, lr: 2.44140625e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 713/1000, Train Loss: 0.02964, lr: 2.44140625e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 714/1000, Train Loss: 0.02964, lr: 2.44140625e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 715/1000, Train Loss: 0.02964, lr: 2.44140625e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 716/1000, Train Loss: 0.02964, lr: 2.44140625e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 717/1000, Train Loss: 0.02964, lr: 1.220703125e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 718/1000, Train Loss: 0.02964, lr: 1.220703125e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 719/1000, Train Loss: 0.02964, lr: 1.220703125e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 720/1000, Train Loss: 0.02964, lr: 1.220703125e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 721/1000, Train Loss: 0.02964, lr: 1.220703125e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 722/1000, Train Loss: 0.02964, lr: 1.220703125e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 723/1000, Train Loss: 0.02964, lr: 1.220703125e-07\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 724/1000, Train Loss: 0.02964, lr: 6.103515625e-08\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 725/1000, Train Loss: 0.02964, lr: 6.103515625e-08\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 726/1000, Train Loss: 0.02964, lr: 6.103515625e-08\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 727/1000, Train Loss: 0.02964, lr: 6.103515625e-08\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 728/1000, Train Loss: 0.02964, lr: 6.103515625e-08\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 729/1000, Train Loss: 0.02964, lr: 6.103515625e-08\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 730/1000, Train Loss: 0.02964, lr: 6.103515625e-08\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 731/1000, Train Loss: 0.02964, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 732/1000, Train Loss: 0.02964, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 733/1000, Train Loss: 0.02964, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 734/1000, Train Loss: 0.02964, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Epoch 735/1000, Train Loss: 0.02964, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03346\n",
      "---------\n",
      "Early stopping ....\n",
      "4601.206604003906 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABkoAAAJdCAYAAAB9KSs4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABy10lEQVR4nOzdeZiddX03/vc5Z7ZMkslKEgJhDzuCbBGkbtAGay24IlIVpH2e+uBWKm3xp+BSnyiKVSsVl1qxlWLRSq0CEnkEF1DZ1LLIJpAIJCFAMllnOef8/jgzk5lkJmSynTDn9bquc51z7vt73/fnPnMmaN75fL+FarVaDQAAAAAAQAMq1rsAAAAAAACAehGUAAAAAAAADUtQAgAAAAAANCxBCQAAAAAA0LAEJQAAAAAAQMMSlAAAAAAAAA1LUAIAAAAAADQsQQkAAAAAANCwBCUAAAAAAEDDEpQAAAAMo1Ao5EMf+lC9ywAAAHYwQQkAALDNvva1r6VQKOT222+vdyl1d++99+ZDH/pQHn300XqXAgAAbAFBCQAAwHZ077335sMf/rCgBAAAnicEJQAAAAAAQMMSlAAAADvNXXfdlVe+8pXp6OjIhAkTcvLJJ+fnP//5kDE9PT358Ic/nLlz56atrS3Tpk3LSSedlIULFw6MWbJkSc4555zsueeeaW1tze67757TTjvtObs4zj777EyYMCG/+93vMn/+/IwfPz6zZ8/ORz7ykVSr1W2u/2tf+1re8IY3JEle/vKXp1AopFAo5KabbtryDwkAANipmupdAAAA0Bjuueee/MEf/EE6OjryN3/zN2lubs4Xv/jFvOxlL8vNN9+cefPmJUk+9KEPZcGCBfnzP//zHH/88ens7Mztt9+eO++8M3/4h3+YJHnd616Xe+65J+9617uyzz77ZNmyZVm4cGEWLVqUffbZZ7N1lMvlnHrqqXnRi16USy65JNdff30uvvji9Pb25iMf+cg21f+Sl7wk7373u/O5z30u73//+3PIIYckycAzAACw6ylUt+SfTQEAAGzG1772tZxzzjm57bbbcuyxxw475jWveU2uvfba3Hfffdlvv/2SJE8++WQOOuigvPCFL8zNN9+cJDnqqKOy55575nvf+96w51mxYkWmTJmST37yk3nf+943qjrPPvvsXHHFFXnXu96Vz33uc0mSarWaV7/61Vm4cGEef/zxTJ8+PUlSKBRy8cUX50Mf+tCo6v/Wt76VN7zhDfnRj36Ul73sZaOqDwAA2PlMvQUAAOxw5XI5N9xwQ04//fSBkCFJdt9997z5zW/OT3/603R2diZJJk+enHvuuScPPvjgsOcaN25cWlpactNNN+XZZ5/dqnre+c53DrwuFAp55zvfme7u7vzwhz/c5voBAIDnF0EJAACwwz311FNZu3ZtDjrooE32HXLIIalUKlm8eHGS5CMf+UhWrFiRAw88MEcccUQuuOCC/OY3vxkY39ramk984hO57rrrMnPmzLzkJS/JJZdckiVLlmxRLcVicUjYkSQHHnhgkoy4xslo6gcAAJ5fBCUAAMAu5SUveUkefvjhfPWrX83hhx+er3zlKzn66KPzla98ZWDMe9/73jzwwANZsGBB2tra8sEPfjCHHHJI7rrrrjpWDgAAPB8JSgAAgB1ut912S3t7e+6///5N9v32t79NsVjMnDlzBrZNnTo155xzTv793/89ixcvzgte8IKBtUL67b///vnrv/7r3HDDDbn77rvT3d2dSy+99DlrqVQq+d3vfjdk2wMPPJAkIy4EP5r6C4XCc9YAAADsOgQlAADADlcqlfJHf/RH+a//+q8h01stXbo0V155ZU466aR0dHQkSZ5++ukhx06YMCEHHHBAurq6kiRr167N+vXrh4zZf//9M3HixIExz+Xzn//8wOtqtZrPf/7zaW5uzsknn7zN9Y8fPz5JbdF5AABg19dU7wIAAICx46tf/Wquv/76Tba/5z3vyd///d9n4cKFOemkk/J//s//SVNTU774xS+mq6srl1xyycDYQw89NC972ctyzDHHZOrUqbn99tvzrW99a2AB9gceeCAnn3xy3vjGN+bQQw9NU1NTvvOd72Tp0qV505ve9Jw1trW15frrr8/b3va2zJs3L9ddd12+//3v5/3vf3922223EY/b0vqPOuqolEqlfOITn8jKlSvT2tqaV7ziFZkxY8ZoPkoAAGAnEZQAAADbzRe+8IVht5999tk57LDD8pOf/CQXXnhhFixYkEqlknnz5uXf/u3fMm/evIGx7373u/Pd7343N9xwQ7q6urL33nvn7//+73PBBRckSebMmZMzzzwzN954Y/71X/81TU1NOfjgg/Mf//Efed3rXvecNZZKpVx//fV5xzvekQsuuCATJ07MxRdfnIsuumizx21p/bNmzcrll1+eBQsW5Nxzz025XM6PfvQjQQkAAOyiCtVqtVrvIgAAAHaGs88+O9/61reyevXqepcCAADsIqxRAgAAAAAANCxBCQAAAAAA0LAEJQAAAAAAQMOyRgkAAAAAANCwdJQAAAAAAAANS1ACAAAAAAA0rKZ6F7A9VCqVPPHEE5k4cWIKhUK9ywEAAAAAAOqoWq1m1apVmT17dorFzfeMjImg5IknnsicOXPqXQYAAAAAALALWbx4cfbcc8/NjhkTQcnEiROT1G64o6OjztUAAAAAAAD11NnZmTlz5gzkB5szJoKS/um2Ojo6BCUAAAAAAECSbNFyHRZzBwAAAAAAGpagBAAAAAAAaFiCEgAAAAAAoGGNiTVKAAAAAABga5XL5fT09NS7DEapubk5pVJpm88jKAEAAAAAoCFVq9UsWbIkK1asqHcpbKXJkydn1qxZW7Ro+0gEJQAAAAAANKT+kGTGjBlpb2/fpr9sZ+eqVqtZu3Ztli1bliTZfffdt/pcghIAAAAAABpOuVweCEmmTZtW73LYCuPGjUuSLFu2LDNmzNjqabgs5g4AAAAAQMPpX5Okvb29zpWwLfp/ftuyxoygBAAAAACAhmW6ree37fHzE5QAAAAAAAANS1ACAAAAAAANap999slnPvOZup+jnizmDgAAAAAAzxMve9nLctRRR223YOK2227L+PHjt8u5nq8EJQAAAAAAMIZUq9WUy+U0NT13BLDbbrvthIp2babeAgAAAACA54Gzzz47N998cz772c+mUCikUCjk0UcfzU033ZRCoZDrrrsuxxxzTFpbW/PTn/40Dz/8cE477bTMnDkzEyZMyHHHHZcf/vCHQ8658bRZhUIhX/nKV/Ka17wm7e3tmTt3br773e+Oqs5FixbltNNOy4QJE9LR0ZE3vvGNWbp06cD+X//613n5y1+eiRMnpqOjI8ccc0xuv/32JMljjz2WV7/61ZkyZUrGjx+fww47LNdee+3Wf2hbQEcJAAAAAACk1omxrqe80687rrmUQqHwnOM++9nP5oEHHsjhhx+ej3zkI0lqHSGPPvpokuTv/u7v8qlPfSr77bdfpkyZksWLF+eP//iP87GPfSytra35+te/nle/+tW5//77s9dee414nQ9/+MO55JJL8slPfjL/+I//mLPOOiuPPfZYpk6d+pw1ViqVgZDk5ptvTm9vb84777ycccYZuemmm5IkZ511Vl74whfmC1/4QkqlUn71q1+lubk5SXLeeeelu7s7P/7xjzN+/Pjce++9mTBhwnNed1sISgAAAAAAIMm6nnIOvegHO/26935kftpbnvuv6ydNmpSWlpa0t7dn1qxZm+z/yEc+kj/8wz8ceD916tQceeSRA+8/+tGP5jvf+U6++93v5p3vfOeI1zn77LNz5plnJkn+7//9v/nc5z6XX/7ylzn11FOfs8Ybb7wx//M//5NHHnkkc+bMSZJ8/etfz2GHHZbbbrstxx13XBYtWpQLLrggBx98cJJk7ty5A8cvWrQor3vd63LEEUckSfbbb7/nvOa2MvUWAAAAAACMAccee+yQ96tXr8773ve+HHLIIZk8eXImTJiQ++67L4sWLdrseV7wghcMvB4/fnw6OjqybNmyLarhvvvuy5w5cwZCkiQ59NBDM3ny5Nx3331JkvPPPz9//ud/nlNOOSUf//jH8/DDDw+Mffe7352///u/z4tf/OJcfPHF+c1vfrNF190WOkoAAAAAACC1KbDu/cj8ulx3exg/fvyQ9+973/uycOHCfOpTn8oBBxyQcePG5fWvf326u7s3e57+abD6FQqFVCqV7VJjknzoQx/Km9/85nz/+9/Pddddl4svvjhXXXVVXvOa1+TP//zPM3/+/Hz/+9/PDTfckAULFuTSSy/Nu971ru12/Y0JSgAAAAAAILVAYEumwKqnlpaWlMtbto7Kz372s5x99tl5zWtek6TWYdK/nsmOcsghh2Tx4sVZvHjxQFfJvffemxUrVuTQQw8dGHfggQfmwAMPzF/91V/lzDPPzL/8y78M1Dlnzpz85V/+Zf7yL/8yF154Yb785S/v0KDE1FsAAAAAAPA8sc8+++QXv/hFHn300SxfvnyznR5z587Nf/7nf+ZXv/pVfv3rX+fNb37zdu0MGc4pp5ySI444ImeddVbuvPPO/PKXv8xb3/rWvPSlL82xxx6bdevW5Z3vfGduuummPPbYY/nZz36W2267LYccckiS5L3vfW9+8IMf5JFHHsmdd96ZH/3oRwP7dhRByRi2vqecnz20PD97aHm9SwEAAAAAYDt43/vel1KplEMPPTS77bbbZtcb+fSnP50pU6bkxBNPzKtf/erMnz8/Rx999A6tr1Ao5L/+678yZcqUvOQlL8kpp5yS/fbbL9/85jeTJKVSKU8//XTe+ta35sADD8wb3/jGvPKVr8yHP/zhJEm5XM55552XQw45JKeeemoOPPDA/NM//dOOrblarVZ36BV2gs7OzkyaNCkrV65MR0dHvcvZZSx+Zm3+4JIfpbWpmPv//pX1LgcAAAAAYJexfv36PPLII9l3333T1tZW73LYSiP9HEeTG+goGcNKxUKSpPL8z8IAAAAAAGCHEJSMYf1BSbkiKAEAAAAAgOEISsawYqG/oyQZAzOsAQAAAADAdicoGcP6O0qSWlgCAAAAAAAMJSgZw0qFDUGJ6bcAAAAAAGBTgpIxrDjop2tBdwAAAAAA2JSgZAwbPPWWjhIAAAAAANiUoGQMKw6eektHCQAAAAAAbEJQMoYNWcxdRwkAAAAAAGxCUDKGWcwdAAAAAICN7bPPPvnMZz4z4v6zzz47p59++k6rp94EJWNYsWjqLQAAAAAA2BxByRjXP/1WpVLnQgAAAAAAYBckKBnj+qff0lECAAAAAPD89qUvfSmzZ89OZaN/GX/aaafl7W9/e5Lk4YcfzmmnnZaZM2dmwoQJOe644/LDH/5wm67b1dWVd7/73ZkxY0ba2tpy0kkn5bbbbhvY/+yzz+ass87KbrvtlnHjxmXu3Ln5l3/5lyRJd3d33vnOd2b33XdPW1tb9t577yxYsGCb6tnemupdADtWsZikbDF3AAAAAIDnVK0mPWt3/nWb25NBa06P5A1veEPe9a535Uc/+lFOPvnkJMkzzzyT66+/Ptdee22SZPXq1fnjP/7jfOxjH0tra2u+/vWv59WvfnXuv//+7LXXXltV3t/8zd/k29/+dq644orsvffeueSSSzJ//vw89NBDmTp1aj74wQ/m3nvvzXXXXZfp06fnoYceyrp165Ikn/vc5/Ld7343//Ef/5G99torixcvzuLFi7eqjh1FUDLGDXSUCEoAAAAAADavZ23yf2fv/Ou+/4mkZfxzDpsyZUpe+cpX5sorrxwISr71rW9l+vTpefnLX54kOfLII3PkkUcOHPPRj3403/nOd/Ld734373znO0dd2po1a/KFL3whX/va1/LKV74ySfLlL385CxcuzD//8z/nggsuyKJFi/LCF74wxx57bJLaYvH9Fi1alLlz5+akk05KoVDI3nvvPeoadjRTb41x/Qu6m3oLAAAAAOD576yzzsq3v/3tdHV1JUm+8Y1v5E1velOKxdpf969evTrve9/7csghh2Ty5MmZMGFC7rvvvixatGirrvfwww+np6cnL37xiwe2NTc35/jjj899992XJHnHO96Rq666KkcddVT+5m/+JrfccsvA2LPPPju/+tWvctBBB+Xd7353brjhhq299R1GR8kYt2Exd0EJAAAAAMBmNbfXujvqcd0t9OpXvzrVajXf//73c9xxx+UnP/lJ/uEf/mFg//ve974sXLgwn/rUp3LAAQdk3Lhxef3rX5/u7u4dUXmS5JWvfGUee+yxXHvttVm4cGFOPvnknHfeefnUpz6Vo48+Oo888kiuu+66/PCHP8wb3/jGnHLKKfnWt761w+oZLUHJGGcxdwAAAACALVQobNEUWPXU1taW1772tfnGN76Rhx56KAcddFCOPvrogf0/+9nPcvbZZ+c1r3lNklqHyaOPPrrV19t///3T0tKSn/3sZwPTZvX09OS2227Le9/73oFxu+22W972trflbW97W/7gD/4gF1xwQT71qU8lSTo6OnLGGWfkjDPOyOtf//qceuqpeeaZZzJ16tStrmt7EpSMcQNTb+koAQAAAAAYE84666z8yZ/8Se6555782Z/92ZB9c+fOzX/+53/m1a9+dQqFQj74wQ+mUqls9bXGjx+fd7zjHbngggsyderU7LXXXrnkkkuydu3anHvuuUmSiy66KMccc0wOO+ywdHV15Xvf+14OOeSQJMmnP/3p7L777nnhC1+YYrGYq6++OrNmzcrkyZO3uqbtTVAyxvV3lGzD7wEAAAAAALuQV7ziFZk6dWruv//+vPnNbx6y79Of/nTe/va358QTT8z06dPzt3/7t+ns7Nym63384x9PpVLJW97ylqxatSrHHntsfvCDH2TKlClJkpaWllx44YV59NFHM27cuPzBH/xBrrrqqiTJxIkTc8kll+TBBx9MqVTKcccdl2uvvXZgTZVdQaFaff7PydTZ2ZlJkyZl5cqV6ejoqHc5u5QXf/z/5fEV63LNeS/OUXMm17scAAAAAIBdwvr16/PII49k3333TVtbW73LYSuN9HMcTW6w60Q27BD9oZyptwAAAAAAYFOCkjFuYOqt53/jEAAAAAAAbHeCkjHOYu4AAAAAADCyrQpKLrvssuyzzz5pa2vLvHnz8stf/nKz46+++uocfPDBaWtryxFHHJFrr712yP5CoTDs45Of/OTWlMcgGxZzF5QAAAAAAMDGRh2UfPOb38z555+fiy++OHfeeWeOPPLIzJ8/P8uWLRt2/C233JIzzzwz5557bu66666cfvrpOf3003P33XcPjHnyySeHPL761a+mUCjkda973dbfGUmSUn9Hiam3AAAAAAA2UfV3p89r2+PnN+qg5NOf/nT+4i/+Iuecc04OPfTQXH755Wlvb89Xv/rVYcd/9rOfzamnnpoLLrgghxxySD760Y/m6KOPzuc///mBMbNmzRry+K//+q+8/OUvz3777bf1d0aSpFgw9RYAAAAAwMaam5uTJGvXrq1zJWyL/p9f/89zazSNZnB3d3fuuOOOXHjhhQPbisViTjnllNx6663DHnPrrbfm/PPPH7Jt/vz5ueaaa4Ydv3Tp0nz/+9/PFVdcMZrSGEF/R4nF3AEAAAAANiiVSpk8efLAbEnt7e0p9P3Dc3Z91Wo1a9euzbJlyzJ58uSUSqWtPteogpLly5enXC5n5syZQ7bPnDkzv/3tb4c9ZsmSJcOOX7JkybDjr7jiikycODGvfe1rR6yjq6srXV1dA+87Ozu39BYazobF3OtcCAAAAADALmbWrFlJMuLSEuz6Jk+ePPBz3FqjCkp2hq9+9as566yz0tbWNuKYBQsW5MMf/vBOrOr5q9QXgJp6CwAAAABgqEKhkN133z0zZsxIT09PvcthlJqbm7epk6TfqIKS6dOnp1QqZenSpUO2L126dMTEZtasWVs8/ic/+Unuv//+fPOb39xsHRdeeOGQ6bw6OzszZ86cLb2NhmLqLQAAAACAzSuVStvlL9x5fhrVYu4tLS055phjcuONNw5sq1QqufHGG3PCCScMe8wJJ5wwZHySLFy4cNjx//zP/5xjjjkmRx555GbraG1tTUdHx5AHw7OYOwAAAAAAjGzUU2+df/75edvb3pZjjz02xx9/fD7zmc9kzZo1Oeecc5Ikb33rW7PHHntkwYIFSZL3vOc9eelLX5pLL700r3rVq3LVVVfl9ttvz5e+9KUh5+3s7MzVV1+dSy+9dDvcFv10lAAAAAAAwMhGHZScccYZeeqpp3LRRRdlyZIlOeqoo3L99dcPLNi+aNGiFIsbGlVOPPHEXHnllfnABz6Q97///Zk7d26uueaaHH744UPOe9VVV6VarebMM8/cxltisFJRRwkAAAAAAIykUK0+/1sNOjs7M2nSpKxcudI0XBt521d/mZsfeCqffP0L8oZjreMCAAAAAMDYN5rcYFRrlPD8Y+otAAAAAAAYmaBkjNuwmHudCwEAAAAAgF2QoGSMK/X9hMs6SgAAAAAAYBOCkjFuYOoti7kDAAAAAMAmBCVj3IaptwQlAAAAAACwMUHJGGcxdwAAAAAAGJmgZIwr6SgBAAAAAIARCUrGuGJfR4nF3AEAAAAAYFOCkjGuv6PEYu4AAAAAALApQckYN9BRUqlzIQAAAAAAsAsSlIxxpb6fsKm3AAAAAABgU4KSMc7UWwAAAAAAMDJByRhnMXcAAAAAABiZoGSM01ECAAAAAAAjE5SMcaWBxdwFJQAAAAAAsDFByRhn6i0AAAAAABiZoGSM6596S04CAAAAAACbEpSMcUVTbwEAAAAAwIgEJWNcf0eJqbcAAAAAAGBTgpIxrtT3E67oKAEAAAAAgE0ISsY4U28BAAAAAMDIBCVjnKm3AAAAAABgZIKSMa7U11Fi6i0AAAAAANiUoGSMKw50lNS5EAAAAAAA2AUJSsY4HSUAAAAAADAyQckYZzF3AAAAAAAYmaBkjLOYOwAAAAAAjExQMsaV+n7Cpt4CAAAAAIBNCUrGuKKOEgAAAAAAGJGgZIwrWaMEAAAAAABGJCgZ4/qDkoqOEgAAAAAA2ISgZIwbmHpLRwkAAAAAAGxCUDLGDXSUVOpcCAAAAAAA7IIEJWOcxdwBAAAAAGBkgpIxzmLuAAAAAAAwMkHJGNeXk1jMHQAAAAAAhiEoGeOKOkoAAAAAAGBEgpIxrlQQlAAAAAAAwEgEJWNc/xolpt4CAAAAAIBNCUrGuKKOEgAAAAAAGJGgZIzb0FFS50IAAAAAAGAXJCgZ40p9P2EdJQAAAAAAsClByRhn6i0AAAAAABiZoGSMs5g7AAAAAACMTFAyxukoAQAAAACAkQlKxjgdJQAAAAAAMDJByRjXH5ToKAEAAAAAgE0JSsY4U28BAAAAAMDIBCVj3Iapt+pcCAAAAAAA7IIEJWNcSUcJAAAAAACMSFAyxhX7fsJli7kDAAAAAMAmBCVj3MDUWzpKAAAAAABgE4KSMW5g6i0dJQAAAAAAsAlByRhX7OsoqVaTqrAEAAAAAACGEJSMcf0dJYkF3QEAAAAAYGOCkjGuv6MkMf0WAAAAAABsTFAyxpUGBSWVSh0LAQAAAACAXZCgZIwbMvWWjhIAAAAAABhCUDLGFQf9hK1RAgAAAAAAQwlKxrjBHSUVQQkAAAAAAAwhKBnjShZzBwAAAACAEQlKxrhCoZD+phIdJQAAAAAAMJSgpAH0T7+lowQAAAAAAIYSlDSAYt/0WxZzBwAAAACAoQQlDaC/o6RSqXMhAAAAAACwixGUNID+Bd1NvQUAAAAAAEMJShpAX05i6i0AAAAAANiIoKQB9HeUVHSUAAAAAADAEIKSBlCymDsAAAAAAAxLUNIAigVBCQAAAAAADEdQ0gBMvQUAAAAAAMMTlDQAHSUAAAAAADA8QUkD0FECAAAAAADDE5Q0gA2Lude5EAAAAAAA2MUIShpAX05i6i0AAAAAANiIoKQBmHoLAAAAAACGJyhpABZzBwAAAACA4QlKGsDAGiU6SgAAAAAAYAhBSQMYmHpLRwkAAAAAAAwhKGkA/VNvyUkAAAAAAGAoQUkDGJh6S1ICAAAAAABDCEoaQGmgo0RQAgAAAAAAgwlKGkCx76esowQAAAAAAIYSlDSAgcXcdZQAAAAAAMAQgpIG0L+Yu44SAAAAAAAYSlDSACzmDgAAAAAAwxOUNACLuQMAAAAAwPAEJQ2gONBRUudCAAAAAABgFyMoaQD9HSVlHSUAAAAAADCEoKQB9K9RUrFGCQAAAAAADCEoaQBFi7kDAAAAAMCwBCUNoFTLSSzmDgAAAAAAGxGUNAAdJQAAAAAAMDxBSQOwmDsAAAAAAAxPUNIALOYOAAAAAADDE5Q0gA1Tb9W5EAAAAAAA2MUIShqAqbcAAAAAAGB4gpIGYOotAAAAAAAYnqCkARR1lAAAAAAAwLAEJQ2g1PdT1lECAAAAAABDCUoawIbF3AUlAAAAAAAwmKCkAVjMHQAAAAAAhicoaQAWcwcAAAAAgOFtVVBy2WWXZZ999klbW1vmzZuXX/7yl5sdf/XVV+fggw9OW1tbjjjiiFx77bWbjLnvvvvyp3/6p5k0aVLGjx+f4447LosWLdqa8tiIxdwBAAAAAGB4ow5KvvnNb+b888/PxRdfnDvvvDNHHnlk5s+fn2XLlg07/pZbbsmZZ56Zc889N3fddVdOP/30nH766bn77rsHxjz88MM56aSTcvDBB+emm27Kb37zm3zwgx9MW1vb1t8ZA0oDa5TUuRAAAAAAANjFFKrV0bUZzJs3L8cdd1w+//nPJ0kqlUrmzJmTd73rXfm7v/u7TcafccYZWbNmTb73ve8NbHvRi16Uo446KpdffnmS5E1velOam5vzr//6r1t1E52dnZk0aVJWrlyZjo6OrTrHWHbZjx7KJ39wf844dk4+8foX1LscAAAAAADYoUaTG4yqo6S7uzt33HFHTjnllA0nKBZzyimn5NZbbx32mFtvvXXI+CSZP3/+wPhKpZLvf//7OfDAAzN//vzMmDEj8+bNyzXXXDOa0tgMU28BAAAAAMDwRhWULF++POVyOTNnzhyyfebMmVmyZMmwxyxZsmSz45ctW5bVq1fn4x//eE499dTccMMNec1rXpPXvva1ufnmm4c9Z1dXVzo7O4c8GFmp76dsMXcAAAAAABiqqd4FVCq1hTNOO+20/NVf/VWS5Kijjsott9ySyy+/PC996Us3OWbBggX58Ic/vFPrfD7TUQIAAAAAAMMbVUfJ9OnTUyqVsnTp0iHbly5dmlmzZg17zKxZszY7fvr06Wlqasqhhx46ZMwhhxySRYsWDXvOCy+8MCtXrhx4LF68eDS30ThWL0u+85d56b0XJUnKOkoAAAAAAGCIUQUlLS0tOeaYY3LjjTcObKtUKrnxxhtzwgknDHvMCSecMGR8kixcuHBgfEtLS4477rjcf//9Q8Y88MAD2XvvvYc9Z2trazo6OoY8GEa5O/n1v2e/JdcnSSo6SgAAAAAAYIhRT711/vnn521ve1uOPfbYHH/88fnMZz6TNWvW5JxzzkmSvPWtb80ee+yRBQsWJEne85735KUvfWkuvfTSvOpVr8pVV12V22+/PV/60pcGznnBBRfkjDPOyEte8pK8/OUvz/XXX5///u//zk033bR97rJRNbcnSUrVnpRS1lECAAAAAAAbGXVQcsYZZ+Spp57KRRddlCVLluSoo47K9ddfP7Bg+6JFi1IsbmhUOfHEE3PllVfmAx/4QN7//vdn7ty5ueaaa3L44YcPjHnNa16Tyy+/PAsWLMi73/3uHHTQQfn2t7+dk046aTvcYgNrGT/wsj1dKVfqWAsAAAAAAOyCCtXq838+ps7OzkyaNCkrV640Dddg1WrykWlJtZzj1l+WIw4+KF89+7h6VwUAAAAAADvUaHKDUa1RwvNMoTAw/VZ7ocvUWwAAAAAAsBFByVjX0heUpMti7gAAAAAAsBFByVjX11EyLjpKAAAAAABgY4KSsa5vQXdTbwEAAAAAwKYEJWNd/xolWW/qLQAAAAAA2IigZKxrMfUWAAAAAACMRFAy1vWvUVLoTllOAgAAAAAAQwhKxrqBqbe6UtFRAgAAAAAAQwhKxjpTbwEAAAAAwIgEJWNd8/gkSXvBYu4AAAAAALAxQclY17Jh6i0dJQAAAAAAMJSgZKxrHjT1lo4SAAAAAAAYQlAy1rX0T71lMXcAAAAAANiYoGSsax6XREcJAAAAAAAMR1Ay1vUt5j4uXalU6lwLAAAAAADsYgQlY13/Yu6FrnSXJSUAAAAAADCYoGSsG7SY++r1vXUuBgAAAAAAdi2CkrGufzH3dGVdTzm9ukoAAAAAAGCAoGSsa94w9VaSrO7SVQIAAAAAAP0EJWNd87gktY6SJFll+i0AAAAAABggKBnr+qbeGlfoSlIVlAAAAAAAwCCCkrGub+qtYqppTY+ptwAAAAAAYBBByVjX11GSJOPSlVXre+pYDAAAAAAA7FoEJWNdsZSUWpPU1inRUQIAAAAAABsIShpBS236rXGFrnRaowQAAAAAAAYIShpBc236rfZ0ZbWgBAAAAAAABghKGkHzuCS1oMQaJQAAAAAAsIGgpBEMTL213holAAAAAAAwiKCkEfRNvTUu3Vll6i0AAAAAABggKGkEfR0ltam3BCUAAAAAANBPUNIImvun3rJGCQAAAAAADCYoaQQttam32mONEgAAAAAAGExQ0giaxyVJ2gum3gIAAAAAgMEEJY2gf+qtdOkoAQAAAACAQQQljWBg6q3aGiXVarXOBQEAAAAAwK5BUNII+jpK2gtd6SlX09VbqXNBAAAAAACwaxCUNIK+jpJx6UoS65QAAAAAAEAfQUkj6OsomVjsSZKsWt9Tz2oAAAAAAGCXIShpBC21oGRCsdZRYkF3AAAAAACoEZQ0gr6OkvHF7iSm3gIAAAAAgH6CkkbQv5i7NUoAAAAAAGAIQUkj6FvMvT3rklijBAAAAAAA+glKGsGEmUmSSeVnU0zFGiUAAAAAANBHUNIIJs5Kik0ppZwZedbUWwAAAAAA0EdQ0giKpaRjdpJkduFpHSUAAAAAANBHUNIoJs1JkuxZWG6NEgAAAAAA6CMoaRR9QcnswnJTbwEAAAAAQB9BSaOYtGeSZA9BCQAAAAAADBCUNIq+oMQaJQAAAAAAsIGgpFFM3jD11hpBCQAAAAAAJBGUNI6+NUr2KCxPd7lS52IAAAAAAGDXIChpFH1Tb3UU1qW5Z1WdiwEAAAAAgF2DoKRRtIxPb+uUJMm03mV1LgYAAAAAAHYNgpIG0jtxjyTJ9LKgBAAAAAAAEkFJQyl31Kbf2k1QAgAAAAAASQQlDaXaF5TMrC6vcyUAAAAAALBrEJQ0kELfgu6z8lSq1WqdqwEAAAAAgPoTlDSQwpS9kiSzC0+nu1ypczUAAAAAAFB/gpIG0tQ+JUkyIevS3SsoAQAAAAAAQUkDaWppS5K0pCc9ZVNvAQAAAACAoKSBFJtakyQthV4dJQAAAAAAEEFJY2lqSZK0RFACAAAAAACJoKSxlPo6StKT7nK5zsUAAAAAAED9CUoaSak5SX9HiTVKAAAAAABAUNJI+tYoaU5vusum3gIAAAAAAEFJI+mbequ5UE53T2+diwEAAAAAgPoTlDSSvqm3kqTcvb6OhQAAAAAAwK5BUNJI+qbeSpKe7q46FgIAAAAAALsGQUkjKbUMvOzVUQIAAAAAAIKShlIopDdNSZLeHkEJAAAAAAAIShpMT6G2Tkmlx9RbAAAAAAAgKGkwvX1BSVlQAgAAAAAAgpJGUx7oKDH1FgAAAAAACEoaTLnY31HSXedKAAAAAACg/gQlDaZcaEmSVHpNvQUAAAAAAIKSBlPp6yipCkoAAAAAAEBQ0mj6p97SUQIAAAAAAIKShlMp1qbeSq81SgAAAAAAQFDSYKql/qBERwkAAAAAAAhKGky1r6OkWtZRAgAAAAAAgpIGU9FRAgAAAAAAAwQljaYvKCnoKAEAAAAAAEFJw+nvKBGUAAAAAACAoKTR9C/mXqgISgAAAAAAQFDSYApNrbVnHSUAAAAAACAoaTh9HSXFSk+dCwEAAAAAgPoTlDSYQlNfUKKjBAAAAAAABCWNpn/qraI1SgAAAAAAQFDSaPqDklJVUAIAAAAAAIKSBlNs7u8o6a1zJQAAAAAAUH+CkgZT1FECAAAAAAADBCUNptTXUdJU6alzJQAAAAAAUH+CkgbTH5SUqqbeAgAAAAAAQUmDKTW3JUmaTb0FAAAAAACCkkZTbOmbeis9qVarda4GAAAAAADqS1DSYJr6pt5qTm96yoISAAAAAAAam6CkwTS31Kbeak1vusuVOlcDAAAAAAD1JShpMKW+oKQlPenuFZQAAAAAANDYBCUNptS0YeotQQkAAAAAAI1OUNJomlqSJC2F3vSYegsAAAAAgAYnKGk0pVpHSUt60qWjBAAAAACABicoaTSl5iRJc8qm3gIAAAAAoOEJShpN04aOkm5TbwEAAAAA0OAEJY2mb+qt1kJvenrLdS4GAAAAAADqa6uCkssuuyz77LNP2traMm/evPzyl7/c7Pirr746Bx98cNra2nLEEUfk2muvHbL/7LPPTqFQGPI49dRTt6Y0nkvf1FtJ0tPdVcdCAAAAAACg/kYdlHzzm9/M+eefn4svvjh33nlnjjzyyMyfPz/Lli0bdvwtt9ySM888M+eee27uuuuunH766Tn99NNz9913Dxl36qmn5sknnxx4/Pu///vW3RGb1zf1VpL0dq+vYyEAAAAAAFB/ow5KPv3pT+cv/uIvcs455+TQQw/N5Zdfnvb29nz1q18ddvxnP/vZnHrqqbngggtyyCGH5KMf/WiOPvrofP7znx8yrrW1NbNmzRp4TJkyZevuiM0rtQy87O0RlAAAAAAA0NhGFZR0d3fnjjvuyCmnnLLhBMViTjnllNx6663DHnPrrbcOGZ8k8+fP32T8TTfdlBkzZuSggw7KO97xjjz99NMj1tHV1ZXOzs4hD7ZQsZRy34+9t7u7zsUAAAAAAEB9jSooWb58ecrlcmbOnDlk+8yZM7NkyZJhj1myZMlzjj/11FPz9a9/PTfeeGM+8YlP5Oabb84rX/nKlMvDLza+YMGCTJo0aeAxZ86c0dxGw+st1LpKyjpKAAAAAABocE31LiBJ3vSmNw28PuKII/KCF7wg+++/f2666aacfPLJm4y/8MILc/755w+87+zsFJaMQm+hOa3V9YISAAAAAAAa3qg6SqZPn55SqZSlS5cO2b506dLMmjVr2GNmzZo1qvFJst9++2X69Ol56KGHht3f2tqajo6OIQ+2XG+hlo9Vek29BQAAAABAYxtVUNLS0pJjjjkmN95448C2SqWSG2+8MSeccMKwx5xwwglDxifJwoULRxyfJL///e/z9NNPZ/fddx9NeWyhct/UW5WerjpXAgAAAAAA9TWqoCRJzj///Hz5y1/OFVdckfvuuy/veMc7smbNmpxzzjlJkre+9a258MILB8a/5z3vyfXXX59LL700v/3tb/OhD30ot99+e975zncmSVavXp0LLrggP//5z/Poo4/mxhtvzGmnnZYDDjgg8+fP3063yWDlYnOSpGLqLQAAAAAAGtyo1yg544wz8tRTT+Wiiy7KkiVLctRRR+X6668fWLB90aJFKRY35C8nnnhirrzyynzgAx/I+9///sydOzfXXHNNDj/88CRJqVTKb37zm1xxxRVZsWJFZs+enT/6oz/KRz/60bS2tm6n22SwcrG/o8TUWwAAAAAANLZCtVqt1ruIbdXZ2ZlJkyZl5cqV1ivZAk9ccnxmr70/Vx/0D3nDmW+vdzkAAAAAALBdjSY3GPXUWzz/Vfo6Sqq91igBAAAAAKCxCUoa0EBQUjb1FgAAAAAAjU1Q0oCqfYu5F8o6SgAAAAAAaGyCkgZULfVPvaWjBAAAAACAxiYoaUD9QYmOEgAAAAAAGp2gpBH1BSXp7alvHQAAAAAAUGeCkkbU1NdRUjH1FgAAAAAAjU1Q0oAKTa2157KgBAAAAACAxiYoaUClvqCk0muNEgAAAAAAGpugpAGVWtpqL3p1lAAAAAAA0NgEJQ2oqT8oKesoAQAAAACgsQlKGlBzS23qrZR76lsIAAAAAADUmaCkATX3dZQUK92pVqt1rgYAAAAAAOpHUNKAWlrH1Z7Tk7Xd5TpXAwAAAAAA9SMoaUDNrbWOkub0Zk1Xb52rAQAAAACA+hGUNKBCqSVJ0pLerBaUAAAAAADQwAQljah1YpJkemFl1nSZegsAAAAAgMYlKGlEexyTJDm08FjWru6sczEAAAAAAFA/gpJGNHlOniruluZCOaUn76h3NQAAAAAAUDeCkgb1YOthSZLxS2+rcyUAAAAAAFA/gpIG9cj4I5Mkk5+6vc6VAAAAAABA/QhKGtQTE2tByfQVv07KvXWuBgAAAAAA6kNQ0qBWdczNymp7msvrkiW/qXc5AAAAAABQF4KSBtXe1pLbKwfV3iz6eX2LAQAAAACAOhGUNKgJraXcWZlbe/Pkr+paCwAAAAAA1IugpEGNb23Ks5lYe9O1ur7FAAAAAABAnQhKGtT41qasq7bU3vSuq28xAAAAAABQJ4KSBjWhtSnr0xeU9AhKAAAAAABoTIKSBjW+tSnrBCUAAAAAADQ4QUmDmtBaSpegBAAAAACABicoaVC1NUpaa2+sUQIAAAAAQIMSlDSo8S3WKAEAAAAAAEFJg5owaI2Sas/6OlcDAAAAAAD1IShpUONbm7K+2t9Rsra+xQAAAAAAQJ0IShpUS1Mx5VJbkqRQLSflnjpXBAAAAAAAO5+gpIGVWsZteKOrBAAAAACABiQoaWDNreNSqRZqb6xTAgAAAABAAxKUNLAJbc0DC7rrKAEAAAAAoBEJShrY+NamrO8PSnp1lAAAAAAA0HgEJQ1sfGtT1qW19kZHCQAAAAAADUhQ0sAmtjalq9pce2ONEgAAAAAAGpCgpIGNby0N6ihZV99iAAAAAACgDgQlDWzoGiWCEgAAAAAAGo+gpIFNaG3KumpfUKKjBAAAAACABiQoaWBDOkoEJQAAAAAANCBBSQOb2CYoAQAAAACgsQlKGtikcc1ZX7VGCQAAAAAAjUtQ0sAmjWvOurTW3ugoAQAAAACgAQlKGtikcc2m3gIAAAAAoKEJShpYraNEUAIAAAAAQOMSlDSwyeNa0tW3Rkm5W1ACAAAAAEDjEZQ0sIltTVlXqAUlPevX1LkaAAAAAADY+QQlDaxYLCRN45Ikvd1r61wNAAAAAADsfIKSBldsqQUllS5BCQAAAAAAjUdQ0uBKLe1Jkoo1SgAAAAAAaECCkgbX1Dq+9qJHRwkAAAAAAI1HUNLgmttqHSXpXV/fQgAAAAAAoA4EJQ2uZVyto6TYa+otAAAAAAAaj6CkwbWNm5AkKZZ1lAAAAAAA0HgEJQ2urb0WlDRVBCUAAAAAADQeQUmDa+8LSporXUm1WudqAAAAAABg5xKUNLj28ROTJKVUknJPnasBAAAAAICdS1DS4CZMmLDhjQXdAQAAAABoMIKSBtcxfnzK1ULtTY+gBAAAAACAxiIoaXCT2luyPi21N4ISAAAAAAAajKCkwU0a15x1aU2SdK9fXedqAAAAAABg5xKUNLiJbU0DHSWr1whKAAAAAABoLIKSBlcsFtJdqHWUrF29qs7VAAAAAADAziUoIT19Qcm6tWvqXAkAAAAAAOxcghJSLrUlSdav1VECAAAAAEBjEZSwIShZt7bOlQAAAAAAwM4lKCHVplpQ0r3OYu4AAAAAADQWQQmpNo1LkvR06SgBAAAAAKCxCEpIoaUWlPR2WcwdAAAAAIDGIighxZb2JEm5a12dKwEAAAAAgJ1LUEKaWmtBSaXb1FsAAAAAADQWQQkpjp+WJGnrWl7nSgAAAAAAYOcSlJDitP2TJDN6Hq9zJQAAAAAAsHMJSkjLzAOTJHtUnqhzJQAAAAAAsHMJSsj4WQckSSZndXpXmX4LAAAAAIDGISghkzom5fFqbZ2S1U/+ts7VAAAAAADAziMoIU2lYhYXZidJupY8WOdqAAAAAABg5xGUkCRZ2rRHkqS8XFACAAAAAEDjEJSQJFneOidJUnzm4TpXAgAAAAAAO4+ghCTJqva9kiStnY/WtxAAAAAAANiJBCUkSdZN3DdJMmH1o0m1Wt9iAAAAAABgJxGUkCQpT947vdVimivrk1VP1rscAAAAAADYKQQlJEkmjR+XxdXdam+etk4JAAAAAACNQVBCkmTy+JY8Ut299ubpB+tbDAAAAAAA7CSCEpIkU9qbc1+1tqB7HvlxfYsBAAAAAICdRFBCkmRqe0uuKx9fe/PAD5LuNfUtCAAAAAAAdgJBCUmSye0tubu6bxZnVtKzNrn/unqXBAAAAAAAO5yghCTJlPHNSQr57/KLahvu+U5d6wEAAAAAgJ1BUEKSZEp7S5Lkv3r7gpIHb0jWr6xjRQAAAAAAsOMJSkiStDWX0tZczP3VOemZMjcpd9fWKgEAAAAAgDFMUMKAWldJIc/u8bLahkU/r2c5AAAAAACwwwlKGDC5b/qtpyYdUdvw+9vqWA0AAAAAAOx4ghIGTB3fnCRZPP7w2oal9yTda+pYEQAAAAAA7FiCEgb0d5Q8WZ2aTNw9qZaTJ35V36IAAAAAAGAHEpQwYLcJrUmSJZ3rkz2PrW18/PY6VgQAAAAAADuWoIQB+04fnyT53VNrkj2Pq220TgkAAAAAAGOYoIQB++3WH5Ss3hCULL4tqVbrWBUAAAAAAOw4ghIG7L/bhCTJY0+vTc/MFySFUrJ6SdL5eJ0rAwAAAACAHWOrgpLLLrss++yzT9ra2jJv3rz88pe/3Oz4q6++OgcffHDa2tpyxBFH5Nprrx1x7F/+5V+mUCjkM5/5zNaUxjaY1dGW9pZSeivVLFqVZOZhtR2P31HXugAAAAAAYEcZdVDyzW9+M+eff34uvvji3HnnnTnyyCMzf/78LFu2bNjxt9xyS84888yce+65ueuuu3L66afn9NNPz913373J2O985zv5+c9/ntmzZ4/+TthmxWJh6Dols46o7Vj22zpWBQAAAAAAO86og5JPf/rT+Yu/+Iucc845OfTQQ3P55Zenvb09X/3qV4cd/9nPfjannnpqLrjgghxyyCH56Ec/mqOPPjqf//znh4x7/PHH8653vSvf+MY30tzcvHV3wzbrn37r4adWJ7sdVNv4lKAEAAAAAICxaVRBSXd3d+64446ccsopG05QLOaUU07JrbfeOuwxt95665DxSTJ//vwh4yuVSt7ylrfkggsuyGGHHfacdXR1daWzs3PIg+1jIChZtjrZ7ZDaRkEJAAAAAABj1KiCkuXLl6dcLmfmzJlDts+cOTNLliwZ9pglS5Y85/hPfOITaWpqyrvf/e4tqmPBggWZNGnSwGPOnDmjuQ02Y7/d+qbeWr5mQ0fJ8geTcm8dqwIAAAAAgB1jqxZz357uuOOOfPazn83Xvva1FAqFLTrmwgsvzMqVKwceixcv3sFVNo7+jpKHlq1OddKeSfP4pNKTPPO7OlcGAAAAAADb36iCkunTp6dUKmXp0qVDti9dujSzZs0a9phZs2ZtdvxPfvKTLFu2LHvttVeamprS1NSUxx57LH/913+dffbZZ9hztra2pqOjY8iD7WPf6eNTKCQr1/XkmbW9yW4H1naYfgsAAAAAgDFoVEFJS0tLjjnmmNx4440D2yqVSm688caccMIJwx5zwgknDBmfJAsXLhwY/5a3vCW/+c1v8qtf/WrgMXv27FxwwQX5wQ9+MNr7YRuNayll9qRxSZKHn1qT7HZwbYegBAAAAACAMahptAecf/75edvb3pZjjz02xx9/fD7zmc9kzZo1Oeecc5Ikb33rW7PHHntkwYIFSZL3vOc9eelLX5pLL700r3rVq3LVVVfl9ttvz5e+9KUkybRp0zJt2rQh12hubs6sWbNy0EEHbev9sRX2nzEhj69Yl/uXrsrxghIAAAAAAMawUQclZ5xxRp566qlcdNFFWbJkSY466qhcf/31Awu2L1q0KMXihkaVE088MVdeeWU+8IEP5P3vf3/mzp2ba665Jocffvj2uwu2q6P3mpwfP/BUfv7w03nLsX1ByTJBCQAAAAAAY0+hWq1W613Eturs7MykSZOycuVK65VsB3c89kxe94VbM7m9OXecNzelfzwqKbUk738yKY06WwMAAAAAgJ1qNLnBqNYooTG8YM/JmdDalBVre3Lv2slJc3tS7k6e+V29SwMAAAAAgO1KUMImmkvFvGi/2roxP3n46WT2C2s7/ufqOlYFAAAAAADbn6CEYf3B3OlJkp8+uDw5/n/VNt725aR7TR2rAgAAAACA7UtQwrBefEAtKLn90Wezbv8/Tqbsm6x7Nrnr3+pcGQAAAAAAbD+CEoa1/27js/uktnSXK7nlkWeTE99Z23Hr55Nyb32LAwAAAACA7URQwrAKhUJeefjuSZIrbn0sOeqspH16smJRcu819S0OAAAAAAC2E0EJIzr7xH1SLCQ/fuCp3P90bzLvf9d2/OyzSbVa3+IAAAAAAGA7EJQwor2mtWf+YbOSJF/96SPJcX+eNLcnS36T/O6m+hYHAAAAAADbgaCEzTr3pH2TJN/51eN5qjw+eeFbajt+9tk6VgUAAAAAANuHoITNOmbvKTlqzuR091bypR8/nJxwXlIoJb/7UXL/9fUuDwAAAAAAtomghM0qFAp57ylzkyRfv/WxLCvNTE74P7Wd//3uZO0zdawOAAAAAAC2jaCE5/TSA3fL0XtNTldvJV+4+eHk5R9Iph+UrF6afO+9SaVS7xIBAAAAAGCrCEp4ToVCIef/4UFJkm/8YlGeXFtNXnN5bQque/8rue5vkmq1zlUCAAAAAMDoCUrYIi8+YFqO33dqunsr+fQNDyR7HJ2c/k9JCsltX05u+EC9SwQAAAAAgFETlLBFCoVC/u6VBydJvn3n73P/klXJkW9K/vRztQG3fj657St1rBAAAAAAAEZPUMIWO3qvKXnl4bNSqSafuP63fRvfmpx8Ue31tX+TPPz/6lcgAAAAAACMkqCEUblg/kFpKhby/367LDfet7S28aTzkyPPTKrl5D/OTp56oK41AgAAAADAlhKUMCr77TYh5560b5Lkg9fcnTVdvUmhkLz6s8mcFyVdK5Mr35isebrOlQIAAAAAwHMTlDBq7zllbvacMi5PrFyfT91wf21jU2vypm8kk/dKnn0k+fwxyXV/l6x8vL7FAgAAAADAZghKGLX2lqb8/emHJ0n+5WeP5hu/eKy2Y/z05M3/UQtL1j2b/OILyRf/IPndTfUrFgAAAAAANkNQwlZ52UEz8o6X7Z8k+cA1d+c/7/x9bceMQ5J3/yo561vJ7kcma59O/vU1yQ0fTNavrF/BAAAAAAAwDEEJW+1v5h+Ut52wd6rV5O/+83/y0LLVtR3FUjL3D5O3/yA56qykWklu+VzyuRcm91xT15oBAAAAAGAwQQlbrVAo5OJXH5aXHLhbunsrueBbv065Ut0woHlcctpltem4ph9Y6y65+m3JtRckvV31KxwAAAAAAPoIStgmxWIhH3/tEZnQ2pS7Fq3Il3/yu6EDCoXkwPnJO25JTvqr2rZffin56vzk2Ud3er0AAAAAADCYoIRtNnvyuPx/rzokSfLx636bL//4d5sOKjUnp3woefPVybgpyRN3JZe/JHnwhzu3WAAAAAAAGERQwnbxpuPm5NyT9k2SfOza+/Kh796TnnJl04EH/lHyv3+S7Hl80rUyufINya2XJb3dO7liAAAAAABICtVqtfrcw3ZtnZ2dmTRpUlauXJmOjo56l9OwqtVqvvTj32XBdb9Nkszbd2o+/+ajs9vE1k0H93Yl3zs/+dW/1d6Pm5Ic9trk+L9IZhyyE6sGAAAAAGCsGU1uIChhu7v+7ifz1//x66zpLmdWR1u+8GdH54V7Tdl0YLWa/OKLyU8/naxeumH7nHnJPiclB/9JssfRO69wAAAAAADGBEEJdffQstX53/96ex5+ak1aSsV8+LTDcubxew0/uFJOHrk5uf2ryW+/n1T7puwqFJM/+ljyonfUFoUHAAAAAIAtIChhl7BqfU/ed/Wv84N7at0ibzpuTj582mFpbSqNfNCKxcnvfpTcf11y/7W1bUeemZz68WTc5GTtM0nLhKSpZfMXr1aT//f3yYM3JK/9cjLj4O1zUwAAAAAA7PIEJewyqtVq/ummh/OpG+5PtZocOWdyvnDW0Zk9edxzHZj8/J+SGz5Q6zCZMDMZPyNZ+j9J+7TkBWckrROTrlW1IGX3Fww9/ocfrk3plSS7HZz8xY+SlvYdc5MAAAAAAOxSBCXscn78wFN591V3ZcXankwb35JPvfHIvPygGc994KM/S/77PcnTD448pnl8cua/J/u9tBaw/PiTyY8+VtvX2pF0dSZHvy35089tn5sBAAAAAGCXJihhl7T4mbX53/96R+59sjNJ8uZ5e+X/++NDMr61afMH9nYl//OtpFhK9n9F8vidyX3fTUotyVO/TRbdmpRak+P+vBaK3PWvteNO+XAy+6jk66cnqSav/lxyzNt25C0CAAAAALALEJSwy1rfU84l19+fr/7skSTJ3tPa8+k3Hplj9p66dSfs7Uq+fW5y338P2lhIXnlJMu9/1d7e9Inkpv+bFJuSt3wn2fcl23YTAAAAAADs0gQl7PJueWh53nf1r/PEyvUpFpK/fOn+ee8pB6alqTj6k1UqyQPXJ3d/O3ny18nJH0wOPW3D/mo1+fafJ3d/K2mblLz1v5LZL9x+NwMAAAAAwC5FUMLzQuf6nnzou/fkP+98PElyyO4d+cwZR+WgWRO3/8V61idf/9Nk8S9q65a89svJbgfVgpPWjqT0HNN/AQAAAADwvCEo4Xnl+rufzPu/c3eeWdOdllIx5//RgXn7i/fduu6SzelalXzjjcmiWzbdd+Cpyeu+krTugJAGAAAAAICdajS5wXb+m2gYvVMP3z3Xv/cPcvLBM9JdruTj1/028z/z4/zw3qXb90KtE5M/+1bygjOSCTOTpnEb9j1wffKvr0nWrdi+1wQAAAAAYJemo4RdRrVazbfu+H0+cf39Wb66K0nyp0fOzof/9LBMGd+yYy5a7kkevzO58o3J+hXJhFnJi96RHPv2pG0rvku93Um1kjS3bfdSAQAAAADYMqbe4nltdVdv/vHGB/Pln/wulWoyfUJL/v70I3Lq4bN23EWX3J38+5uSlYtr78dNTV7yvuSgP0469kiaWmqLwq99Oul8IikUalN5PXFXsmZ5MuOQ5OmHkp9fnvSuT174Z8neJybLH0jmzEsOOHnH1Q4AAAAAwBCCEsaEXy1ekQuu/nUeXLY6SfLCvSbnuH2m5vSj9sihs3fAz7m3O/mfq5Of/kPy9IND9zW1JYVi0rN29OdtmZD87WMWjAcAAAAA2EkEJYwZ63vK+eyND+aLNz+cSt83tVQs5H+/ZL+8++S5aWsubf+LlnuTX30j+cUXk2cernWIDDZ+Rq2jpNSazDoimTgzWXpvUmxKjv+LpH1qcus/JWuX1zpVetclf/mzZNbh279WAAAAAAA2IShhzFn8zNr8/HdP54Z7l2Zh3yLvsye15bxXHJA3HDMnLU3FHXPhajVZ+0zSsyap9CYTZ49u/ZGv/Uny6E+S0y6rTccFAAAAAMAON5rcYAf97TJsX3OmtucNx87Jl996bC7/s6Mzq6MtT6xcn//vO3fn5Z+6Kd+8bVF6ypXtf+FCIRk/LZm8VzJ1v9Ev0r77kbXnJ+7a/rUBAAAAALDNBCU875x6+O656YKX5eJXH5rdJrbm8RXr8rff/p+cfOnNufr2xVnfU653iRvMfmHt+Ylf1bUMAAAAAACGZ+otntfWdZfzjV88li/c9HCeXtOdJBnXXMoL9pyU3z+7LuVKNe86+YCcedxeKRYLO7/Apx9O/vHo2mLwFz5uQXcAAAAAgJ3AGiU0nLXdvbnilsfybz9/LI+vWLfJ/hftNzWfeN0Lsve08Tu3sEol+cTeSVenBd0BAAAAAHYSQQkNq1qt5u7HO3Pfks7sM218fvP7FfnUDfdnfU8lbc3F/PUfHpS3nbjPjlv8fTj9C7r/6eeTo9+y864LAAAAANCgLOZOwyoUCjliz0l547Fzcvy+U/Pnf7BffvDel+SE/aZlfU8lH7v2vrzi0pvyHztzLZP+Bd2f/NXOuR4AAAAAAFtMRwkNoVqt5j9uX5xLb3ggy1Z1JUmmtDfnjOP2yttfvE9mdLTtuIv/z7eSb5+btE9Pjn17MuOQpG1S0jE7mTQnaZ2w464NAAAAANCATL0FI1jXXc7Xb300X791w1omLaViXnfMnvnfL9kv+0zfAWuYrFqS/OOxSfeq4fe3T0sm75WM3632un1a0j41GTc1GT89mTAzmTCj9tw8bvvXBwAAAAAwxghK4DmUK9XceN/SfPHHv8sdjz2bJCkWklccPDOvP2aPvOLgmdt3HZO1zyQP/CB5aGEtOFn3bNL5eLJ+5ejO0zqpFppMnLUhPJkws9ah0jwuaWpLmttr3SpT901advLi9QAAAAAAuwBBCYzCbY8+ky/c9HD+32+XDWybMbE1b3nR3jnjuDk7dlqudSuSlYuTFYuTtU9veKx7phaurHkqWb00WbU0KXeN/vzN7bUQZcTH5E23NY9LqpWkWq09F4pJsSlpnVjrdmke9HlUq0mhsL0+DQAAAACA7UJQAlvhoWWr8q07Hs+37/x9nupbx6RYSE7cf3r+9KjZOfXwWeloa65PcdVq0tVZC0xWb/RYtTTpXp30rE161tVer/x9rWtlR2iZkIybUrvO+pW1tVcm71V7dMxOSi21cKVQTIqlDa8LxVrYMmWfWjdMqTVpaqmNH3jdWntf3IZunmo1KfckveuT3q7ac7k7mbLvtp0XAAAAAHjeEJTANujureS6u5/M1299bGBariRpaSrm5INn5LSj9sjLDtotbc2lOla5BdavrIUl61fWOlfWr9zMY9D+nnW1gCOFWrdItZpUepKuVUmld+fUXmxOmlqTUvOGEKWprdbtUijVOm7Wr0wqlb7ul3Lfc6UWkmSYP9ZmHp782bdr05YBAAAAAGOaoAS2k8XPrM13f/1Errnr8Ty4bPXA9oltTfnjw3fPaUfNzrz9pqVUbIDpp6rVWjix9ulaANMyvjZV1+plyYpFtSnEOp+ohRWVQcHF4BBj7bPJs4/Uju/tqnV69HbVgpgdpdTaV1NPMnW/5FWXJpP3Tlo7atOINbf3BUMAAAAAwFghKIHtrFqt5t4nO/PdXz2R7/76iTy5cv3AvpkdrfnTI2fnlUfsnsNnT9q+i8A3ikpfkDE4PCl3Jb3dQ597+qbRap9aW1+l2FSbTmtgeq9SbVtz29BpvJ55JPn6acmKx4a/fqklaRpX61hpHlcLgcZN2fBon5qMm7rhuaU9Ax03hWLf8X0dL/3PzeNq5zTdFwAAAADsdIIS2IEqlWp+8cgz+e6vH8/3f/NkOtdvmI6qpVTM0XtPzlnz9s6ph89Kc8lfku8yOp9Irv+7ZOk9SeeTSc+anXPdgRCmbaMQpa02tVhze9I+rbZ+S7VaC2km7VkLaAqFbJgCbaNOnVJLbWxze+3R0v/cv21c3/EAAAAA0HgEJbCTdPWWc/P9T+W/fvVEfvbw8qxYu2EKqdamYvbbbUJef8yeOfekfetYJcOqVGoLvfesS3rX1Z571ta6VrpX1dZ1WfdssvaZ2pooa5+pvV/3TG1stZqk2rcuSnftuP7zlLvrfXc1LROT8dOTto5a50v6OmD6O2F61tXWnik116YiKzXXjis29a0R07dOTLE0qGtn0KO5fcP0ZU1tfce0bDi2qXXTbT1rk+7VtWCofXpt/ZmBbqBS7bmptTat23BBT6Vc+3yL/XUJgwAAAADYlKAE6qBareaxp9fmP+96PFf+YlGWr+5KkpSKhfzm4j/K+NamOlfITlMp94Uw62vBwEAgs35DGNO7Pqn0Jt1rkrXLk67Vtb/0X99ZW++la9WgMKa6Iazof+7trnXFdK/tCx/WbLjWWNDcXpterauz9nmOm5KkmqxeWgun+hWbaqFJqaUW6vQ/ik21z6RrdVJq6gtu2mrBzECo01oLXVYuro3buCunpb127a5VScuEZMKM2rkrvX2PyobX/eFY+7RayNMfmA0OkwamZmvf0FnUPK52XLlnw7kKxb77aupbP6c/DBr0n+tKuS+wK9dCp1Jr7XWld1DnUTWZc3wyVVALAAAANB5BCdRZuVLN4mfW5k1f+nmWdK7PlX8+LyceML3eZdEIKuW+MGZdLXRZu7z23B+4VCu115Vy7S/t2zpqf7m+bkXtL9qr1dr7/nViyj0bpvsaeFRra8r0Bz794U+5a9A6M+s3XWOmt7sWGrSMr4UPa5bXrjUwrVi59pzn/X+Wdh0Td0/e8+taMAQAAADQQEaTG/gn7rADlIqF7DN9fI7bd2r++9dP5LZHnxWUsHMUS7UOg9aJtQ6IHFDvikavZ33S+Xitm6S1o3ZP656tBTQds2tBS6U3KffWApv+boxyd9+jb1vrhFonSH+HT3+QM/BYXzv3pD03dIF0rx3aqVMo1s7RvSpZvawW6hSbNu36KLXUal+zPFm/staNUmzuC4zWbwiv+h+DtyW18/R3wvQHUZW+DpEh+rpLCsXaNVKoTWXW27Whlv7OoyfuSlY9mdz9n8lRZ+6snx4AAADA846gBHag4/aZkv/+9RO5/bFn6l0KPH80tyXT9h+6bco+dSnlee0nn05u/HDy839KjnyT9VwAAAAARlCsdwEwlh2z95QkyV2LVqRcMZ0QsBMdc3bSNC5Z8pvksVvqXQ0AAADALktHCexAB8/qyITWpqzu6s39S1bl0NnW0AF2kvaptU6SO/4lueJPalOBFYobPQpDX/fbZPmy6tbta25PXvyeZN7/1tECAAAA7LIEJbADlYqFvHCvyfnJg8tzx2PPCEqAnevEdyX/c3VtHZNy186//voVyfV/myz+RbL/K2qLypdaao+mlr7wpi+sSaEvTNncc0YxduPzZhRjR3rOKOvdaKywCAAAAHZJghLYwY7de2p+8uDy3Pbos3nLCfvUuxygkUzbP3nfA8m6FbWF6Psfqda6PwZvq1YysFh8Msxf6m/FvgdvSG74YHLPf9Ye1BRKSbGUFJv6XhdrrwcepU3fbxz6bO51siGsec7X2ej1tp5rmOOH3Psow6IRx4/m3CNsH3b8aMbuyDp25LlHGjvCqcfUPe6gOgrFWhDcPG5DIDzc78ZI5x1xzHDHPNeY5/gd2+zv4Gb27ezjnvPYrT1ua2vdlmN39nH1uOaO+t5t9qSbv+aO0joxGb9b0tQ2qJRhfv9G9Xs8eNcWjtsi2/AZ7exrusdd6Hr1uGYd7tE/YAKGISiBHey4fackSa6/e0muuevxnP7CPepcEdBQWsbXHvUwfW4y++jkti8nXX1dLb3dtedyd1LuzZDQpv/1sM8ZxdhBz1syZqTz7ijVclIu1z4DAABgjHuehE/1uKZ73P7X++v7krZJW3nNxiYogR3sRftOy6uO2D3f/58n895v/io//93TOfekfTN35sR6lwaw4+19Qu3xfDVS2LItYU2lXAtLKr2115W+10O29Q59Xa0MWgOmOvLr/pq36PXGx2c7nmuYoGmT9W0GdoyweRTjRxw70imGG78j69hVzl2POkYYPpbusVJOeruS3vW1R38IOuzv00bnGXHMcLWMcJ7N/d6NeK5Ndu46x+2wa27t9bblmjv7uHpcc1f73m2+nG1TTdZ3JmueSio9fZu24PcaYGv/PNja/65tC3900cAK1Wo9fuu2r87OzkyaNCkrV65MR4c1INj1VCrVXPKD+3P5zQ8PbDtqzuS88vBZOXH/6Tl494lpLhXrWCEAAAA7VP9fv2w2BB1p26gutJWHbc1xO/svgN3b9rne8+Av7p8Xn+NWXs+9bZ9r7ezrPV/ubfJetembSTK63EBQAjvRz3/3dP7lZ49k4b1LUxn0mzeuuZQX7DkpR+89JUfvNSVH7zU50ya01q9QAAAAAIDnMUEJ7OKWda7PD+5dmh/euzR3Lno2q9b3bjJmn2ntOXqvKXnh3lPywjmTc+DMiWlp0nUCAAAAAPBcBCXwPFKpVPPwU6tz56Jnc8djz+bORSvy0LLVm4xrKRVz4KwJOXz2pBy2x6QcNrsjh8zqyLgW7XQAAAAAAIMJSuB5buXanty1uBaa3PnYs/nN71ekc5iuk2IhOWDGhBw2uxacHL7HpBw6uyMdbc11qBoAAAAAYNcgKIExplqt5vfPrsvdj6/M3U+szD1PdObux1dm+eruYcfvPa09h8+uhSaHze7IgTMnZvdJbSkUCju5cgAAAACAnU9QAg2gWq1m2aqu3P34huDknic68/iKdcOOb28pZd/p47P/bhOy/24Tst9u4wee25pN3wUAAAAAjB2CEmhgz67prgUnfZ0n9zyxMoueXpveyvC/6oVCssfkcTlgxoQcsNuE7D9jQg6YMSH7Th+faeNbdKEAAAAAAM87ghJgiJ5yJYueWZuHl63O75avycPLVufhp1bn4afWZOW6nhGPG9dcyp5TxmXO1Pba85T2Ie8njWsWpAAAAAAAu5zR5AZNO6kmoI6aS8WBKbcGq1areXpNdx5etjoPPbU6Dy2rhScPL1udJ1auy7qech5ctjoPLls97HkntjZlzxFClDlT2zOh1R8xAAAAAMCuTUcJMKyu3nKeWLE+v392bRY/s672/Oy6gffLV3c95zkmtzdvGqBMac8+08dnr6ntKRV1owAAAAAA25+OEmCbtTbVFn/fd/r4Yfev6y7n8RXDhyi/f3Ztnl3bkxVre7Ji7cr8z+MrNzl+XHMp+0wfn9mT2rL75LbsPmlcZvc/TxqXmZNa09pkkXkAAAAAYMcSlABbZVxLKQfMmJgDZkwcdv+q9T15fMW6DUHKoEDld0+tzrqecu57sjP3Pdk54jWmT2jtC082ClImt2XGxLa0NZcyobUp41oEKgAAAADA1hGUADvExLbmHDyrOQfP2rStrVyp5tGn12TR02vzxMp1eXLF+oHnJ1euy5Mr16ert5Llq7uyfHVXfvP7TTtSButoa8q0Ca1pKRUzsa0pe04Zlz2ntGfO1HHZY3J79pgyLntPbU/RVF8AAAAAwEYEJcBOVyoWhl1cvl+1Ws0za7rz5Mr1eWJFLTgZHKQ8sWJ9lq/uSldvJUnSub43net7B46//bFnNznnyQfPyD+ffdyOuSEAAAAA4HlLUALscgqFQqZNaM20Ca05fI9JI46rVqtZ1dWbpSvX55k13ekpV7NiXXd+P2i9lCdWrMvDT63Ojb9dlnueWJnDZo98PgAAAACg8QhKgOetQqGQjrbmdLQ1b3bcu/79rvz3r5/IFbc8mktef+ROqg4AAAAAeD4o1rsAgB3t7BP3TpL816+eyLNruutcDQAAAACwKxGUAGPe0XtNyWGzO9LVW8lXfvq7VKvVepcEAAAAAOwiBCXAmFcoFHL2ifskSS770cN5w+W35opbHs2PH3gqi59Zm3JFcAIAAAAAjcoaJUBDeN3Re+b3z67LF3/8cG5/7Nnc/tizA/uaS4VMG9+aqeNbMm1CS6aOrz2mjW/J1I22Txvfko625hSLhTreDQAAAACwvRSqY2AOms7OzkyaNCkrV65MR0dHvcsBdmFLVq7PN37xWO5fsiqPLF+Tx55em+5yZVTnKBULmdLeH6TUHpPamzOlvTmTx7VkcntzJre31N73vZ40rjnNJU18AAAAALAzjCY3EJQADa1cqWZp5/o8vbo7T6/pyjNruvPMmu48vaY7z6zue+7b/vSa7qxa37vV15rY2tQXqLQMClCaMrGtORNamzKxrSkTWvsebU2Z2NqcCX3bJrY1pbWpmEJBJwsAAAAAPJfR5Aam3gIaWqlYyOzJ4zJ78rgtGt/dW8mza7vz9Or+QKUrK9b25Nm13Vmxticr1nbn2bU9WbGu7/Wa7nT2hSurunqzqqs3v3923VbV2lQsDAQnQ4KVEYOWpiFBy4S+4KW9uWTqMAAAAADoIygBGIWWpmJmdrRlZkfbFh9TrlSzct2gEKUvVHl2bXc61/VkdVc5q7t6srqrN6vW92Z1V29WD37u7k21mvRWqn1hTM823UOhkExo2RCiDA1TNgQqg4OWTYKX1uaMby2lyXRiAAAAADzPCUoAdrBSsTCwlsnWqFSqWdO9IThZtVGQsuH98GHLwPuu3pQr1VSrG7pbttW45tKmocomwcqg4GW4YKatKa1NpW2uBQAAAAC2hqAEYBdXLBYysa05E9uak0lbf55qtZr1PZWs6uoZIWgZHKz0DBu09I/v7q0kSdb1lLOup5ynVnVt0z22lIqbBC0TWpsyrqWU9uZS2ltKGdfSlHEDr2vP7S2ltDWX0t7SNGT7uOba65aSdV0AAAAA2DxBCUCDKBQKGdcXJsyYuG3n6uotZ01XuS846RkmaNm4o6Vn2GBmbXc5SdJdruSZNbV1X7anUrGQ9r7QZFxfgFILWJqGvN8QvDT1BS8bApfa2GLGNTdt2N63z9RjAAAAAM9/ghIARq21qZTWptJWTyfWr1ypDulWWd3VMxC0rOkLUtZ2l7Ouu9a5Untd276up7Z97cC+2vb1PeX0lKsD599e04wNp6VU3NDFMiR4aRrohGnbqCtmcMfLhpBm0+1tTaUUi7phAAAAAHY0QQkAdVMqFjJpXHMmjWveruftKVcGApa13b1DQpVasNKbdd2V2r7uctb27d/wekNIs75n0HHdvVnbU061lsOku1xJ97pKVq7r2a719+sPTsYN7nIZ0umy+ZBm3n7TtjnMAgAAABjrBCUAjDnNpWImjStu9wAmqa310tVbGRKq9IcuazfqchnofhmybUP3y3BdMet7KgPX6l8DZmtNaW/O1X95Yg6YMWF73DoAAADAmCQoAYBRKBQKaWuuLSI/ZQecv1KpZn1veUjAsnFXzIZOmcqG7peNumIeXrY6j69Yl7f88y/ymTOOytTxLZkyviVT21tM6QUAAAAwiKAEAHYhxWKhb82SbftP9DNruvPGL96ah5atzhlf+vnA9lKxkGnjWzJ9QmsmtjVlfGttfZTxfVN5jW/dsGZKLRAqpq2p9rq1uVjb1tS3vbmU1qbiQHBUEsAAAAAAz0OCEgAYg6aOb8m/nnt8/uZbv8nDy1ZnbU85K9f1pFypZtmqrixb1bXdr9lSKg4EKG3NpUwa15y3n7RPTj9qjxQKQhQAAABg11SoVvuXpN1yl112WT75yU9myZIlOfLII/OP//iPOf7440ccf/XVV+eDH/xgHn300cydOzef+MQn8sd//McD+z/0oQ/lqquuyuLFi9PS0pJjjjkmH/vYxzJv3rwtqqezszOTJk3KypUr09HRMdrbAYCG0Fuu5Ok13XlqVVeeWt2VNV29WdtVzpqBxet7s6ar77m7nK6ectb3VLK+p5yu3trz+t5B23oq6S5XnvO6px42K39y5O7ZZ9r4gQ6WtubaAvQtTcWdcOcAAABAoxlNbjDqoOSb3/xm3vrWt+byyy/PvHnz8pnPfCZXX3117r///syYMWOT8bfcckte8pKXZMGCBfmTP/mTXHnllfnEJz6RO++8M4cffniS5Morr8yMGTOy3377Zd26dfmHf/iHXH311XnooYey2267bdcbBgC2n3Klmq6+8GRdTznr+9ZK6eot56cPPp1//H8Pprcy8v/UaCoWMq65lLaW2nRezaViWkrFtDRteN3cVExLqVB737e9uVRMa1MxzX3b+/e1lGrbWppKfc9Dz9NcKqS1qZimYjFNfceWioU0971vKg16PfBc0BEDAAAAzzM7NCiZN29ejjvuuHz+859PklQqlcyZMyfvete78nd/93ebjD/jjDOyZs2afO973xvY9qIXvShHHXVULr/88s3ewA9/+MOcfPLJz1mToAQAdk13P74yX7/10Ty4bHV+/+y6gYXoN5Od7JKaihtClFJfiNK8UbBS6t9WLKSp1P+6OHBsU6mY5kH7SsVCxrc05aS503PCftPSVNJdAwAAANvLaHKDUa1R0t3dnTvuuCMXXnjhwLZisZhTTjklt95667DH3HrrrTn//POHbJs/f36uueaaEa/xpS99KZMmTcqRRx45mvIAgF3M4XtMyiWvH/rf82q1mp5yNeu6y1nXU3us7e5NV28lPb2V9JSr6S6X091bTU+5ku7eSnrKtUd3uTr0fW9t+q+eciU9vdV0l/veD7e9t/8clfSWa+furVTTO/BcTU+lkuH+CUlvpZreSjXr89xTjY3WF3/8u0xsa8oek8dlSnvLQBdLqS9kKQ2832h7//tSIaVC7X2xUHuUikmxWNteLBT6Xielvu6YUt++Qt+2TY4dfL7+82y8ve/YwduGO7ZYyMC5B78uFLPJtv7XhUJ08QAAALDTjCooWb58ecrlcmbOnDlk+8yZM/Pb3/522GOWLFky7PglS5YM2fa9730vb3rTm7J27drsvvvuWbhwYaZPnz7sObu6utLVtWER2s7OztHcBgBQR4VCIS1NtWmxJqW53uVsolzZEKKU+8KTjYOVnnK1Nq5vX2+5kp6N9vVWaq8H7+stVzeco1LNss71ueHepXlmTXd+u2RVvW99l1IoZCDoKRSySZDSH8YUhoQtGQiCBo8tDjrP4KCnsHFQUxw6dkiQs1HoM2R/ceOgZ8PrWji1cf0bgqbnrn/o/uIm9Y9U46b7B38WxUKSjd4XNnoe7pjCZs678fPAMdk0GBsYG+EYAABQf6MKSnakl7/85fnVr36V5cuX58tf/nLe+MY35he/+MWw654sWLAgH/7wh+tQJQAw1tU6LEo77Xp/f3olDyxdnadWd2XF2u6+kKW64bk/tNl4e2VDmFOuVlOpVFOpZuB1ue99pVp7Xa5WU+1/Xalt799XqVZTqQw6ttp3bGXQ/uqGc244ZqOxfWOqfWP6r9lfx2gmfK1Wk95qNcnzbJ42tlpho5CnP0QZEq4UB4crzx3WDBfIbBx0FYZ733/tvoAnw9TSf/2WplL2nDIusye1pVQsDhtcDR/S1bqw+l8X+j6DQvrOnw21bfo6SYbe3+Bjs9H74gjHDpx30OczcOxwNQ1X36AxtfBt+GMH17TJGCEZAAB1NqqgZPr06SmVSlm6dOmQ7UuXLs2sWbOGPWbWrFlbNH78+PE54IADcsABB+RFL3pR5s6dm3/+538eMs1XvwsvvHDIdF6dnZ2ZM2fOaG4FAGCX0FQq5tDZjbHGWrUvLKkMCk8Gv64ODnA2HlvZ8H7jAKZS2RDElPuOrw46rlKtXbs8JLTZcNzGtVSr1SHnHTqmb39lhHsYFBoNOdeg/f1BU3WY6w4OuDb5rCob1zj089n0HjYcV82mn1WlumH7kFr6Zpgb6XzVTerbMGZwzaP/ftR+fuXau+32veP5Y3AAUxwUvowYIiWD9g8KZDI0iBkaDG16noFrP8c1stF5h4ZAg4OfoefJkGOGCZiG3MfwwdSm137ue9z4vjLs9Te+zxE+q0HX2PAZb+az3Pg8W3CN/vNmo89n4/OMeI3NfFYD24b9mQzz2Q33WRWSSeOas/ukcWlvKY3QdbfhXBsCyr7zFYYPNof7HPvPBQDsPKMKSlpaWnLMMcfkxhtvzOmnn56ktpj7jTfemHe+853DHnPCCSfkxhtvzHvf+96BbQsXLswJJ5yw2WtVKpUh02sN1tramtbW1tGUDgBAnQ38y/34y59GsDXhytAAqLZv2GOyoVtq8JgNwU//2JHPVa0m1QwNzJJNQ7GN6xvyPrX1lhY9szbLOrtGDNbKw4RctdoHve77zGqfXV9tg+431QyMqSZDPtNstL2a/jprB1aHObZaHWH7xuOHGbPjvjN9EZnADAY8V4C4IXzZEEYN7SIbtH+jQGhwIFPYJMzZ6HzZOODZNBws9m0YHFJtEhgNua8N1x7yPCj02vA59I0dMm7T/ZtuH3KWTbaPNLYwzNjNjclW1rQl4zPoc3nucwxf30j3kI1+Blt1viHjN/3feKM/x/Dn26GfyTAFb9vnMPj1dv5MRhg/3PVG/b0bbswWnW8r72GUvwtb8ru49Z/J5u/huf4s2fTcW34Pw/1ZslXn69szb7+paS4Vw+iNeuqt888/P29729ty7LHH5vjjj89nPvOZrFmzJuecc06S5K1vfWv22GOPLFiwIEnynve8Jy996Utz6aWX5lWvelWuuuqq3H777fnSl76UJFmzZk0+9rGP5U//9E+z++67Z/ny5bnsssvy+OOP5w1veMN2vFUAAGBnKRQKKRWSkmBsTKpWhwY5g0OWZPiwp9o3m97gMGZwaLUhDBoc9mx6zpHDm8H7Bo0dobbhzzNoXP/Yje5xuDoHnyebXHvDZ7bxdUY6zyZ1brxv4+sMc55sHIxlmPMM3Ovw5xmoe0uuMdxnN+Tzeo5rDHf+jT+vEe55uCBxuJ/JwOc6zHmG/kyGOU81eWZNd5Z0rk9XT3lDEDzo92G000xuCQEiAKPxmw/9kaBkK406KDnjjDPy1FNP5aKLLsqSJUty1FFH5frrrx9YsH3RokUpFjf8ME488cRceeWV+cAHPpD3v//9mTt3bq655pocfvjhSZJSqZTf/va3ueKKK7J8+fJMmzYtxx13XH7yk5/ksMMO2063CQAAwPbSH4RFEAabGNx9trngrppB4crGAeGg41LNQBfbJh1fIxyXgW0jXHukOgYft4X1bxwOblxHZdBxGdg+NEQc+vkNM3Yzxww5ujr45aAx1U2HVEcxduMd1WE2j1TTSOfb+L6HH18dZtvmx268fei5Nx0z2vON6jPZgnOMdM0RXm4S9G7p+Z7rM9mWz2HouYf5jLfhfM/1mWzL9+65fneGnGN7fyZbUNPW/u6MWN+ov99bPna095Ct/d5twfWLw7XVsEUK1ZG+Xc8jnZ2dmTRpUlauXJmOjsaY4xsAAAAAABjeaHIDfTgAAAAAAEDDEpQAAAAAAAANS1ACAAAAAAA0LEEJAAAAAADQsAQlAAAAAABAwxKUAAAAAAAADUtQAgAAAAAANCxBCQAAAAAA0LAEJQAAAAAAQMMSlAAAAAAAAA1LUAIAAAAAADQsQQkAAAAAANCwBCUAAAAAAEDDEpQAAAAAAAANS1ACAAAAAAA0LEEJAAAAAADQsAQlAAAAAABAwxKUAAAAAAAADUtQAgAAAAAANCxBCQAAAAAA0LAEJQAAAAAAQMNqqncB20O1Wk2SdHZ21rkSAAAAAACg3vrzgv78YHPGRFCyatWqJMmcOXPqXAkAAAAAALCrWLVqVSZNmrTZMYXqlsQpu7hKpZInnngiEydOTKFQqHc5u4TOzs7MmTMnixcvTkdHR73LgZ3K959G5vtPI/P9p5H5/tPIfP9pZL7/NDLff55LtVrNqlWrMnv27BSLm1+FZEx0lBSLxey55571LmOX1NHR4Q8KGpbvP43M959G5vtPI/P9p5H5/tPIfP9pZL7/bM5zdZL0s5g7AAAAAADQsAQlAAAAAABAwxKUjFGtra25+OKL09raWu9SYKfz/aeR+f7TyHz/aWS+/zQy338ame8/jcz3n+1pTCzmDgAAAAAAsDV0lAAAAAAAAA1LUAIAAAAAADQsQQkAAAAAANCwBCUAAAAAAEDDEpSMUZdddln22WeftLW1Zd68efnlL39Z75Jgm/z4xz/Oq1/96syePTuFQiHXXHPNkP3VajUXXXRRdt9994wbNy6nnHJKHnzwwSFjnnnmmZx11lnp6OjI5MmTc+6552b16tU78S5g6yxYsCDHHXdcJk6cmBkzZuT000/P/fffP2TM+vXrc95552XatGmZMGFCXve612Xp0qVDxixatCivetWr0t7enhkzZuSCCy5Ib2/vzrwVGLUvfOELecELXpCOjo50dHTkhBNOyHXXXTew33efRvHxj388hUIh733vewe2+f4zln3oQx9KoVAY8jj44IMH9vv+M9Y9/vjj+bM/+7NMmzYt48aNyxFHHJHbb799YL//D8xYtc8++2zy53+hUMh5552XxJ//7DiCkjHom9/8Zs4///xcfPHFufPOO3PkkUdm/vz5WbZsWb1Lg622Zs2aHHnkkbnsssuG3X/JJZfkc5/7XC6//PL84he/yPjx4zN//vysX79+YMxZZ52Ve+65JwsXLsz3vve9/PjHP87/+l//a2fdAmy1m2++Oeedd15+/vOfZ+HChenp6ckf/dEfZc2aNQNj/uqv/ir//d//nauvvjo333xznnjiibz2ta8d2F8ul/OqV70q3d3dueWWW3LFFVfka1/7Wi666KJ63BJssT333DMf//jHc8cdd+T222/PK17xipx22mm55557kvju0xhuu+22fPGLX8wLXvCCIdt9/xnrDjvssDz55JMDj5/+9KcD+3z/GcueffbZvPjFL05zc3Ouu+663Hvvvbn00kszZcqUgTH+PzBj1W233Tbkz/6FCxcmSd7whjck8ec/O1CVMef444+vnnfeeQPvy+Vydfbs2dUFCxbUsSrYfpJUv/Od7wy8r1Qq1VmzZlU/+clPDmxbsWJFtbW1tfrv//7v1Wq1Wr333nurSaq33XbbwJjrrruuWigUqo8//vhOqx22h2XLllWTVG+++eZqtVr7vjc3N1evvvrqgTH33XdfNUn11ltvrVar1eq1115bLRaL1SVLlgyM+cIXvlDt6OiodnV17dwbgG00ZcqU6le+8hXffRrCqlWrqnPnzq0uXLiw+tKXvrT6nve8p1qt+rOfse/iiy+uHnnkkcPu8/1nrPvbv/3b6kknnTTifv8fmEbynve8p7r//vtXK5WKP//ZoXSUjDHd3d254447csoppwxsKxaLOeWUU3LrrbfWsTLYcR555JEsWbJkyPd+0qRJmTdv3sD3/tZbb83kyZNz7LHHDow55ZRTUiwW84tf/GKn1wzbYuXKlUmSqVOnJknuuOOO9PT0DPkdOPjgg7PXXnsN+R044ogjMnPmzIEx8+fPT2dn58C/zIddXblczlVXXZU1a9bkhBNO8N2nIZx33nl51ateNeR7nvizn8bw4IMPZvbs2dlvv/1y1llnZdGiRUl8/xn7vvvd7+bYY4/NG97whsyYMSMvfOEL8+Uvf3lgv/8PTKPo7u7Ov/3bv+Xtb397CoWCP//ZoQQlY8zy/7+9+wtpsv3jOP6N1pYitmK6rUIRsj8yClMaQ6KDRSQdREciHVgRoSUZeJAddFh2FFQHRhAaFEgEFnWgmdNBQUa2oRKsrGUdOMdTWIuk0vv7HPzohmU81O/3W+vZ/X7BYF7Xzc33hs914cWXbX/9JfPz82mbgYiI2+2WRCKRpaqAzPqW7X/KfSKRkOLi4rR5m80mK1asYG3gX8UwDDl27JjU1NSIz+cTkf/k2263i9PpTLv2+zXwozXybQ74k42NjUlBQYE4HA5pbGyUnp4eqaioIPvIed3d3fLkyRNpb29fMEf+kev8fr90dXVJb2+vdHR0SDwel61bt0oqlSL/yHkvX76Ujo4OKS8vl76+PmlqapKjR4/KlStXRIQzMKzj5s2bMjMzI/v27RMR/v9BZtmyXQAAAPh5R44ckfHx8bTv6AZy3bp16yQajcr79+/lxo0b0tDQIOFwONtlARn15s0baWlpkf7+flm6dGm2ywF+u9raWvP9xo0bxe/3S2lpqVy/fl3y8vKyWBmQeYZhSHV1tZw+fVpERCorK2V8fFwuXrwoDQ0NWa4O+H0uX74stbW1snLlymyXAgvgEyU5xuVyyeLFi2V6ejptfHp6WjweT5aqAjLrW7b/Kfcej0eSyWTa/NzcnLx79461gX+N5uZmuXPnjgwODsrq1avNcY/HI1++fJGZmZm0679fAz9aI9/mgD+Z3W6XNWvWSFVVlbS3t8umTZvk3LlzZB85bWRkRJLJpGzevFlsNpvYbDYJh8Ny/vx5sdls4na7yT8sxel0ytq1a2ViYoL9HznP6/VKRUVF2tiGDRvMr5/jDAwrmJyclHv37snBgwfNMfZ/ZBKNkhxjt9ulqqpKBgYGzDHDMGRgYEACgUAWKwMyp6ysTDweT1ruP3z4IMPDw2buA4GAzMzMyMjIiHlNKBQSwzDE7/f/9pqBX6Gq0tzcLD09PRIKhaSsrCxtvqqqSpYsWZK2BmKxmLx+/TptDYyNjaUdlvr7+6WwsHDBIQz40xmGIZ8/fyb7yGnBYFDGxsYkGo2ar+rqatm7d6/5nvzDSj5+/CgvXrwQr9fL/o+cV1NTI7FYLG3s2bNnUlpaKiKcgWENnZ2dUlxcLLt27TLH2P+RUdn+NXn8/3V3d6vD4dCuri59+vSpHjp0SJ1OpyYSiWyXBvzXUqmURiIRjUQiKiJ69uxZjUQiOjk5qaqqZ86cUafTqbdu3dLR0VHdvXu3lpWV6ezsrHmPnTt3amVlpQ4PD+v9+/e1vLxc6+vrs/VIwE9ramrSZcuW6dDQkE5NTZmvT58+mdc0NjZqSUmJhkIhffz4sQYCAQ0EAub83Nyc+nw+3bFjh0ajUe3t7dWioiI9ceJENh4J+GltbW0aDoc1Ho/r6OiotrW16aJFi/Tu3buqSvZhLdu2bdOWlhbzb/KPXNba2qpDQ0Maj8f1wYMHun37dnW5XJpMJlWV/CO3PXr0SG02m546dUqfP3+u165d0/z8fL169ap5DWdg5LL5+XktKSnR48ePL5hj/0em0CjJURcuXNCSkhK12+26ZcsWffjwYbZLAv4ng4ODKiILXg0NDaqqahiGnjx5Ut1utzocDg0GgxqLxdLu8fbtW62vr9eCggItLCzU/fv3ayqVysLTAL/mR9kXEe3s7DSvmZ2d1cOHD+vy5cs1Pz9f9+zZo1NTU2n3efXqldbW1mpeXp66XC5tbW3Vr1+//uanAX7NgQMHtLS0VO12uxYVFWkwGDSbJKpkH9byfaOE/COX1dXVqdfrVbvdrqtWrdK6ujqdmJgw58k/ct3t27fV5/Opw+HQ9evX66VLl9LmOQMjl/X19amILMi0Kvs/MmeRqmpWPsoCAAAAAAAAAACQZfxGCQAAAAAAAAAAsCwaJQAAAAAAAAAAwLJolAAAAAAAAAAAAMuiUQIAAAAAAAAAACyLRgkAAAAAAAAAALAsGiUAAAAAAAAAAMCyaJQAAAAAAAAAAADLolECAAAAAAAAAAAsi0YJAAAAAAAAAACwLBolAAAAAAAAAADAsmiUAAAAAAAAAAAAy6JRAgAAAAAAAAAALOtv5F4d4mE3cnoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1460, 32, 48, 6, 1), y_hat_i: (4, 32, 48, 6, 1), y_i: (4, 32, 48, 6, 1), batch.x: torch.Size([128, 48, 6, 6]), y: (1460, 32, 48, 6, 1)\n",
      "RMSE for t2m: 1.6852784260272506; MAE for t2m: 1.2613711623156734;\n",
      "RMSE for sp: 1.5742302228942904; MAE for sp: 1.190911801505125;\n",
      "RMSE for tcc: 0.28940308896069983; MAE for tcc: 0.19285198653125665;\n",
      "RMSE for u10: 1.2358987498606424; MAE for u10: 0.9178780496877932;\n",
      "RMSE for v10: 1.2060066846754747; MAE for v10: 0.8830313647548902;\n",
      "RMSE for tp: 0.2906925234959438; MAE for tp: 0.07954716774903182;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 1)\n",
      "182\n",
      "y_hat: (1460, 32, 48, 6, 1), y_hat_i: (4, 32, 48, 6, 1), y_i: (4, 32, 48, 6, 1), batch.x: torch.Size([128, 48, 6, 6]), y: (1460, 32, 48, 6, 1)\n",
      "RMSE for t2m: 1.6852784260272506; MAE for t2m: 1.2613711623156734;\n",
      "RMSE for sp: 1.5742302228942904; MAE for sp: 1.190911801505125;\n",
      "RMSE for tcc: 0.2886209530987951; MAE for tcc: 0.19156196929890384;\n",
      "RMSE for u10: 1.2358987498606424; MAE for u10: 0.9178780496877932;\n",
      "RMSE for v10: 1.2060066846754747; MAE for v10: 0.8830313647548902;\n",
      "RMSE for tp: 0.2906925234959438; MAE for tp: 0.07954716774903182;\n",
      "Epoch 1/1000, Train Loss: 0.07988, lr: 0.001------------------------------------| 9.1% Complete\n",
      "Val Loss: 0.06758\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.06446, lr: 0.001\n",
      "Val Loss: 0.06100\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.05825, lr: 0.001\n",
      "Val Loss: 0.05730\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.05321, lr: 0.001\n",
      "Val Loss: 0.05283\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.05089, lr: 0.001\n",
      "Val Loss: 0.05288\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.05013, lr: 0.001\n",
      "Val Loss: 0.05206\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.04955, lr: 0.001\n",
      "Val Loss: 0.05131\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.04908, lr: 0.001\n",
      "Val Loss: 0.05073\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.04878, lr: 0.001\n",
      "Val Loss: 0.05025\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.04844, lr: 0.001\n",
      "Val Loss: 0.04997\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.04821, lr: 0.001\n",
      "Val Loss: 0.04972\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.04802, lr: 0.001\n",
      "Val Loss: 0.04949\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.04785, lr: 0.001\n",
      "Val Loss: 0.04929\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.04767, lr: 0.001\n",
      "Val Loss: 0.04906\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.04738, lr: 0.001\n",
      "Val Loss: 0.04848\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.04632, lr: 0.001\n",
      "Val Loss: 0.04686\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.04542, lr: 0.001\n",
      "Val Loss: 0.04661\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.04506, lr: 0.001\n",
      "Val Loss: 0.04636\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.04485, lr: 0.001\n",
      "Val Loss: 0.04617\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.04467, lr: 0.001\n",
      "Val Loss: 0.04607\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.04454, lr: 0.001\n",
      "Val Loss: 0.04596\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.04442, lr: 0.001\n",
      "Val Loss: 0.04588\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.04432, lr: 0.001\n",
      "Val Loss: 0.04583\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.04423, lr: 0.001\n",
      "Val Loss: 0.04579\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.04415, lr: 0.001\n",
      "Val Loss: 0.04574\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.04408, lr: 0.001\n",
      "Val Loss: 0.04568\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.04400, lr: 0.001\n",
      "Val Loss: 0.04561\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.04393, lr: 0.001\n",
      "Val Loss: 0.04553\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.04387, lr: 0.001\n",
      "Val Loss: 0.04545\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.04381, lr: 0.001\n",
      "Val Loss: 0.04537\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.04376, lr: 0.001\n",
      "Val Loss: 0.04531\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.04370, lr: 0.001\n",
      "Val Loss: 0.04524\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.04365, lr: 0.001\n",
      "Val Loss: 0.04513\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.04360, lr: 0.001\n",
      "Val Loss: 0.04507\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.04354, lr: 0.001\n",
      "Val Loss: 0.04496\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.04350, lr: 0.001\n",
      "Val Loss: 0.04490\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.04346, lr: 0.001\n",
      "Val Loss: 0.04484\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.04341, lr: 0.001\n",
      "Val Loss: 0.04479\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.04337, lr: 0.001\n",
      "Val Loss: 0.04473\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.04333, lr: 0.001\n",
      "Val Loss: 0.04469\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.04329, lr: 0.001\n",
      "Val Loss: 0.04465\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.04325, lr: 0.001\n",
      "Val Loss: 0.04462\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.04321, lr: 0.001\n",
      "Val Loss: 0.04456\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.04317, lr: 0.001\n",
      "Val Loss: 0.04453\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.04313, lr: 0.001\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.04310, lr: 0.001\n",
      "Val Loss: 0.04446\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.04305, lr: 0.001\n",
      "Val Loss: 0.04443\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.04302, lr: 0.001\n",
      "Val Loss: 0.04439\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.04297, lr: 0.001\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.04293, lr: 0.001\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.04289, lr: 0.001\n",
      "Val Loss: 0.04434\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.04285, lr: 0.001\n",
      "Val Loss: 0.04431\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.04280, lr: 0.001\n",
      "Val Loss: 0.04427\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.04275, lr: 0.001\n",
      "Val Loss: 0.04422\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.04268, lr: 0.001\n",
      "Val Loss: 0.04417\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.04257, lr: 0.001\n",
      "Val Loss: 0.04407\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.04248, lr: 0.001\n",
      "Val Loss: 0.04389\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.04236, lr: 0.001\n",
      "Val Loss: 0.04374\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.04225, lr: 0.001\n",
      "Val Loss: 0.04360\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.04216, lr: 0.001\n",
      "Val Loss: 0.04353\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.04208, lr: 0.001\n",
      "Val Loss: 0.04350\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.04200, lr: 0.001\n",
      "Val Loss: 0.04346\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.04194, lr: 0.001\n",
      "Val Loss: 0.04341\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.04189, lr: 0.001\n",
      "Val Loss: 0.04338\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.04184, lr: 0.001\n",
      "Val Loss: 0.04334\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.04179, lr: 0.001\n",
      "Val Loss: 0.04329\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.04175, lr: 0.001\n",
      "Val Loss: 0.04326\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.04171, lr: 0.001\n",
      "Val Loss: 0.04324\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.04167, lr: 0.001\n",
      "Val Loss: 0.04323\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.04163, lr: 0.001\n",
      "Val Loss: 0.04321\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.04160, lr: 0.001\n",
      "Val Loss: 0.04318\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.04157, lr: 0.001\n",
      "Val Loss: 0.04321\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.04155, lr: 0.001\n",
      "Val Loss: 0.04318\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.04153, lr: 0.001\n",
      "Val Loss: 0.04316\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.04150, lr: 0.001\n",
      "Val Loss: 0.04318\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.04148, lr: 0.001\n",
      "Val Loss: 0.04318\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.04146, lr: 0.001\n",
      "Val Loss: 0.04316\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.04142, lr: 0.001\n",
      "Val Loss: 0.04316\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.04141, lr: 0.001\n",
      "Val Loss: 0.04312\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.04138, lr: 0.001\n",
      "Val Loss: 0.04314\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.04137, lr: 0.001\n",
      "Val Loss: 0.04310\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.04134, lr: 0.001\n",
      "Val Loss: 0.04311\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.04132, lr: 0.001\n",
      "Val Loss: 0.04309\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.04130, lr: 0.001\n",
      "Val Loss: 0.04307\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.04128, lr: 0.001\n",
      "Val Loss: 0.04308\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.04126, lr: 0.001\n",
      "Val Loss: 0.04308\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.04124, lr: 0.001\n",
      "Val Loss: 0.04307\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.04123, lr: 0.001\n",
      "Val Loss: 0.04304\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.04121, lr: 0.001\n",
      "Val Loss: 0.04305\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.04119, lr: 0.001\n",
      "Val Loss: 0.04305\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.04117, lr: 0.001\n",
      "Val Loss: 0.04305\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.04116, lr: 0.001\n",
      "Val Loss: 0.04305\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.04114, lr: 0.001\n",
      "Val Loss: 0.04305\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.04112, lr: 0.001\n",
      "Val Loss: 0.04304\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.04111, lr: 0.001\n",
      "Val Loss: 0.04303\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.04109, lr: 0.001\n",
      "Val Loss: 0.04303\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.04108, lr: 0.001\n",
      "Val Loss: 0.04301\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.04107, lr: 0.001\n",
      "Val Loss: 0.04303\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.04105, lr: 0.001\n",
      "Val Loss: 0.04303\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.04104, lr: 0.001\n",
      "Val Loss: 0.04302\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.04102, lr: 0.001\n",
      "Val Loss: 0.04301\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.04101, lr: 0.001\n",
      "Val Loss: 0.04303\n",
      "---------\n",
      "Epoch 103/1000, Train Loss: 0.04099, lr: 0.001\n",
      "Val Loss: 0.04303\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.04098, lr: 0.001\n",
      "Val Loss: 0.04304\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.04097, lr: 0.001\n",
      "Val Loss: 0.04302\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.04095, lr: 0.001\n",
      "Val Loss: 0.04301\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.04093, lr: 0.001\n",
      "Val Loss: 0.04300\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.04093, lr: 0.001\n",
      "Val Loss: 0.04300\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.04092, lr: 0.001\n",
      "Val Loss: 0.04303\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.04091, lr: 0.001\n",
      "Val Loss: 0.04302\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.04090, lr: 0.001\n",
      "Val Loss: 0.04302\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.04089, lr: 0.001\n",
      "Val Loss: 0.04302\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.04088, lr: 0.001\n",
      "Val Loss: 0.04302\n",
      "---------\n",
      "Epoch 114/1000, Train Loss: 0.04086, lr: 0.001\n",
      "Val Loss: 0.04300\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 115/1000, Train Loss: 0.04028, lr: 0.0005\n",
      "Val Loss: 0.04240\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.04024, lr: 0.0005\n",
      "Val Loss: 0.04238\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.04021, lr: 0.0005\n",
      "Val Loss: 0.04238\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.04020, lr: 0.0005\n",
      "Val Loss: 0.04239\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.04018, lr: 0.0005\n",
      "Val Loss: 0.04240\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.04017, lr: 0.0005\n",
      "Val Loss: 0.04241\n",
      "---------\n",
      "Epoch 121/1000, Train Loss: 0.04016, lr: 0.0005\n",
      "Val Loss: 0.04242\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.04015, lr: 0.0005\n",
      "Val Loss: 0.04243\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.04014, lr: 0.0005\n",
      "Val Loss: 0.04243\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 124/1000, Train Loss: 0.03968, lr: 0.00025\n",
      "Val Loss: 0.04129\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.03963, lr: 0.00025\n",
      "Val Loss: 0.04129\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.03962, lr: 0.00025\n",
      "Val Loss: 0.04129\n",
      "---------\n",
      "Epoch 127/1000, Train Loss: 0.03961, lr: 0.00025\n",
      "Val Loss: 0.04129\n",
      "---------\n",
      "Epoch 128/1000, Train Loss: 0.03961, lr: 0.00025\n",
      "Val Loss: 0.04129\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.03960, lr: 0.00025\n",
      "Val Loss: 0.04129\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.03960, lr: 0.00025\n",
      "Val Loss: 0.04129\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.03960, lr: 0.00025\n",
      "Val Loss: 0.04129\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.03959, lr: 0.00025\n",
      "Val Loss: 0.04129\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 133/1000, Train Loss: 0.03934, lr: 0.000125\n",
      "Val Loss: 0.04099\n",
      "---------\n",
      "Epoch 134/1000, Train Loss: 0.03930, lr: 0.000125\n",
      "Val Loss: 0.04098\n",
      "---------\n",
      "Epoch 135/1000, Train Loss: 0.03930, lr: 0.000125\n",
      "Val Loss: 0.04098\n",
      "---------\n",
      "Epoch 136/1000, Train Loss: 0.03930, lr: 0.000125\n",
      "Val Loss: 0.04097\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.03929, lr: 0.000125\n",
      "Val Loss: 0.04097\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.03929, lr: 0.000125\n",
      "Val Loss: 0.04097\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.03929, lr: 0.000125\n",
      "Val Loss: 0.04097\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.03929, lr: 0.000125\n",
      "Val Loss: 0.04096\n",
      "---------\n",
      "Epoch 141/1000, Train Loss: 0.03928, lr: 0.000125\n",
      "Val Loss: 0.04096\n",
      "---------\n",
      "Epoch 142/1000, Train Loss: 0.03928, lr: 0.000125\n",
      "Val Loss: 0.04096\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.03928, lr: 0.000125\n",
      "Val Loss: 0.04096\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.03928, lr: 0.000125\n",
      "Val Loss: 0.04095\n",
      "---------\n",
      "Epoch 145/1000, Train Loss: 0.03927, lr: 0.000125\n",
      "Val Loss: 0.04095\n",
      "---------\n",
      "Epoch 146/1000, Train Loss: 0.03927, lr: 0.000125\n",
      "Val Loss: 0.04095\n",
      "---------\n",
      "Epoch 147/1000, Train Loss: 0.03927, lr: 0.000125\n",
      "Val Loss: 0.04095\n",
      "---------\n",
      "Epoch 148/1000, Train Loss: 0.03926, lr: 0.000125\n",
      "Val Loss: 0.04095\n",
      "---------\n",
      "Epoch 149/1000, Train Loss: 0.03926, lr: 0.000125\n",
      "Val Loss: 0.04094\n",
      "---------\n",
      "Epoch 150/1000, Train Loss: 0.03926, lr: 0.000125\n",
      "Val Loss: 0.04094\n",
      "---------\n",
      "Epoch 151/1000, Train Loss: 0.03926, lr: 0.000125\n",
      "Val Loss: 0.04094\n",
      "---------\n",
      "Epoch 152/1000, Train Loss: 0.03925, lr: 0.000125\n",
      "Val Loss: 0.04094\n",
      "---------\n",
      "Epoch 153/1000, Train Loss: 0.03925, lr: 0.000125\n",
      "Val Loss: 0.04094\n",
      "---------\n",
      "Epoch 154/1000, Train Loss: 0.03925, lr: 0.000125\n",
      "Val Loss: 0.04093\n",
      "---------\n",
      "Epoch 155/1000, Train Loss: 0.03925, lr: 0.000125\n",
      "Val Loss: 0.04093\n",
      "---------\n",
      "Epoch 156/1000, Train Loss: 0.03924, lr: 0.000125\n",
      "Val Loss: 0.04093\n",
      "---------\n",
      "Epoch 157/1000, Train Loss: 0.03924, lr: 0.000125\n",
      "Val Loss: 0.04093\n",
      "---------\n",
      "Epoch 158/1000, Train Loss: 0.03924, lr: 0.000125\n",
      "Val Loss: 0.04093\n",
      "---------\n",
      "Epoch 159/1000, Train Loss: 0.03923, lr: 0.000125\n",
      "Val Loss: 0.04093\n",
      "---------\n",
      "Epoch 160/1000, Train Loss: 0.03923, lr: 0.000125\n",
      "Val Loss: 0.04093\n",
      "---------\n",
      "Epoch 161/1000, Train Loss: 0.03923, lr: 0.000125\n",
      "Val Loss: 0.04092\n",
      "---------\n",
      "Epoch 162/1000, Train Loss: 0.03923, lr: 0.000125\n",
      "Val Loss: 0.04092\n",
      "---------\n",
      "Epoch 163/1000, Train Loss: 0.03922, lr: 0.000125\n",
      "Val Loss: 0.04092\n",
      "---------\n",
      "Epoch 164/1000, Train Loss: 0.03922, lr: 0.000125\n",
      "Val Loss: 0.04092\n",
      "---------\n",
      "Epoch 165/1000, Train Loss: 0.03922, lr: 0.000125\n",
      "Val Loss: 0.04092\n",
      "---------\n",
      "Epoch 166/1000, Train Loss: 0.03922, lr: 0.000125\n",
      "Val Loss: 0.04092\n",
      "---------\n",
      "Epoch 167/1000, Train Loss: 0.03921, lr: 0.000125\n",
      "Val Loss: 0.04092\n",
      "---------\n",
      "Epoch 168/1000, Train Loss: 0.03921, lr: 0.000125\n",
      "Val Loss: 0.04091\n",
      "---------\n",
      "Epoch 169/1000, Train Loss: 0.03921, lr: 0.000125\n",
      "Val Loss: 0.04091\n",
      "---------\n",
      "Epoch 170/1000, Train Loss: 0.03921, lr: 0.000125\n",
      "Val Loss: 0.04091\n",
      "---------\n",
      "Epoch 171/1000, Train Loss: 0.03921, lr: 0.000125\n",
      "Val Loss: 0.04091\n",
      "---------\n",
      "Epoch 172/1000, Train Loss: 0.03920, lr: 0.000125\n",
      "Val Loss: 0.04091\n",
      "---------\n",
      "Epoch 173/1000, Train Loss: 0.03920, lr: 0.000125\n",
      "Val Loss: 0.04091\n",
      "---------\n",
      "Epoch 174/1000, Train Loss: 0.03920, lr: 0.000125\n",
      "Val Loss: 0.04091\n",
      "---------\n",
      "Epoch 175/1000, Train Loss: 0.03920, lr: 0.000125\n",
      "Val Loss: 0.04090\n",
      "---------\n",
      "Epoch 176/1000, Train Loss: 0.03919, lr: 0.000125\n",
      "Val Loss: 0.04090\n",
      "---------\n",
      "Epoch 177/1000, Train Loss: 0.03919, lr: 0.000125\n",
      "Val Loss: 0.04090\n",
      "---------\n",
      "Epoch 178/1000, Train Loss: 0.03919, lr: 0.000125\n",
      "Val Loss: 0.04090\n",
      "---------\n",
      "Epoch 179/1000, Train Loss: 0.03919, lr: 0.000125\n",
      "Val Loss: 0.04090\n",
      "---------\n",
      "Epoch 180/1000, Train Loss: 0.03919, lr: 0.000125\n",
      "Val Loss: 0.04090\n",
      "---------\n",
      "Epoch 181/1000, Train Loss: 0.03918, lr: 0.000125\n",
      "Val Loss: 0.04090\n",
      "---------\n",
      "Epoch 182/1000, Train Loss: 0.03918, lr: 0.000125\n",
      "Val Loss: 0.04090\n",
      "---------\n",
      "Epoch 183/1000, Train Loss: 0.03918, lr: 0.000125\n",
      "Val Loss: 0.04089\n",
      "---------\n",
      "Epoch 184/1000, Train Loss: 0.03918, lr: 0.000125\n",
      "Val Loss: 0.04089\n",
      "---------\n",
      "Epoch 185/1000, Train Loss: 0.03917, lr: 0.000125\n",
      "Val Loss: 0.04089\n",
      "---------\n",
      "Epoch 186/1000, Train Loss: 0.03917, lr: 0.000125\n",
      "Val Loss: 0.04089\n",
      "---------\n",
      "Epoch 187/1000, Train Loss: 0.03917, lr: 0.000125\n",
      "Val Loss: 0.04089\n",
      "---------\n",
      "Epoch 188/1000, Train Loss: 0.03917, lr: 0.000125\n",
      "Val Loss: 0.04089\n",
      "---------\n",
      "Epoch 189/1000, Train Loss: 0.03917, lr: 0.000125\n",
      "Val Loss: 0.04089\n",
      "---------\n",
      "Epoch 190/1000, Train Loss: 0.03916, lr: 0.000125\n",
      "Val Loss: 0.04088\n",
      "---------\n",
      "Epoch 191/1000, Train Loss: 0.03916, lr: 0.000125\n",
      "Val Loss: 0.04088\n",
      "---------\n",
      "Epoch 192/1000, Train Loss: 0.03916, lr: 0.000125\n",
      "Val Loss: 0.04088\n",
      "---------\n",
      "Epoch 193/1000, Train Loss: 0.03916, lr: 0.000125\n",
      "Val Loss: 0.04088\n",
      "---------\n",
      "Epoch 194/1000, Train Loss: 0.03916, lr: 0.000125\n",
      "Val Loss: 0.04088\n",
      "---------\n",
      "Epoch 195/1000, Train Loss: 0.03915, lr: 0.000125\n",
      "Val Loss: 0.04088\n",
      "---------\n",
      "Epoch 196/1000, Train Loss: 0.03915, lr: 0.000125\n",
      "Val Loss: 0.04088\n",
      "---------\n",
      "Epoch 197/1000, Train Loss: 0.03915, lr: 0.000125\n",
      "Val Loss: 0.04088\n",
      "---------\n",
      "Epoch 198/1000, Train Loss: 0.03915, lr: 0.000125\n",
      "Val Loss: 0.04088\n",
      "---------\n",
      "Epoch 199/1000, Train Loss: 0.03915, lr: 0.000125\n",
      "Val Loss: 0.04087\n",
      "---------\n",
      "Epoch 200/1000, Train Loss: 0.03914, lr: 0.000125\n",
      "Val Loss: 0.04087\n",
      "---------\n",
      "Epoch 201/1000, Train Loss: 0.03914, lr: 0.000125\n",
      "Val Loss: 0.04087\n",
      "---------\n",
      "Epoch 202/1000, Train Loss: 0.03914, lr: 0.000125\n",
      "Val Loss: 0.04087\n",
      "---------\n",
      "Epoch 203/1000, Train Loss: 0.03914, lr: 0.000125\n",
      "Val Loss: 0.04087\n",
      "---------\n",
      "Epoch 204/1000, Train Loss: 0.03914, lr: 0.000125\n",
      "Val Loss: 0.04087\n",
      "---------\n",
      "Epoch 205/1000, Train Loss: 0.03913, lr: 0.000125\n",
      "Val Loss: 0.04087\n",
      "---------\n",
      "Epoch 206/1000, Train Loss: 0.03913, lr: 0.000125\n",
      "Val Loss: 0.04087\n",
      "---------\n",
      "Epoch 207/1000, Train Loss: 0.03913, lr: 0.000125\n",
      "Val Loss: 0.04087\n",
      "---------\n",
      "Epoch 208/1000, Train Loss: 0.03913, lr: 0.000125\n",
      "Val Loss: 0.04087\n",
      "---------\n",
      "Epoch 209/1000, Train Loss: 0.03913, lr: 0.000125\n",
      "Val Loss: 0.04087\n",
      "---------\n",
      "Epoch 210/1000, Train Loss: 0.03913, lr: 0.000125\n",
      "Val Loss: 0.04086\n",
      "---------\n",
      "Epoch 211/1000, Train Loss: 0.03912, lr: 0.000125\n",
      "Val Loss: 0.04086\n",
      "---------\n",
      "Epoch 212/1000, Train Loss: 0.03912, lr: 0.000125\n",
      "Val Loss: 0.04086\n",
      "---------\n",
      "Epoch 213/1000, Train Loss: 0.03912, lr: 0.000125\n",
      "Val Loss: 0.04086\n",
      "---------\n",
      "Epoch 214/1000, Train Loss: 0.03912, lr: 0.000125\n",
      "Val Loss: 0.04086\n",
      "---------\n",
      "Epoch 215/1000, Train Loss: 0.03912, lr: 0.000125\n",
      "Val Loss: 0.04086\n",
      "---------\n",
      "Epoch 216/1000, Train Loss: 0.03911, lr: 0.000125\n",
      "Val Loss: 0.04086\n",
      "---------\n",
      "Epoch 217/1000, Train Loss: 0.03911, lr: 0.000125\n",
      "Val Loss: 0.04086\n",
      "---------\n",
      "Epoch 218/1000, Train Loss: 0.03911, lr: 0.000125\n",
      "Val Loss: 0.04086\n",
      "---------\n",
      "Epoch 219/1000, Train Loss: 0.03911, lr: 0.000125\n",
      "Val Loss: 0.04086\n",
      "---------\n",
      "Epoch 220/1000, Train Loss: 0.03911, lr: 0.000125\n",
      "Val Loss: 0.04085\n",
      "---------\n",
      "Epoch 221/1000, Train Loss: 0.03911, lr: 0.000125\n",
      "Val Loss: 0.04085\n",
      "---------\n",
      "Epoch 222/1000, Train Loss: 0.03910, lr: 0.000125\n",
      "Val Loss: 0.04085\n",
      "---------\n",
      "Epoch 223/1000, Train Loss: 0.03910, lr: 0.000125\n",
      "Val Loss: 0.04085\n",
      "---------\n",
      "Epoch 224/1000, Train Loss: 0.03910, lr: 0.000125\n",
      "Val Loss: 0.04085\n",
      "---------\n",
      "Epoch 225/1000, Train Loss: 0.03910, lr: 0.000125\n",
      "Val Loss: 0.04085\n",
      "---------\n",
      "Epoch 226/1000, Train Loss: 0.03910, lr: 0.000125\n",
      "Val Loss: 0.04085\n",
      "---------\n",
      "Epoch 227/1000, Train Loss: 0.03910, lr: 0.000125\n",
      "Val Loss: 0.04085\n",
      "---------\n",
      "Epoch 228/1000, Train Loss: 0.03909, lr: 0.000125\n",
      "Val Loss: 0.04085\n",
      "---------\n",
      "Epoch 229/1000, Train Loss: 0.03909, lr: 0.000125\n",
      "Val Loss: 0.04085\n",
      "---------\n",
      "Epoch 230/1000, Train Loss: 0.03909, lr: 0.000125\n",
      "Val Loss: 0.04085\n",
      "---------\n",
      "Epoch 231/1000, Train Loss: 0.03909, lr: 0.000125\n",
      "Val Loss: 0.04084\n",
      "---------\n",
      "Epoch 232/1000, Train Loss: 0.03909, lr: 0.000125\n",
      "Val Loss: 0.04084\n",
      "---------\n",
      "Epoch 233/1000, Train Loss: 0.03909, lr: 0.000125\n",
      "Val Loss: 0.04084\n",
      "---------\n",
      "Epoch 234/1000, Train Loss: 0.03908, lr: 0.000125\n",
      "Val Loss: 0.04084\n",
      "---------\n",
      "Epoch 235/1000, Train Loss: 0.03908, lr: 0.000125\n",
      "Val Loss: 0.04084\n",
      "---------\n",
      "Epoch 236/1000, Train Loss: 0.03908, lr: 0.000125\n",
      "Val Loss: 0.04084\n",
      "---------\n",
      "Epoch 237/1000, Train Loss: 0.03908, lr: 0.000125\n",
      "Val Loss: 0.04084\n",
      "---------\n",
      "Epoch 238/1000, Train Loss: 0.03908, lr: 0.000125\n",
      "Val Loss: 0.04084\n",
      "---------\n",
      "Epoch 239/1000, Train Loss: 0.03908, lr: 0.000125\n",
      "Val Loss: 0.04084\n",
      "---------\n",
      "Epoch 240/1000, Train Loss: 0.03907, lr: 0.000125\n",
      "Val Loss: 0.04084\n",
      "---------\n",
      "Epoch 241/1000, Train Loss: 0.03907, lr: 0.000125\n",
      "Val Loss: 0.04084\n",
      "---------\n",
      "Epoch 242/1000, Train Loss: 0.03907, lr: 0.000125\n",
      "Val Loss: 0.04084\n",
      "---------\n",
      "Epoch 243/1000, Train Loss: 0.03907, lr: 0.000125\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 244/1000, Train Loss: 0.03907, lr: 0.000125\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 245/1000, Train Loss: 0.03907, lr: 0.000125\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 246/1000, Train Loss: 0.03907, lr: 0.000125\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 247/1000, Train Loss: 0.03906, lr: 0.000125\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 248/1000, Train Loss: 0.03906, lr: 0.000125\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 249/1000, Train Loss: 0.03906, lr: 0.000125\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 250/1000, Train Loss: 0.03906, lr: 0.000125\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 251/1000, Train Loss: 0.03906, lr: 0.000125\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 252/1000, Train Loss: 0.03906, lr: 0.000125\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 253/1000, Train Loss: 0.03905, lr: 0.000125\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 254/1000, Train Loss: 0.03905, lr: 0.000125\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 255/1000, Train Loss: 0.03905, lr: 0.000125\n",
      "Val Loss: 0.04083\n",
      "---------\n",
      "Epoch 256/1000, Train Loss: 0.03905, lr: 0.000125\n",
      "Val Loss: 0.04082\n",
      "---------\n",
      "Epoch 257/1000, Train Loss: 0.03905, lr: 0.000125\n",
      "Val Loss: 0.04082\n",
      "---------\n",
      "Epoch 258/1000, Train Loss: 0.03905, lr: 0.000125\n",
      "Val Loss: 0.04082\n",
      "---------\n",
      "Epoch 259/1000, Train Loss: 0.03905, lr: 0.000125\n",
      "Val Loss: 0.04082\n",
      "---------\n",
      "Epoch 260/1000, Train Loss: 0.03904, lr: 0.000125\n",
      "Val Loss: 0.04082\n",
      "---------\n",
      "Epoch 261/1000, Train Loss: 0.03904, lr: 0.000125\n",
      "Val Loss: 0.04082\n",
      "---------\n",
      "Epoch 262/1000, Train Loss: 0.03904, lr: 0.000125\n",
      "Val Loss: 0.04082\n",
      "---------\n",
      "Epoch 263/1000, Train Loss: 0.03904, lr: 0.000125\n",
      "Val Loss: 0.04082\n",
      "---------\n",
      "Epoch 264/1000, Train Loss: 0.03904, lr: 0.000125\n",
      "Val Loss: 0.04082\n",
      "---------\n",
      "Epoch 265/1000, Train Loss: 0.03904, lr: 0.000125\n",
      "Val Loss: 0.04082\n",
      "---------\n",
      "Epoch 266/1000, Train Loss: 0.03904, lr: 0.000125\n",
      "Val Loss: 0.04082\n",
      "---------\n",
      "Epoch 267/1000, Train Loss: 0.03903, lr: 0.000125\n",
      "Val Loss: 0.04082\n",
      "---------\n",
      "Epoch 268/1000, Train Loss: 0.03903, lr: 0.000125\n",
      "Val Loss: 0.04082\n",
      "---------\n",
      "Epoch 269/1000, Train Loss: 0.03903, lr: 0.000125\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 270/1000, Train Loss: 0.03903, lr: 0.000125\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 271/1000, Train Loss: 0.03903, lr: 0.000125\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 272/1000, Train Loss: 0.03903, lr: 0.000125\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 273/1000, Train Loss: 0.03903, lr: 0.000125\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 274/1000, Train Loss: 0.03902, lr: 0.000125\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 275/1000, Train Loss: 0.03902, lr: 0.000125\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 276/1000, Train Loss: 0.03902, lr: 0.000125\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 277/1000, Train Loss: 0.03902, lr: 0.000125\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 278/1000, Train Loss: 0.03902, lr: 0.000125\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 279/1000, Train Loss: 0.03902, lr: 0.000125\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 280/1000, Train Loss: 0.03902, lr: 0.000125\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 281/1000, Train Loss: 0.03901, lr: 0.000125\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 282/1000, Train Loss: 0.03901, lr: 0.000125\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 283/1000, Train Loss: 0.03901, lr: 0.000125\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 284/1000, Train Loss: 0.03901, lr: 0.000125\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 285/1000, Train Loss: 0.03901, lr: 0.000125\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 286/1000, Train Loss: 0.03901, lr: 0.000125\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 287/1000, Train Loss: 0.03901, lr: 0.000125\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 288/1000, Train Loss: 0.03900, lr: 0.000125\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 289/1000, Train Loss: 0.03900, lr: 0.000125\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 290/1000, Train Loss: 0.03900, lr: 0.000125\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 291/1000, Train Loss: 0.03900, lr: 0.000125\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 292/1000, Train Loss: 0.03900, lr: 0.000125\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 293/1000, Train Loss: 0.03900, lr: 0.000125\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 294/1000, Train Loss: 0.03900, lr: 0.000125\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 295/1000, Train Loss: 0.03900, lr: 0.000125\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 296/1000, Train Loss: 0.03899, lr: 0.000125\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 297/1000, Train Loss: 0.03899, lr: 0.000125\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 298/1000, Train Loss: 0.03899, lr: 0.000125\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 299/1000, Train Loss: 0.03899, lr: 0.000125\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 300/1000, Train Loss: 0.03899, lr: 0.000125\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 301/1000, Train Loss: 0.03899, lr: 0.000125\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 302/1000, Train Loss: 0.03899, lr: 0.000125\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 303/1000, Train Loss: 0.03899, lr: 0.000125\n",
      "Val Loss: 0.04080\n",
      "---------\n",
      "Epoch 304/1000, Train Loss: 0.03898, lr: 0.000125\n",
      "Val Loss: 0.04079\n",
      "---------\n",
      "Epoch 305/1000, Train Loss: 0.03898, lr: 0.000125\n",
      "Val Loss: 0.04079\n",
      "---------\n",
      "Epoch 306/1000, Train Loss: 0.03898, lr: 0.000125\n",
      "Val Loss: 0.04079\n",
      "---------\n",
      "Epoch 307/1000, Train Loss: 0.03898, lr: 0.000125\n",
      "Val Loss: 0.04079\n",
      "---------\n",
      "Epoch 308/1000, Train Loss: 0.03898, lr: 0.000125\n",
      "Val Loss: 0.04079\n",
      "---------\n",
      "Epoch 309/1000, Train Loss: 0.03898, lr: 0.000125\n",
      "Val Loss: 0.04079\n",
      "---------\n",
      "Epoch 310/1000, Train Loss: 0.03898, lr: 0.000125\n",
      "Val Loss: 0.04079\n",
      "---------\n",
      "Epoch 311/1000, Train Loss: 0.03898, lr: 0.000125\n",
      "Val Loss: 0.04079\n",
      "---------\n",
      "Epoch 312/1000, Train Loss: 0.03897, lr: 0.000125\n",
      "Val Loss: 0.04079\n",
      "---------\n",
      "Epoch 313/1000, Train Loss: 0.03897, lr: 0.000125\n",
      "Val Loss: 0.04079\n",
      "---------\n",
      "Epoch 314/1000, Train Loss: 0.03897, lr: 0.000125\n",
      "Val Loss: 0.04079\n",
      "---------\n",
      "Epoch 315/1000, Train Loss: 0.03897, lr: 0.000125\n",
      "Val Loss: 0.04079\n",
      "---------\n",
      "Epoch 316/1000, Train Loss: 0.03897, lr: 0.000125\n",
      "Val Loss: 0.04079\n",
      "---------\n",
      "Epoch 317/1000, Train Loss: 0.03897, lr: 0.000125\n",
      "Val Loss: 0.04079\n",
      "---------\n",
      "Epoch 318/1000, Train Loss: 0.03897, lr: 0.000125\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 319/1000, Train Loss: 0.03897, lr: 0.000125\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 320/1000, Train Loss: 0.03896, lr: 0.000125\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 321/1000, Train Loss: 0.03896, lr: 0.000125\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 322/1000, Train Loss: 0.03896, lr: 0.000125\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 323/1000, Train Loss: 0.03896, lr: 0.000125\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 324/1000, Train Loss: 0.03896, lr: 0.000125\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 325/1000, Train Loss: 0.03896, lr: 0.000125\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 326/1000, Train Loss: 0.03896, lr: 0.000125\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 327/1000, Train Loss: 0.03896, lr: 0.000125\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 328/1000, Train Loss: 0.03895, lr: 0.000125\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 329/1000, Train Loss: 0.03895, lr: 0.000125\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 330/1000, Train Loss: 0.03895, lr: 0.000125\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 331/1000, Train Loss: 0.03895, lr: 0.000125\n",
      "Val Loss: 0.04078\n",
      "---------\n",
      "Epoch 332/1000, Train Loss: 0.03895, lr: 0.000125\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 333/1000, Train Loss: 0.03895, lr: 0.000125\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 334/1000, Train Loss: 0.03895, lr: 0.000125\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 335/1000, Train Loss: 0.03895, lr: 0.000125\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 336/1000, Train Loss: 0.03895, lr: 0.000125\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 337/1000, Train Loss: 0.03894, lr: 0.000125\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 338/1000, Train Loss: 0.03894, lr: 0.000125\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 339/1000, Train Loss: 0.03894, lr: 0.000125\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 340/1000, Train Loss: 0.03894, lr: 0.000125\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 341/1000, Train Loss: 0.03894, lr: 0.000125\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 342/1000, Train Loss: 0.03894, lr: 0.000125\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 343/1000, Train Loss: 0.03894, lr: 0.000125\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 344/1000, Train Loss: 0.03894, lr: 0.000125\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 345/1000, Train Loss: 0.03894, lr: 0.000125\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 346/1000, Train Loss: 0.03893, lr: 0.000125\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 347/1000, Train Loss: 0.03893, lr: 0.000125\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 348/1000, Train Loss: 0.03893, lr: 0.000125\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 349/1000, Train Loss: 0.03893, lr: 0.000125\n",
      "Val Loss: 0.04077\n",
      "---------\n",
      "Epoch 350/1000, Train Loss: 0.03893, lr: 0.000125\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 351/1000, Train Loss: 0.03893, lr: 0.000125\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 352/1000, Train Loss: 0.03893, lr: 0.000125\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 353/1000, Train Loss: 0.03893, lr: 0.000125\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 354/1000, Train Loss: 0.03892, lr: 0.000125\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 355/1000, Train Loss: 0.03892, lr: 0.000125\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 356/1000, Train Loss: 0.03892, lr: 0.000125\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 357/1000, Train Loss: 0.03892, lr: 0.000125\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 358/1000, Train Loss: 0.03892, lr: 0.000125\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 359/1000, Train Loss: 0.03892, lr: 0.000125\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 360/1000, Train Loss: 0.03892, lr: 0.000125\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 361/1000, Train Loss: 0.03892, lr: 0.000125\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 362/1000, Train Loss: 0.03892, lr: 0.000125\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 363/1000, Train Loss: 0.03891, lr: 0.000125\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 364/1000, Train Loss: 0.03891, lr: 0.000125\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 365/1000, Train Loss: 0.03891, lr: 0.000125\n",
      "Val Loss: 0.04076\n",
      "---------\n",
      "Epoch 366/1000, Train Loss: 0.03891, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 367/1000, Train Loss: 0.03891, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 368/1000, Train Loss: 0.03891, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 369/1000, Train Loss: 0.03891, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 370/1000, Train Loss: 0.03891, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 371/1000, Train Loss: 0.03891, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 372/1000, Train Loss: 0.03890, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 373/1000, Train Loss: 0.03890, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 374/1000, Train Loss: 0.03890, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 375/1000, Train Loss: 0.03890, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 376/1000, Train Loss: 0.03890, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 377/1000, Train Loss: 0.03890, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 378/1000, Train Loss: 0.03890, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 379/1000, Train Loss: 0.03890, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 380/1000, Train Loss: 0.03890, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 381/1000, Train Loss: 0.03889, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 382/1000, Train Loss: 0.03889, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 383/1000, Train Loss: 0.03889, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 384/1000, Train Loss: 0.03889, lr: 0.000125\n",
      "Val Loss: 0.04075\n",
      "---------\n",
      "Epoch 385/1000, Train Loss: 0.03889, lr: 0.000125\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 386/1000, Train Loss: 0.03889, lr: 0.000125\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 387/1000, Train Loss: 0.03889, lr: 0.000125\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 388/1000, Train Loss: 0.03889, lr: 0.000125\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 389/1000, Train Loss: 0.03889, lr: 0.000125\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 390/1000, Train Loss: 0.03889, lr: 0.000125\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 391/1000, Train Loss: 0.03888, lr: 0.000125\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 392/1000, Train Loss: 0.03888, lr: 0.000125\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 393/1000, Train Loss: 0.03888, lr: 0.000125\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 394/1000, Train Loss: 0.03888, lr: 0.000125\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 395/1000, Train Loss: 0.03888, lr: 0.000125\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 396/1000, Train Loss: 0.03888, lr: 0.000125\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 397/1000, Train Loss: 0.03888, lr: 0.000125\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 398/1000, Train Loss: 0.03888, lr: 0.000125\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 399/1000, Train Loss: 0.03888, lr: 0.000125\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 400/1000, Train Loss: 0.03888, lr: 0.000125\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 401/1000, Train Loss: 0.03887, lr: 0.000125\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 402/1000, Train Loss: 0.03887, lr: 0.000125\n",
      "Val Loss: 0.04074\n",
      "---------\n",
      "Epoch 403/1000, Train Loss: 0.03887, lr: 0.000125\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 404/1000, Train Loss: 0.03887, lr: 0.000125\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 405/1000, Train Loss: 0.03887, lr: 0.000125\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 406/1000, Train Loss: 0.03887, lr: 0.000125\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 407/1000, Train Loss: 0.03887, lr: 0.000125\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 408/1000, Train Loss: 0.03887, lr: 0.000125\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 409/1000, Train Loss: 0.03887, lr: 0.000125\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 410/1000, Train Loss: 0.03887, lr: 0.000125\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 411/1000, Train Loss: 0.03886, lr: 0.000125\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 412/1000, Train Loss: 0.03886, lr: 0.000125\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 413/1000, Train Loss: 0.03886, lr: 0.000125\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 414/1000, Train Loss: 0.03886, lr: 0.000125\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 415/1000, Train Loss: 0.03886, lr: 0.000125\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 416/1000, Train Loss: 0.03886, lr: 0.000125\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 417/1000, Train Loss: 0.03886, lr: 0.000125\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 418/1000, Train Loss: 0.03886, lr: 0.000125\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 419/1000, Train Loss: 0.03886, lr: 0.000125\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 420/1000, Train Loss: 0.03885, lr: 0.000125\n",
      "Val Loss: 0.04073\n",
      "---------\n",
      "Epoch 421/1000, Train Loss: 0.03885, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 422/1000, Train Loss: 0.03885, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 423/1000, Train Loss: 0.03885, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 424/1000, Train Loss: 0.03885, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 425/1000, Train Loss: 0.03885, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 426/1000, Train Loss: 0.03885, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 427/1000, Train Loss: 0.03885, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 428/1000, Train Loss: 0.03885, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 429/1000, Train Loss: 0.03885, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 430/1000, Train Loss: 0.03884, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 431/1000, Train Loss: 0.03884, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 432/1000, Train Loss: 0.03884, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 433/1000, Train Loss: 0.03884, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 434/1000, Train Loss: 0.03884, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 435/1000, Train Loss: 0.03884, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 436/1000, Train Loss: 0.03884, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 437/1000, Train Loss: 0.03884, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 438/1000, Train Loss: 0.03884, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 439/1000, Train Loss: 0.03884, lr: 0.000125\n",
      "Val Loss: 0.04072\n",
      "---------\n",
      "Epoch 440/1000, Train Loss: 0.03883, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 441/1000, Train Loss: 0.03883, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 442/1000, Train Loss: 0.03883, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 443/1000, Train Loss: 0.03883, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 444/1000, Train Loss: 0.03883, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 445/1000, Train Loss: 0.03883, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 446/1000, Train Loss: 0.03883, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 447/1000, Train Loss: 0.03883, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 448/1000, Train Loss: 0.03883, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 449/1000, Train Loss: 0.03883, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 450/1000, Train Loss: 0.03883, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 451/1000, Train Loss: 0.03882, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 452/1000, Train Loss: 0.03882, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 453/1000, Train Loss: 0.03882, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 454/1000, Train Loss: 0.03882, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 455/1000, Train Loss: 0.03882, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 456/1000, Train Loss: 0.03882, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 457/1000, Train Loss: 0.03882, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 458/1000, Train Loss: 0.03882, lr: 0.000125\n",
      "Val Loss: 0.04071\n",
      "---------\n",
      "Epoch 459/1000, Train Loss: 0.03882, lr: 0.000125\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 460/1000, Train Loss: 0.03882, lr: 0.000125\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 461/1000, Train Loss: 0.03881, lr: 0.000125\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 462/1000, Train Loss: 0.03881, lr: 0.000125\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 463/1000, Train Loss: 0.03881, lr: 0.000125\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 464/1000, Train Loss: 0.03881, lr: 0.000125\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 465/1000, Train Loss: 0.03881, lr: 0.000125\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 466/1000, Train Loss: 0.03881, lr: 0.000125\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 467/1000, Train Loss: 0.03881, lr: 0.000125\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 468/1000, Train Loss: 0.03881, lr: 0.000125\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 469/1000, Train Loss: 0.03881, lr: 0.000125\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 470/1000, Train Loss: 0.03881, lr: 0.000125\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 471/1000, Train Loss: 0.03881, lr: 0.000125\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 472/1000, Train Loss: 0.03880, lr: 0.000125\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 473/1000, Train Loss: 0.03880, lr: 0.000125\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 474/1000, Train Loss: 0.03880, lr: 0.000125\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 475/1000, Train Loss: 0.03880, lr: 0.000125\n",
      "Val Loss: 0.04070\n",
      "---------\n",
      "Epoch 476/1000, Train Loss: 0.03880, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 477/1000, Train Loss: 0.03880, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 478/1000, Train Loss: 0.03880, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 479/1000, Train Loss: 0.03880, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 480/1000, Train Loss: 0.03880, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 481/1000, Train Loss: 0.03880, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 482/1000, Train Loss: 0.03880, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 483/1000, Train Loss: 0.03879, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 484/1000, Train Loss: 0.03879, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 485/1000, Train Loss: 0.03879, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 486/1000, Train Loss: 0.03879, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 487/1000, Train Loss: 0.03879, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 488/1000, Train Loss: 0.03879, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 489/1000, Train Loss: 0.03879, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 490/1000, Train Loss: 0.03879, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 491/1000, Train Loss: 0.03879, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 492/1000, Train Loss: 0.03879, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 493/1000, Train Loss: 0.03879, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 494/1000, Train Loss: 0.03878, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 495/1000, Train Loss: 0.03878, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 496/1000, Train Loss: 0.03878, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 497/1000, Train Loss: 0.03878, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 498/1000, Train Loss: 0.03878, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 499/1000, Train Loss: 0.03878, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 500/1000, Train Loss: 0.03878, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 501/1000, Train Loss: 0.03878, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 502/1000, Train Loss: 0.03878, lr: 0.000125\n",
      "Val Loss: 0.04069\n",
      "---------\n",
      "Epoch 503/1000, Train Loss: 0.03878, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 504/1000, Train Loss: 0.03878, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 505/1000, Train Loss: 0.03877, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 506/1000, Train Loss: 0.03877, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 507/1000, Train Loss: 0.03877, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 508/1000, Train Loss: 0.03877, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 509/1000, Train Loss: 0.03877, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 510/1000, Train Loss: 0.03877, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 511/1000, Train Loss: 0.03877, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 512/1000, Train Loss: 0.03877, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 513/1000, Train Loss: 0.03877, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 514/1000, Train Loss: 0.03877, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 515/1000, Train Loss: 0.03877, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 516/1000, Train Loss: 0.03876, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 517/1000, Train Loss: 0.03876, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 518/1000, Train Loss: 0.03876, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 519/1000, Train Loss: 0.03876, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 520/1000, Train Loss: 0.03876, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 521/1000, Train Loss: 0.03876, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 522/1000, Train Loss: 0.03876, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 523/1000, Train Loss: 0.03876, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 524/1000, Train Loss: 0.03876, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 525/1000, Train Loss: 0.03876, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 526/1000, Train Loss: 0.03876, lr: 0.000125\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 527/1000, Train Loss: 0.03875, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 528/1000, Train Loss: 0.03875, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 529/1000, Train Loss: 0.03875, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 530/1000, Train Loss: 0.03875, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 531/1000, Train Loss: 0.03875, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 532/1000, Train Loss: 0.03875, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 533/1000, Train Loss: 0.03875, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 534/1000, Train Loss: 0.03875, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 535/1000, Train Loss: 0.03875, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 536/1000, Train Loss: 0.03875, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 537/1000, Train Loss: 0.03875, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 538/1000, Train Loss: 0.03875, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 539/1000, Train Loss: 0.03874, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 540/1000, Train Loss: 0.03874, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 541/1000, Train Loss: 0.03874, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 542/1000, Train Loss: 0.03874, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 543/1000, Train Loss: 0.03874, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 544/1000, Train Loss: 0.03874, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 545/1000, Train Loss: 0.03874, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 546/1000, Train Loss: 0.03874, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 547/1000, Train Loss: 0.03874, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 548/1000, Train Loss: 0.03874, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 549/1000, Train Loss: 0.03874, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 550/1000, Train Loss: 0.03874, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 551/1000, Train Loss: 0.03873, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 552/1000, Train Loss: 0.03873, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 553/1000, Train Loss: 0.03873, lr: 0.000125\n",
      "Val Loss: 0.04067\n",
      "---------\n",
      "Epoch 554/1000, Train Loss: 0.03873, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 555/1000, Train Loss: 0.03873, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 556/1000, Train Loss: 0.03873, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 557/1000, Train Loss: 0.03873, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 558/1000, Train Loss: 0.03873, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 559/1000, Train Loss: 0.03873, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 560/1000, Train Loss: 0.03873, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 561/1000, Train Loss: 0.03873, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 562/1000, Train Loss: 0.03873, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 563/1000, Train Loss: 0.03872, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 564/1000, Train Loss: 0.03872, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 565/1000, Train Loss: 0.03872, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 566/1000, Train Loss: 0.03872, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 567/1000, Train Loss: 0.03872, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 568/1000, Train Loss: 0.03872, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 569/1000, Train Loss: 0.03872, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 570/1000, Train Loss: 0.03872, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 571/1000, Train Loss: 0.03872, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 572/1000, Train Loss: 0.03872, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 573/1000, Train Loss: 0.03872, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 574/1000, Train Loss: 0.03872, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 575/1000, Train Loss: 0.03871, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 576/1000, Train Loss: 0.03871, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 577/1000, Train Loss: 0.03871, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 578/1000, Train Loss: 0.03871, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 579/1000, Train Loss: 0.03871, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 580/1000, Train Loss: 0.03871, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 581/1000, Train Loss: 0.03871, lr: 0.000125\n",
      "Val Loss: 0.04066\n",
      "---------\n",
      "Epoch 582/1000, Train Loss: 0.03871, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 583/1000, Train Loss: 0.03871, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 584/1000, Train Loss: 0.03871, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 585/1000, Train Loss: 0.03871, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 586/1000, Train Loss: 0.03871, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 587/1000, Train Loss: 0.03870, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 588/1000, Train Loss: 0.03870, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 589/1000, Train Loss: 0.03870, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 590/1000, Train Loss: 0.03870, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 591/1000, Train Loss: 0.03870, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 592/1000, Train Loss: 0.03870, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 593/1000, Train Loss: 0.03870, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 594/1000, Train Loss: 0.03870, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 595/1000, Train Loss: 0.03870, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 596/1000, Train Loss: 0.03870, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 597/1000, Train Loss: 0.03870, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 598/1000, Train Loss: 0.03870, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 599/1000, Train Loss: 0.03870, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 600/1000, Train Loss: 0.03869, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 601/1000, Train Loss: 0.03869, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 602/1000, Train Loss: 0.03869, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 603/1000, Train Loss: 0.03869, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 604/1000, Train Loss: 0.03869, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 605/1000, Train Loss: 0.03869, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 606/1000, Train Loss: 0.03869, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 607/1000, Train Loss: 0.03869, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 608/1000, Train Loss: 0.03869, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 609/1000, Train Loss: 0.03869, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 610/1000, Train Loss: 0.03869, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 611/1000, Train Loss: 0.03869, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 612/1000, Train Loss: 0.03869, lr: 0.000125\n",
      "Val Loss: 0.04065\n",
      "---------\n",
      "Epoch 613/1000, Train Loss: 0.03868, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 614/1000, Train Loss: 0.03868, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 615/1000, Train Loss: 0.03868, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 616/1000, Train Loss: 0.03868, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 617/1000, Train Loss: 0.03868, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 618/1000, Train Loss: 0.03868, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 619/1000, Train Loss: 0.03868, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 620/1000, Train Loss: 0.03868, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 621/1000, Train Loss: 0.03868, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 622/1000, Train Loss: 0.03868, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 623/1000, Train Loss: 0.03868, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 624/1000, Train Loss: 0.03868, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 625/1000, Train Loss: 0.03868, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 626/1000, Train Loss: 0.03867, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 627/1000, Train Loss: 0.03867, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 628/1000, Train Loss: 0.03867, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 629/1000, Train Loss: 0.03867, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 630/1000, Train Loss: 0.03867, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 631/1000, Train Loss: 0.03867, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 632/1000, Train Loss: 0.03867, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 633/1000, Train Loss: 0.03867, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 634/1000, Train Loss: 0.03867, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 635/1000, Train Loss: 0.03867, lr: 0.000125\n",
      "Val Loss: 0.04064\n",
      "---------\n",
      "Epoch 636/1000, Train Loss: 0.03867, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 637/1000, Train Loss: 0.03867, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 638/1000, Train Loss: 0.03867, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 639/1000, Train Loss: 0.03866, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 640/1000, Train Loss: 0.03866, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 641/1000, Train Loss: 0.03866, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 642/1000, Train Loss: 0.03866, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 643/1000, Train Loss: 0.03866, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 644/1000, Train Loss: 0.03866, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 645/1000, Train Loss: 0.03866, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 646/1000, Train Loss: 0.03866, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 647/1000, Train Loss: 0.03866, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 648/1000, Train Loss: 0.03866, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 649/1000, Train Loss: 0.03866, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 650/1000, Train Loss: 0.03866, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 651/1000, Train Loss: 0.03866, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 652/1000, Train Loss: 0.03865, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 653/1000, Train Loss: 0.03865, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 654/1000, Train Loss: 0.03865, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 655/1000, Train Loss: 0.03865, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 656/1000, Train Loss: 0.03865, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 657/1000, Train Loss: 0.03865, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 658/1000, Train Loss: 0.03865, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 659/1000, Train Loss: 0.03865, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 660/1000, Train Loss: 0.03865, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 661/1000, Train Loss: 0.03865, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 662/1000, Train Loss: 0.03865, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 663/1000, Train Loss: 0.03865, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 664/1000, Train Loss: 0.03865, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 665/1000, Train Loss: 0.03864, lr: 0.000125\n",
      "Val Loss: 0.04063\n",
      "---------\n",
      "Epoch 666/1000, Train Loss: 0.03864, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 667/1000, Train Loss: 0.03864, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 668/1000, Train Loss: 0.03864, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 669/1000, Train Loss: 0.03864, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 670/1000, Train Loss: 0.03864, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 671/1000, Train Loss: 0.03864, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 672/1000, Train Loss: 0.03864, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 673/1000, Train Loss: 0.03864, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 674/1000, Train Loss: 0.03864, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 675/1000, Train Loss: 0.03864, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 676/1000, Train Loss: 0.03864, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 677/1000, Train Loss: 0.03864, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 678/1000, Train Loss: 0.03864, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 679/1000, Train Loss: 0.03863, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 680/1000, Train Loss: 0.03863, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 681/1000, Train Loss: 0.03863, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 682/1000, Train Loss: 0.03863, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 683/1000, Train Loss: 0.03863, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 684/1000, Train Loss: 0.03863, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 685/1000, Train Loss: 0.03863, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 686/1000, Train Loss: 0.03863, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 687/1000, Train Loss: 0.03863, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 688/1000, Train Loss: 0.03863, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 689/1000, Train Loss: 0.03863, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 690/1000, Train Loss: 0.03863, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 691/1000, Train Loss: 0.03863, lr: 0.000125\n",
      "Val Loss: 0.04062\n",
      "---------\n",
      "Epoch 692/1000, Train Loss: 0.03863, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 693/1000, Train Loss: 0.03862, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 694/1000, Train Loss: 0.03862, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 695/1000, Train Loss: 0.03862, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 696/1000, Train Loss: 0.03862, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 697/1000, Train Loss: 0.03862, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 698/1000, Train Loss: 0.03862, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 699/1000, Train Loss: 0.03862, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 700/1000, Train Loss: 0.03862, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 701/1000, Train Loss: 0.03862, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 702/1000, Train Loss: 0.03862, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 703/1000, Train Loss: 0.03862, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 704/1000, Train Loss: 0.03862, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 705/1000, Train Loss: 0.03862, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 706/1000, Train Loss: 0.03862, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 707/1000, Train Loss: 0.03862, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 708/1000, Train Loss: 0.03861, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 709/1000, Train Loss: 0.03861, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 710/1000, Train Loss: 0.03861, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 711/1000, Train Loss: 0.03861, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 712/1000, Train Loss: 0.03861, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 713/1000, Train Loss: 0.03861, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 714/1000, Train Loss: 0.03861, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 715/1000, Train Loss: 0.03861, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 716/1000, Train Loss: 0.03861, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 717/1000, Train Loss: 0.03861, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 718/1000, Train Loss: 0.03861, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 719/1000, Train Loss: 0.03861, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 720/1000, Train Loss: 0.03861, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 721/1000, Train Loss: 0.03861, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 722/1000, Train Loss: 0.03860, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 723/1000, Train Loss: 0.03860, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 724/1000, Train Loss: 0.03860, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 725/1000, Train Loss: 0.03860, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 726/1000, Train Loss: 0.03860, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 727/1000, Train Loss: 0.03860, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 728/1000, Train Loss: 0.03860, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 729/1000, Train Loss: 0.03860, lr: 0.000125\n",
      "Val Loss: 0.04061\n",
      "---------\n",
      "Epoch 730/1000, Train Loss: 0.03860, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 731/1000, Train Loss: 0.03860, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 732/1000, Train Loss: 0.03860, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 733/1000, Train Loss: 0.03860, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 734/1000, Train Loss: 0.03860, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 735/1000, Train Loss: 0.03860, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 736/1000, Train Loss: 0.03859, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 737/1000, Train Loss: 0.03859, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 738/1000, Train Loss: 0.03859, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 739/1000, Train Loss: 0.03859, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 740/1000, Train Loss: 0.03859, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 741/1000, Train Loss: 0.03859, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 742/1000, Train Loss: 0.03859, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 743/1000, Train Loss: 0.03859, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 744/1000, Train Loss: 0.03859, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 745/1000, Train Loss: 0.03859, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 746/1000, Train Loss: 0.03859, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 747/1000, Train Loss: 0.03859, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 748/1000, Train Loss: 0.03859, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 749/1000, Train Loss: 0.03859, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 750/1000, Train Loss: 0.03859, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 751/1000, Train Loss: 0.03858, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 752/1000, Train Loss: 0.03858, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 753/1000, Train Loss: 0.03858, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 754/1000, Train Loss: 0.03858, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 755/1000, Train Loss: 0.03858, lr: 0.000125\n",
      "Val Loss: 0.04060\n",
      "---------\n",
      "Epoch 756/1000, Train Loss: 0.03858, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 757/1000, Train Loss: 0.03858, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 758/1000, Train Loss: 0.03858, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 759/1000, Train Loss: 0.03858, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 760/1000, Train Loss: 0.03858, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 761/1000, Train Loss: 0.03858, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 762/1000, Train Loss: 0.03858, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 763/1000, Train Loss: 0.03858, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 764/1000, Train Loss: 0.03858, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 765/1000, Train Loss: 0.03858, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 766/1000, Train Loss: 0.03857, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 767/1000, Train Loss: 0.03857, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 768/1000, Train Loss: 0.03857, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 769/1000, Train Loss: 0.03857, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 770/1000, Train Loss: 0.03857, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 771/1000, Train Loss: 0.03857, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 772/1000, Train Loss: 0.03857, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 773/1000, Train Loss: 0.03857, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 774/1000, Train Loss: 0.03857, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 775/1000, Train Loss: 0.03857, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 776/1000, Train Loss: 0.03857, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 777/1000, Train Loss: 0.03857, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 778/1000, Train Loss: 0.03857, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 779/1000, Train Loss: 0.03857, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 780/1000, Train Loss: 0.03857, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 781/1000, Train Loss: 0.03856, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 782/1000, Train Loss: 0.03856, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 783/1000, Train Loss: 0.03856, lr: 0.000125\n",
      "Val Loss: 0.04059\n",
      "---------\n",
      "Epoch 784/1000, Train Loss: 0.03856, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 785/1000, Train Loss: 0.03856, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 786/1000, Train Loss: 0.03856, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 787/1000, Train Loss: 0.03856, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 788/1000, Train Loss: 0.03856, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 789/1000, Train Loss: 0.03856, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 790/1000, Train Loss: 0.03856, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 791/1000, Train Loss: 0.03856, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 792/1000, Train Loss: 0.03856, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 793/1000, Train Loss: 0.03856, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 794/1000, Train Loss: 0.03856, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 795/1000, Train Loss: 0.03856, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 796/1000, Train Loss: 0.03855, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 797/1000, Train Loss: 0.03855, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 798/1000, Train Loss: 0.03855, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 799/1000, Train Loss: 0.03855, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 800/1000, Train Loss: 0.03855, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 801/1000, Train Loss: 0.03855, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 802/1000, Train Loss: 0.03855, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 803/1000, Train Loss: 0.03855, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 804/1000, Train Loss: 0.03855, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 805/1000, Train Loss: 0.03855, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 806/1000, Train Loss: 0.03855, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 807/1000, Train Loss: 0.03855, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 808/1000, Train Loss: 0.03855, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 809/1000, Train Loss: 0.03855, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 810/1000, Train Loss: 0.03855, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 811/1000, Train Loss: 0.03855, lr: 0.000125\n",
      "Val Loss: 0.04058\n",
      "---------\n",
      "Epoch 812/1000, Train Loss: 0.03854, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 813/1000, Train Loss: 0.03854, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 814/1000, Train Loss: 0.03854, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 815/1000, Train Loss: 0.03854, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 816/1000, Train Loss: 0.03854, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 817/1000, Train Loss: 0.03854, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 818/1000, Train Loss: 0.03854, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 819/1000, Train Loss: 0.03854, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 820/1000, Train Loss: 0.03854, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 821/1000, Train Loss: 0.03854, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 822/1000, Train Loss: 0.03854, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 823/1000, Train Loss: 0.03854, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 824/1000, Train Loss: 0.03854, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 825/1000, Train Loss: 0.03854, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 826/1000, Train Loss: 0.03854, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 827/1000, Train Loss: 0.03854, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 828/1000, Train Loss: 0.03853, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 829/1000, Train Loss: 0.03853, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 830/1000, Train Loss: 0.03853, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 831/1000, Train Loss: 0.03853, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 832/1000, Train Loss: 0.03853, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 833/1000, Train Loss: 0.03853, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 834/1000, Train Loss: 0.03853, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 835/1000, Train Loss: 0.03853, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 836/1000, Train Loss: 0.03853, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 837/1000, Train Loss: 0.03853, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 838/1000, Train Loss: 0.03853, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 839/1000, Train Loss: 0.03853, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 840/1000, Train Loss: 0.03853, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 841/1000, Train Loss: 0.03853, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 842/1000, Train Loss: 0.03853, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 843/1000, Train Loss: 0.03853, lr: 0.000125\n",
      "Val Loss: 0.04057\n",
      "---------\n",
      "Epoch 844/1000, Train Loss: 0.03852, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 845/1000, Train Loss: 0.03852, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 846/1000, Train Loss: 0.03852, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 847/1000, Train Loss: 0.03852, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 848/1000, Train Loss: 0.03852, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 849/1000, Train Loss: 0.03852, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 850/1000, Train Loss: 0.03852, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 851/1000, Train Loss: 0.03852, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 852/1000, Train Loss: 0.03852, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 853/1000, Train Loss: 0.03852, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 854/1000, Train Loss: 0.03852, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 855/1000, Train Loss: 0.03852, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 856/1000, Train Loss: 0.03852, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 857/1000, Train Loss: 0.03852, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 858/1000, Train Loss: 0.03852, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 859/1000, Train Loss: 0.03852, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 860/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 861/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 862/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 863/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 864/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 865/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 866/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 867/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 868/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 869/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 870/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 871/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 872/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 873/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 874/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 875/1000, Train Loss: 0.03851, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 876/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 877/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.04056\n",
      "---------\n",
      "Epoch 878/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 879/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 880/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 881/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 882/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 883/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 884/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 885/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 886/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 887/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 888/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 889/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 890/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 891/1000, Train Loss: 0.03850, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 892/1000, Train Loss: 0.03849, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 893/1000, Train Loss: 0.03849, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 894/1000, Train Loss: 0.03849, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 895/1000, Train Loss: 0.03849, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 896/1000, Train Loss: 0.03849, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 897/1000, Train Loss: 0.03849, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 898/1000, Train Loss: 0.03849, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 899/1000, Train Loss: 0.03849, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 900/1000, Train Loss: 0.03849, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 901/1000, Train Loss: 0.03849, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 902/1000, Train Loss: 0.03849, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 903/1000, Train Loss: 0.03849, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 904/1000, Train Loss: 0.03849, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 905/1000, Train Loss: 0.03849, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 906/1000, Train Loss: 0.03849, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 907/1000, Train Loss: 0.03849, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 908/1000, Train Loss: 0.03849, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 909/1000, Train Loss: 0.03848, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 910/1000, Train Loss: 0.03848, lr: 0.000125\n",
      "Val Loss: 0.04055\n",
      "---------\n",
      "Epoch 911/1000, Train Loss: 0.03848, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 912/1000, Train Loss: 0.03848, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 913/1000, Train Loss: 0.03848, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 914/1000, Train Loss: 0.03848, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 915/1000, Train Loss: 0.03848, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 916/1000, Train Loss: 0.03848, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 917/1000, Train Loss: 0.03848, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 918/1000, Train Loss: 0.03848, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 919/1000, Train Loss: 0.03848, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 920/1000, Train Loss: 0.03848, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 921/1000, Train Loss: 0.03848, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 922/1000, Train Loss: 0.03848, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 923/1000, Train Loss: 0.03848, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 924/1000, Train Loss: 0.03848, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 925/1000, Train Loss: 0.03848, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 926/1000, Train Loss: 0.03847, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 927/1000, Train Loss: 0.03847, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 928/1000, Train Loss: 0.03847, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 929/1000, Train Loss: 0.03847, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 930/1000, Train Loss: 0.03847, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 931/1000, Train Loss: 0.03847, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 932/1000, Train Loss: 0.03847, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 933/1000, Train Loss: 0.03847, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 934/1000, Train Loss: 0.03847, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 935/1000, Train Loss: 0.03847, lr: 0.000125\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 936/1000, Train Loss: 0.03847, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 937/1000, Train Loss: 0.03847, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 938/1000, Train Loss: 0.03847, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 939/1000, Train Loss: 0.03847, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 940/1000, Train Loss: 0.03847, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 941/1000, Train Loss: 0.03847, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 942/1000, Train Loss: 0.03847, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 943/1000, Train Loss: 0.03846, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 944/1000, Train Loss: 0.03846, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 945/1000, Train Loss: 0.03846, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 946/1000, Train Loss: 0.03846, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 947/1000, Train Loss: 0.03846, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 948/1000, Train Loss: 0.03846, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 949/1000, Train Loss: 0.03846, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 950/1000, Train Loss: 0.03846, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 951/1000, Train Loss: 0.03846, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 952/1000, Train Loss: 0.03846, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 953/1000, Train Loss: 0.03846, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 954/1000, Train Loss: 0.03846, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 955/1000, Train Loss: 0.03846, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 956/1000, Train Loss: 0.03846, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 957/1000, Train Loss: 0.03846, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 958/1000, Train Loss: 0.03846, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 959/1000, Train Loss: 0.03846, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 960/1000, Train Loss: 0.03845, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 961/1000, Train Loss: 0.03845, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 962/1000, Train Loss: 0.03845, lr: 0.000125\n",
      "Val Loss: 0.04053\n",
      "---------\n",
      "Epoch 963/1000, Train Loss: 0.03845, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 964/1000, Train Loss: 0.03845, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 965/1000, Train Loss: 0.03845, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 966/1000, Train Loss: 0.03845, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 967/1000, Train Loss: 0.03845, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 968/1000, Train Loss: 0.03845, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 969/1000, Train Loss: 0.03845, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 970/1000, Train Loss: 0.03845, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 971/1000, Train Loss: 0.03845, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 972/1000, Train Loss: 0.03845, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 973/1000, Train Loss: 0.03845, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 974/1000, Train Loss: 0.03845, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 975/1000, Train Loss: 0.03845, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 976/1000, Train Loss: 0.03845, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 977/1000, Train Loss: 0.03844, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 978/1000, Train Loss: 0.03844, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 979/1000, Train Loss: 0.03844, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 980/1000, Train Loss: 0.03844, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 981/1000, Train Loss: 0.03844, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 982/1000, Train Loss: 0.03844, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 983/1000, Train Loss: 0.03844, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 984/1000, Train Loss: 0.03844, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 985/1000, Train Loss: 0.03844, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 986/1000, Train Loss: 0.03844, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 987/1000, Train Loss: 0.03844, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 988/1000, Train Loss: 0.03844, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 989/1000, Train Loss: 0.03844, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 990/1000, Train Loss: 0.03844, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 991/1000, Train Loss: 0.03844, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 992/1000, Train Loss: 0.03844, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 993/1000, Train Loss: 0.03844, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 994/1000, Train Loss: 0.03843, lr: 0.000125\n",
      "Val Loss: 0.04052\n",
      "---------\n",
      "Epoch 995/1000, Train Loss: 0.03843, lr: 0.000125\n",
      "Val Loss: 0.04051\n",
      "---------\n",
      "Epoch 996/1000, Train Loss: 0.03843, lr: 0.000125\n",
      "Val Loss: 0.04051\n",
      "---------\n",
      "Epoch 997/1000, Train Loss: 0.03843, lr: 0.000125\n",
      "Val Loss: 0.04051\n",
      "---------\n",
      "Epoch 998/1000, Train Loss: 0.03843, lr: 0.000125\n",
      "Val Loss: 0.04051\n",
      "---------\n",
      "Epoch 999/1000, Train Loss: 0.03843, lr: 0.000125\n",
      "Val Loss: 0.04051\n",
      "---------\n",
      "Epoch 1000/1000, Train Loss: 0.03843, lr: 0.000125\n",
      "Val Loss: 0.04051\n",
      "---------\n",
      "6430.947214603424 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 2)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 2)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 2)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 2)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 2)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 2)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 2)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 2)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 2)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 2)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 2)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 2)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 2)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 2)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 2)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 2)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 2)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 2)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 2)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 2)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 2)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 2)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 2)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 2)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 2)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 2)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 2)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 2)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 2)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 2)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 2)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 2)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 2)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 2)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 2)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 2)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 2)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 2)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 2)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 2)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 2)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 2)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 2)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 2)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 2)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 2)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 2)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 2)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 2)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 2)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 2)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 2)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 2)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 2)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 2)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 2)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 2)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 2)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 2)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 2)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 2)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 2)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 2)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 2)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 2)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 2)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 2)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 2)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 2)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 2)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 2)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 2)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 2)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 2)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 2)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 2)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 2)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 2)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 2)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 2)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 2)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 2)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 2)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 2)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 2)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 2)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 2)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 2)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 2)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 2)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 2)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 2)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 2)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 2)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 2)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 2)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 2)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 2)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 2)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 2)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 2)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 2)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 2)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 2)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 2)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 2)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 2)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 2)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 2)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 2)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 2)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 2)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 2)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 2)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 2)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 2)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 2)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 2)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 2)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 2)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 2)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 2)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 2)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 2)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 2)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 2)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 2)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 2)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 2)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 2)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 2)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 2)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 2)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 2)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 2)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 2)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 2)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 2)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 2)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 2)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 2)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 2)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 2)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 2)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 2)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 2)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 2)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 2)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 2)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 2)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 2)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 2)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 2)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 2)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 2)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 2)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 2)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 2)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 2)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 2)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 2)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 2)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 2)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 2)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 2)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 2)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 2)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 2)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 2)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 2)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 2)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 2)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 2)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 2)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 2)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 2)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 2)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 2)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 2)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 2)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 2)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 2)\n",
      "182\n",
      "y_hat: (1459, 32, 48, 6, 2), y_hat_i: (3, 32, 48, 6, 2), y_i: (3, 32, 48, 6, 2), batch.x: torch.Size([96, 48, 6, 6]), y: (1459, 32, 48, 6, 2)\n",
      "RMSE for t2m: 2.031754903908402; MAE for t2m: 1.5247316067221877;\n",
      "RMSE for sp: 2.2459826445433606; MAE for sp: 1.5952936672773994;\n",
      "RMSE for tcc: 0.3187812825519198; MAE for tcc: 0.22661281214917245;\n",
      "RMSE for u10: 1.6457927520316042; MAE for u10: 1.2068274446466958;\n",
      "RMSE for v10: 1.6272068016493992; MAE for v10: 1.190532562929684;\n",
      "RMSE for tp: 0.30990250309496986; MAE for tp: 0.08332708760771977;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 2)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 2)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 2)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 2)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 2)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 2)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 2)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 2)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 2)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 2)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 2)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 2)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 2)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 2)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 2)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 2)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 2)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 2)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 2)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 2)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 2)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 2)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 2)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 2)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 2)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 2)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 2)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 2)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 2)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 2)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 2)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 2)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 2)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 2)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 2)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 2)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 2)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 2)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 2)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 2)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 2)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 2)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 2)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 2)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 2)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 2)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 2)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 2)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 2)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 2)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 2)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 2)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 2)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 2)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 2)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 2)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 2)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 2)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 2)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 2)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 2)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 2)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 2)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 2)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 2)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 2)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 2)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 2)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 2)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 2)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 2)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 2)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 2)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 2)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 2)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 2)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 2)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 2)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 2)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 2)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 2)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 2)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 2)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 2)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 2)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 2)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 2)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 2)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 2)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 2)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 2)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 2)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 2)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 2)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 2)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 2)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 2)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 2)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 2)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 2)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 2)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 2)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 2)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 2)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 2)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 2)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 2)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 2)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 2)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 2)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 2)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 2)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 2)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 2)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 2)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 2)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 2)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 2)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 2)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 2)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 2)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 2)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 2)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 2)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 2)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 2)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 2)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 2)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 2)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 2)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 2)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 2)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 2)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 2)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 2)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 2)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 2)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 2)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 2)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 2)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 2)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 2)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 2)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 2)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 2)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 2)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 2)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 2)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 2)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 2)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 2)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 2)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 2)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 2)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 2)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 2)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 2)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 2)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 2)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 2)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 2)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 2)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 2)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 2)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 2)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 2)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 2)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 2)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 2)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 2)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 2)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 2)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 2)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 2)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 2)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 2)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 2)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 2)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 2)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 2)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 2)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 2), y_hat_i: (8, 32, 48, 6, 2), y_i: (8, 32, 48, 6, 2), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 2)\n",
      "182\n",
      "y_hat: (1459, 32, 48, 6, 2), y_hat_i: (3, 32, 48, 6, 2), y_i: (3, 32, 48, 6, 2), batch.x: torch.Size([96, 48, 6, 6]), y: (1459, 32, 48, 6, 2)\n",
      "RMSE for t2m: 2.031754903908402; MAE for t2m: 1.5247316067221877;\n",
      "RMSE for sp: 2.2459826445433606; MAE for sp: 1.5952936672773994;\n",
      "RMSE for tcc: 0.31832221095088065; MAE for tcc: 0.22558925224577714;\n",
      "RMSE for u10: 1.6457927520316042; MAE for u10: 1.2068274446466958;\n",
      "RMSE for v10: 1.6272068016493992; MAE for v10: 1.190532562929684;\n",
      "RMSE for tp: 0.30990250309496986; MAE for tp: 0.08332708760771977;\n",
      "Epoch 1/1000, Train Loss: 0.07641, lr: 0.001------------------------------------| 18.2% Complete\n",
      "Val Loss: 0.06534\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.06241, lr: 0.001\n",
      "Val Loss: 0.06048\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.05920, lr: 0.001\n",
      "Val Loss: 0.05957\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.05745, lr: 0.001\n",
      "Val Loss: 0.05775\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.05493, lr: 0.001\n",
      "Val Loss: 0.05543\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.05356, lr: 0.001\n",
      "Val Loss: 0.05440\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.05284, lr: 0.001\n",
      "Val Loss: 0.05392\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.05235, lr: 0.001\n",
      "Val Loss: 0.05361\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.05194, lr: 0.001\n",
      "Val Loss: 0.05302\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.05155, lr: 0.001\n",
      "Val Loss: 0.05257\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.05110, lr: 0.001\n",
      "Val Loss: 0.05251\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.05081, lr: 0.001\n",
      "Val Loss: 0.05253\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.05057, lr: 0.001\n",
      "Val Loss: 0.05232\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.05033, lr: 0.001\n",
      "Val Loss: 0.05217\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.05008, lr: 0.001\n",
      "Val Loss: 0.05181\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.04988, lr: 0.001\n",
      "Val Loss: 0.05158\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.04967, lr: 0.001\n",
      "Val Loss: 0.05135\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.04941, lr: 0.001\n",
      "Val Loss: 0.05096\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.04905, lr: 0.001\n",
      "Val Loss: 0.05083\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.04871, lr: 0.001\n",
      "Val Loss: 0.05049\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.04836, lr: 0.001\n",
      "Val Loss: 0.05048\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.04809, lr: 0.001\n",
      "Val Loss: 0.05042\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.04786, lr: 0.001\n",
      "Val Loss: 0.05040\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.04769, lr: 0.001\n",
      "Val Loss: 0.05018\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.04753, lr: 0.001\n",
      "Val Loss: 0.05000\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.04739, lr: 0.001\n",
      "Val Loss: 0.04979\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.04726, lr: 0.001\n",
      "Val Loss: 0.04960\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.04715, lr: 0.001\n",
      "Val Loss: 0.04956\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.04705, lr: 0.001\n",
      "Val Loss: 0.04948\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.04696, lr: 0.001\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.04688, lr: 0.001\n",
      "Val Loss: 0.04925\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.04680, lr: 0.001\n",
      "Val Loss: 0.04922\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.04674, lr: 0.001\n",
      "Val Loss: 0.04918\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.04667, lr: 0.001\n",
      "Val Loss: 0.04918\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.04659, lr: 0.001\n",
      "Val Loss: 0.04918\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.04653, lr: 0.001\n",
      "Val Loss: 0.04917\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.04647, lr: 0.001\n",
      "Val Loss: 0.04911\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.04641, lr: 0.001\n",
      "Val Loss: 0.04922\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.04636, lr: 0.001\n",
      "Val Loss: 0.04918\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.04630, lr: 0.001\n",
      "Val Loss: 0.04926\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.04625, lr: 0.001\n",
      "Val Loss: 0.04927\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.04619, lr: 0.001\n",
      "Val Loss: 0.04929\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.04615, lr: 0.001\n",
      "Val Loss: 0.04924\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.04610, lr: 0.001\n",
      "Val Loss: 0.04924\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 45/1000, Train Loss: 0.04547, lr: 0.0005\n",
      "Val Loss: 0.04761\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.04531, lr: 0.0005\n",
      "Val Loss: 0.04763\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.04524, lr: 0.0005\n",
      "Val Loss: 0.04766\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.04518, lr: 0.0005\n",
      "Val Loss: 0.04768\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.04513, lr: 0.0005\n",
      "Val Loss: 0.04771\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.04510, lr: 0.0005\n",
      "Val Loss: 0.04774\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.04505, lr: 0.0005\n",
      "Val Loss: 0.04775\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.04502, lr: 0.0005\n",
      "Val Loss: 0.04778\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 53/1000, Train Loss: 0.04465, lr: 0.00025\n",
      "Val Loss: 0.04685\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.04456, lr: 0.00025\n",
      "Val Loss: 0.04683\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.04452, lr: 0.00025\n",
      "Val Loss: 0.04682\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.04450, lr: 0.00025\n",
      "Val Loss: 0.04682\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.04447, lr: 0.00025\n",
      "Val Loss: 0.04682\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.04444, lr: 0.00025\n",
      "Val Loss: 0.04682\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.04442, lr: 0.00025\n",
      "Val Loss: 0.04681\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.04439, lr: 0.00025\n",
      "Val Loss: 0.04681\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.04437, lr: 0.00025\n",
      "Val Loss: 0.04681\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.04434, lr: 0.00025\n",
      "Val Loss: 0.04681\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.04432, lr: 0.00025\n",
      "Val Loss: 0.04682\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.04430, lr: 0.00025\n",
      "Val Loss: 0.04682\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.04428, lr: 0.00025\n",
      "Val Loss: 0.04682\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.04426, lr: 0.00025\n",
      "Val Loss: 0.04683\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.04424, lr: 0.00025\n",
      "Val Loss: 0.04684\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.04422, lr: 0.00025\n",
      "Val Loss: 0.04685\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 69/1000, Train Loss: 0.04397, lr: 0.000125\n",
      "Val Loss: 0.04636\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.04393, lr: 0.000125\n",
      "Val Loss: 0.04635\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.04391, lr: 0.000125\n",
      "Val Loss: 0.04634\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.04389, lr: 0.000125\n",
      "Val Loss: 0.04634\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.04388, lr: 0.000125\n",
      "Val Loss: 0.04634\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.04386, lr: 0.000125\n",
      "Val Loss: 0.04634\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.04385, lr: 0.000125\n",
      "Val Loss: 0.04633\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.04384, lr: 0.000125\n",
      "Val Loss: 0.04633\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.04382, lr: 0.000125\n",
      "Val Loss: 0.04633\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.04381, lr: 0.000125\n",
      "Val Loss: 0.04633\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.04380, lr: 0.000125\n",
      "Val Loss: 0.04633\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.04379, lr: 0.000125\n",
      "Val Loss: 0.04632\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.04378, lr: 0.000125\n",
      "Val Loss: 0.04632\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.04376, lr: 0.000125\n",
      "Val Loss: 0.04632\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.04375, lr: 0.000125\n",
      "Val Loss: 0.04632\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.04374, lr: 0.000125\n",
      "Val Loss: 0.04631\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.04373, lr: 0.000125\n",
      "Val Loss: 0.04631\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.04372, lr: 0.000125\n",
      "Val Loss: 0.04631\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.04371, lr: 0.000125\n",
      "Val Loss: 0.04630\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.04369, lr: 0.000125\n",
      "Val Loss: 0.04630\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.04368, lr: 0.000125\n",
      "Val Loss: 0.04630\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.04367, lr: 0.000125\n",
      "Val Loss: 0.04630\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.04366, lr: 0.000125\n",
      "Val Loss: 0.04629\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.04365, lr: 0.000125\n",
      "Val Loss: 0.04629\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.04364, lr: 0.000125\n",
      "Val Loss: 0.04629\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.04363, lr: 0.000125\n",
      "Val Loss: 0.04629\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.04362, lr: 0.000125\n",
      "Val Loss: 0.04629\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.04361, lr: 0.000125\n",
      "Val Loss: 0.04629\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.04360, lr: 0.000125\n",
      "Val Loss: 0.04628\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.04359, lr: 0.000125\n",
      "Val Loss: 0.04628\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.04358, lr: 0.000125\n",
      "Val Loss: 0.04628\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.04357, lr: 0.000125\n",
      "Val Loss: 0.04628\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.04356, lr: 0.000125\n",
      "Val Loss: 0.04628\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.04355, lr: 0.000125\n",
      "Val Loss: 0.04627\n",
      "---------\n",
      "Epoch 103/1000, Train Loss: 0.04354, lr: 0.000125\n",
      "Val Loss: 0.04627\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.04353, lr: 0.000125\n",
      "Val Loss: 0.04627\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.04352, lr: 0.000125\n",
      "Val Loss: 0.04627\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.04351, lr: 0.000125\n",
      "Val Loss: 0.04627\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.04350, lr: 0.000125\n",
      "Val Loss: 0.04627\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.04349, lr: 0.000125\n",
      "Val Loss: 0.04627\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.04348, lr: 0.000125\n",
      "Val Loss: 0.04626\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.04347, lr: 0.000125\n",
      "Val Loss: 0.04626\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.04346, lr: 0.000125\n",
      "Val Loss: 0.04626\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.04345, lr: 0.000125\n",
      "Val Loss: 0.04625\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.04344, lr: 0.000125\n",
      "Val Loss: 0.04625\n",
      "---------\n",
      "Epoch 114/1000, Train Loss: 0.04343, lr: 0.000125\n",
      "Val Loss: 0.04625\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.04342, lr: 0.000125\n",
      "Val Loss: 0.04625\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.04342, lr: 0.000125\n",
      "Val Loss: 0.04624\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.04341, lr: 0.000125\n",
      "Val Loss: 0.04624\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.04340, lr: 0.000125\n",
      "Val Loss: 0.04624\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.04339, lr: 0.000125\n",
      "Val Loss: 0.04624\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.04338, lr: 0.000125\n",
      "Val Loss: 0.04624\n",
      "---------\n",
      "Epoch 121/1000, Train Loss: 0.04337, lr: 0.000125\n",
      "Val Loss: 0.04623\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.04336, lr: 0.000125\n",
      "Val Loss: 0.04623\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.04335, lr: 0.000125\n",
      "Val Loss: 0.04623\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.04334, lr: 0.000125\n",
      "Val Loss: 0.04623\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.04333, lr: 0.000125\n",
      "Val Loss: 0.04622\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.04333, lr: 0.000125\n",
      "Val Loss: 0.04622\n",
      "---------\n",
      "Epoch 127/1000, Train Loss: 0.04332, lr: 0.000125\n",
      "Val Loss: 0.04622\n",
      "---------\n",
      "Epoch 128/1000, Train Loss: 0.04331, lr: 0.000125\n",
      "Val Loss: 0.04622\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.04330, lr: 0.000125\n",
      "Val Loss: 0.04622\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.04329, lr: 0.000125\n",
      "Val Loss: 0.04622\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.04328, lr: 0.000125\n",
      "Val Loss: 0.04621\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.04327, lr: 0.000125\n",
      "Val Loss: 0.04621\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.04327, lr: 0.000125\n",
      "Val Loss: 0.04621\n",
      "---------\n",
      "Epoch 134/1000, Train Loss: 0.04326, lr: 0.000125\n",
      "Val Loss: 0.04621\n",
      "---------\n",
      "Epoch 135/1000, Train Loss: 0.04325, lr: 0.000125\n",
      "Val Loss: 0.04621\n",
      "---------\n",
      "Epoch 136/1000, Train Loss: 0.04324, lr: 0.000125\n",
      "Val Loss: 0.04621\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.04323, lr: 0.000125\n",
      "Val Loss: 0.04620\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.04322, lr: 0.000125\n",
      "Val Loss: 0.04620\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.04321, lr: 0.000125\n",
      "Val Loss: 0.04620\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.04321, lr: 0.000125\n",
      "Val Loss: 0.04620\n",
      "---------\n",
      "Epoch 141/1000, Train Loss: 0.04320, lr: 0.000125\n",
      "Val Loss: 0.04620\n",
      "---------\n",
      "Epoch 142/1000, Train Loss: 0.04319, lr: 0.000125\n",
      "Val Loss: 0.04620\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.04318, lr: 0.000125\n",
      "Val Loss: 0.04620\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.04317, lr: 0.000125\n",
      "Val Loss: 0.04620\n",
      "---------\n",
      "Epoch 145/1000, Train Loss: 0.04317, lr: 0.000125\n",
      "Val Loss: 0.04619\n",
      "---------\n",
      "Epoch 146/1000, Train Loss: 0.04316, lr: 0.000125\n",
      "Val Loss: 0.04619\n",
      "---------\n",
      "Epoch 147/1000, Train Loss: 0.04315, lr: 0.000125\n",
      "Val Loss: 0.04619\n",
      "---------\n",
      "Epoch 148/1000, Train Loss: 0.04314, lr: 0.000125\n",
      "Val Loss: 0.04619\n",
      "---------\n",
      "Epoch 149/1000, Train Loss: 0.04313, lr: 0.000125\n",
      "Val Loss: 0.04619\n",
      "---------\n",
      "Epoch 150/1000, Train Loss: 0.04313, lr: 0.000125\n",
      "Val Loss: 0.04619\n",
      "---------\n",
      "Epoch 151/1000, Train Loss: 0.04312, lr: 0.000125\n",
      "Val Loss: 0.04619\n",
      "---------\n",
      "Epoch 152/1000, Train Loss: 0.04311, lr: 0.000125\n",
      "Val Loss: 0.04619\n",
      "---------\n",
      "Epoch 153/1000, Train Loss: 0.04310, lr: 0.000125\n",
      "Val Loss: 0.04619\n",
      "---------\n",
      "Epoch 154/1000, Train Loss: 0.04309, lr: 0.000125\n",
      "Val Loss: 0.04619\n",
      "---------\n",
      "Epoch 155/1000, Train Loss: 0.04309, lr: 0.000125\n",
      "Val Loss: 0.04619\n",
      "---------\n",
      "Epoch 156/1000, Train Loss: 0.04308, lr: 0.000125\n",
      "Val Loss: 0.04619\n",
      "---------\n",
      "Epoch 157/1000, Train Loss: 0.04307, lr: 0.000125\n",
      "Val Loss: 0.04619\n",
      "---------\n",
      "Epoch 158/1000, Train Loss: 0.04306, lr: 0.000125\n",
      "Val Loss: 0.04618\n",
      "---------\n",
      "Epoch 159/1000, Train Loss: 0.04305, lr: 0.000125\n",
      "Val Loss: 0.04618\n",
      "---------\n",
      "Epoch 160/1000, Train Loss: 0.04305, lr: 0.000125\n",
      "Val Loss: 0.04618\n",
      "---------\n",
      "Epoch 161/1000, Train Loss: 0.04304, lr: 0.000125\n",
      "Val Loss: 0.04617\n",
      "---------\n",
      "Epoch 162/1000, Train Loss: 0.04303, lr: 0.000125\n",
      "Val Loss: 0.04617\n",
      "---------\n",
      "Epoch 163/1000, Train Loss: 0.04302, lr: 0.000125\n",
      "Val Loss: 0.04617\n",
      "---------\n",
      "Epoch 164/1000, Train Loss: 0.04302, lr: 0.000125\n",
      "Val Loss: 0.04618\n",
      "---------\n",
      "Epoch 165/1000, Train Loss: 0.04301, lr: 0.000125\n",
      "Val Loss: 0.04617\n",
      "---------\n",
      "Epoch 166/1000, Train Loss: 0.04300, lr: 0.000125\n",
      "Val Loss: 0.04617\n",
      "---------\n",
      "Epoch 167/1000, Train Loss: 0.04299, lr: 0.000125\n",
      "Val Loss: 0.04617\n",
      "---------\n",
      "Epoch 168/1000, Train Loss: 0.04299, lr: 0.000125\n",
      "Val Loss: 0.04617\n",
      "---------\n",
      "Epoch 169/1000, Train Loss: 0.04298, lr: 0.000125\n",
      "Val Loss: 0.04617\n",
      "---------\n",
      "Epoch 170/1000, Train Loss: 0.04297, lr: 0.000125\n",
      "Val Loss: 0.04617\n",
      "---------\n",
      "Epoch 171/1000, Train Loss: 0.04297, lr: 0.000125\n",
      "Val Loss: 0.04617\n",
      "---------\n",
      "Epoch 172/1000, Train Loss: 0.04296, lr: 0.000125\n",
      "Val Loss: 0.04617\n",
      "---------\n",
      "Epoch 173/1000, Train Loss: 0.04295, lr: 0.000125\n",
      "Val Loss: 0.04617\n",
      "---------\n",
      "Epoch 174/1000, Train Loss: 0.04294, lr: 0.000125\n",
      "Val Loss: 0.04617\n",
      "---------\n",
      "Epoch 175/1000, Train Loss: 0.04294, lr: 0.000125\n",
      "Val Loss: 0.04617\n",
      "---------\n",
      "Epoch 176/1000, Train Loss: 0.04293, lr: 0.000125\n",
      "Val Loss: 0.04616\n",
      "---------\n",
      "Epoch 177/1000, Train Loss: 0.04292, lr: 0.000125\n",
      "Val Loss: 0.04616\n",
      "---------\n",
      "Epoch 178/1000, Train Loss: 0.04291, lr: 0.000125\n",
      "Val Loss: 0.04616\n",
      "---------\n",
      "Epoch 179/1000, Train Loss: 0.04291, lr: 0.000125\n",
      "Val Loss: 0.04616\n",
      "---------\n",
      "Epoch 180/1000, Train Loss: 0.04290, lr: 0.000125\n",
      "Val Loss: 0.04616\n",
      "---------\n",
      "Epoch 181/1000, Train Loss: 0.04289, lr: 0.000125\n",
      "Val Loss: 0.04616\n",
      "---------\n",
      "Epoch 182/1000, Train Loss: 0.04289, lr: 0.000125\n",
      "Val Loss: 0.04616\n",
      "---------\n",
      "Epoch 183/1000, Train Loss: 0.04288, lr: 0.000125\n",
      "Val Loss: 0.04615\n",
      "---------\n",
      "Epoch 184/1000, Train Loss: 0.04287, lr: 0.000125\n",
      "Val Loss: 0.04615\n",
      "---------\n",
      "Epoch 185/1000, Train Loss: 0.04287, lr: 0.000125\n",
      "Val Loss: 0.04615\n",
      "---------\n",
      "Epoch 186/1000, Train Loss: 0.04286, lr: 0.000125\n",
      "Val Loss: 0.04615\n",
      "---------\n",
      "Epoch 187/1000, Train Loss: 0.04285, lr: 0.000125\n",
      "Val Loss: 0.04615\n",
      "---------\n",
      "Epoch 188/1000, Train Loss: 0.04284, lr: 0.000125\n",
      "Val Loss: 0.04615\n",
      "---------\n",
      "Epoch 189/1000, Train Loss: 0.04284, lr: 0.000125\n",
      "Val Loss: 0.04615\n",
      "---------\n",
      "Epoch 190/1000, Train Loss: 0.04283, lr: 0.000125\n",
      "Val Loss: 0.04615\n",
      "---------\n",
      "Epoch 191/1000, Train Loss: 0.04282, lr: 0.000125\n",
      "Val Loss: 0.04615\n",
      "---------\n",
      "Epoch 192/1000, Train Loss: 0.04282, lr: 0.000125\n",
      "Val Loss: 0.04615\n",
      "---------\n",
      "Epoch 193/1000, Train Loss: 0.04281, lr: 0.000125\n",
      "Val Loss: 0.04615\n",
      "---------\n",
      "Epoch 194/1000, Train Loss: 0.04280, lr: 0.000125\n",
      "Val Loss: 0.04615\n",
      "---------\n",
      "Epoch 195/1000, Train Loss: 0.04280, lr: 0.000125\n",
      "Val Loss: 0.04615\n",
      "---------\n",
      "Epoch 196/1000, Train Loss: 0.04279, lr: 0.000125\n",
      "Val Loss: 0.04614\n",
      "---------\n",
      "Epoch 197/1000, Train Loss: 0.04278, lr: 0.000125\n",
      "Val Loss: 0.04614\n",
      "---------\n",
      "Epoch 198/1000, Train Loss: 0.04278, lr: 0.000125\n",
      "Val Loss: 0.04614\n",
      "---------\n",
      "Epoch 199/1000, Train Loss: 0.04277, lr: 0.000125\n",
      "Val Loss: 0.04614\n",
      "---------\n",
      "Epoch 200/1000, Train Loss: 0.04276, lr: 0.000125\n",
      "Val Loss: 0.04614\n",
      "---------\n",
      "Epoch 201/1000, Train Loss: 0.04275, lr: 0.000125\n",
      "Val Loss: 0.04614\n",
      "---------\n",
      "Epoch 202/1000, Train Loss: 0.04275, lr: 0.000125\n",
      "Val Loss: 0.04613\n",
      "---------\n",
      "Epoch 203/1000, Train Loss: 0.04274, lr: 0.000125\n",
      "Val Loss: 0.04614\n",
      "---------\n",
      "Epoch 204/1000, Train Loss: 0.04273, lr: 0.000125\n",
      "Val Loss: 0.04613\n",
      "---------\n",
      "Epoch 205/1000, Train Loss: 0.04273, lr: 0.000125\n",
      "Val Loss: 0.04613\n",
      "---------\n",
      "Epoch 206/1000, Train Loss: 0.04272, lr: 0.000125\n",
      "Val Loss: 0.04613\n",
      "---------\n",
      "Epoch 207/1000, Train Loss: 0.04271, lr: 0.000125\n",
      "Val Loss: 0.04613\n",
      "---------\n",
      "Epoch 208/1000, Train Loss: 0.04271, lr: 0.000125\n",
      "Val Loss: 0.04613\n",
      "---------\n",
      "Epoch 209/1000, Train Loss: 0.04270, lr: 0.000125\n",
      "Val Loss: 0.04613\n",
      "---------\n",
      "Epoch 210/1000, Train Loss: 0.04269, lr: 0.000125\n",
      "Val Loss: 0.04613\n",
      "---------\n",
      "Epoch 211/1000, Train Loss: 0.04269, lr: 0.000125\n",
      "Val Loss: 0.04613\n",
      "---------\n",
      "Epoch 212/1000, Train Loss: 0.04268, lr: 0.000125\n",
      "Val Loss: 0.04613\n",
      "---------\n",
      "Epoch 213/1000, Train Loss: 0.04267, lr: 0.000125\n",
      "Val Loss: 0.04613\n",
      "---------\n",
      "Epoch 214/1000, Train Loss: 0.04267, lr: 0.000125\n",
      "Val Loss: 0.04612\n",
      "---------\n",
      "Epoch 215/1000, Train Loss: 0.04266, lr: 0.000125\n",
      "Val Loss: 0.04613\n",
      "---------\n",
      "Epoch 216/1000, Train Loss: 0.04266, lr: 0.000125\n",
      "Val Loss: 0.04612\n",
      "---------\n",
      "Epoch 217/1000, Train Loss: 0.04265, lr: 0.000125\n",
      "Val Loss: 0.04612\n",
      "---------\n",
      "Epoch 218/1000, Train Loss: 0.04264, lr: 0.000125\n",
      "Val Loss: 0.04612\n",
      "---------\n",
      "Epoch 219/1000, Train Loss: 0.04264, lr: 0.000125\n",
      "Val Loss: 0.04612\n",
      "---------\n",
      "Epoch 220/1000, Train Loss: 0.04263, lr: 0.000125\n",
      "Val Loss: 0.04612\n",
      "---------\n",
      "Epoch 221/1000, Train Loss: 0.04262, lr: 0.000125\n",
      "Val Loss: 0.04612\n",
      "---------\n",
      "Epoch 222/1000, Train Loss: 0.04262, lr: 0.000125\n",
      "Val Loss: 0.04612\n",
      "---------\n",
      "Epoch 223/1000, Train Loss: 0.04261, lr: 0.000125\n",
      "Val Loss: 0.04612\n",
      "---------\n",
      "Epoch 224/1000, Train Loss: 0.04260, lr: 0.000125\n",
      "Val Loss: 0.04612\n",
      "---------\n",
      "Epoch 225/1000, Train Loss: 0.04260, lr: 0.000125\n",
      "Val Loss: 0.04612\n",
      "---------\n",
      "Epoch 226/1000, Train Loss: 0.04259, lr: 0.000125\n",
      "Val Loss: 0.04612\n",
      "---------\n",
      "Epoch 227/1000, Train Loss: 0.04258, lr: 0.000125\n",
      "Val Loss: 0.04612\n",
      "---------\n",
      "Epoch 228/1000, Train Loss: 0.04258, lr: 0.000125\n",
      "Val Loss: 0.04612\n",
      "---------\n",
      "Epoch 229/1000, Train Loss: 0.04257, lr: 0.000125\n",
      "Val Loss: 0.04611\n",
      "---------\n",
      "Epoch 230/1000, Train Loss: 0.04257, lr: 0.000125\n",
      "Val Loss: 0.04611\n",
      "---------\n",
      "Epoch 231/1000, Train Loss: 0.04256, lr: 0.000125\n",
      "Val Loss: 0.04611\n",
      "---------\n",
      "Epoch 232/1000, Train Loss: 0.04255, lr: 0.000125\n",
      "Val Loss: 0.04611\n",
      "---------\n",
      "Epoch 233/1000, Train Loss: 0.04255, lr: 0.000125\n",
      "Val Loss: 0.04611\n",
      "---------\n",
      "Epoch 234/1000, Train Loss: 0.04254, lr: 0.000125\n",
      "Val Loss: 0.04611\n",
      "---------\n",
      "Epoch 235/1000, Train Loss: 0.04253, lr: 0.000125\n",
      "Val Loss: 0.04611\n",
      "---------\n",
      "Epoch 236/1000, Train Loss: 0.04253, lr: 0.000125\n",
      "Val Loss: 0.04610\n",
      "---------\n",
      "Epoch 237/1000, Train Loss: 0.04252, lr: 0.000125\n",
      "Val Loss: 0.04610\n",
      "---------\n",
      "Epoch 238/1000, Train Loss: 0.04252, lr: 0.000125\n",
      "Val Loss: 0.04610\n",
      "---------\n",
      "Epoch 239/1000, Train Loss: 0.04251, lr: 0.000125\n",
      "Val Loss: 0.04610\n",
      "---------\n",
      "Epoch 240/1000, Train Loss: 0.04250, lr: 0.000125\n",
      "Val Loss: 0.04610\n",
      "---------\n",
      "Epoch 241/1000, Train Loss: 0.04250, lr: 0.000125\n",
      "Val Loss: 0.04610\n",
      "---------\n",
      "Epoch 242/1000, Train Loss: 0.04249, lr: 0.000125\n",
      "Val Loss: 0.04609\n",
      "---------\n",
      "Epoch 243/1000, Train Loss: 0.04248, lr: 0.000125\n",
      "Val Loss: 0.04609\n",
      "---------\n",
      "Epoch 244/1000, Train Loss: 0.04248, lr: 0.000125\n",
      "Val Loss: 0.04609\n",
      "---------\n",
      "Epoch 245/1000, Train Loss: 0.04247, lr: 0.000125\n",
      "Val Loss: 0.04609\n",
      "---------\n",
      "Epoch 246/1000, Train Loss: 0.04246, lr: 0.000125\n",
      "Val Loss: 0.04609\n",
      "---------\n",
      "Epoch 247/1000, Train Loss: 0.04246, lr: 0.000125\n",
      "Val Loss: 0.04609\n",
      "---------\n",
      "Epoch 248/1000, Train Loss: 0.04245, lr: 0.000125\n",
      "Val Loss: 0.04609\n",
      "---------\n",
      "Epoch 249/1000, Train Loss: 0.04244, lr: 0.000125\n",
      "Val Loss: 0.04609\n",
      "---------\n",
      "Epoch 250/1000, Train Loss: 0.04244, lr: 0.000125\n",
      "Val Loss: 0.04609\n",
      "---------\n",
      "Epoch 251/1000, Train Loss: 0.04243, lr: 0.000125\n",
      "Val Loss: 0.04609\n",
      "---------\n",
      "Epoch 252/1000, Train Loss: 0.04242, lr: 0.000125\n",
      "Val Loss: 0.04609\n",
      "---------\n",
      "Epoch 253/1000, Train Loss: 0.04242, lr: 0.000125\n",
      "Val Loss: 0.04608\n",
      "---------\n",
      "Epoch 254/1000, Train Loss: 0.04241, lr: 0.000125\n",
      "Val Loss: 0.04608\n",
      "---------\n",
      "Epoch 255/1000, Train Loss: 0.04240, lr: 0.000125\n",
      "Val Loss: 0.04608\n",
      "---------\n",
      "Epoch 256/1000, Train Loss: 0.04240, lr: 0.000125\n",
      "Val Loss: 0.04608\n",
      "---------\n",
      "Epoch 257/1000, Train Loss: 0.04239, lr: 0.000125\n",
      "Val Loss: 0.04608\n",
      "---------\n",
      "Epoch 258/1000, Train Loss: 0.04239, lr: 0.000125\n",
      "Val Loss: 0.04607\n",
      "---------\n",
      "Epoch 259/1000, Train Loss: 0.04238, lr: 0.000125\n",
      "Val Loss: 0.04607\n",
      "---------\n",
      "Epoch 260/1000, Train Loss: 0.04237, lr: 0.000125\n",
      "Val Loss: 0.04607\n",
      "---------\n",
      "Epoch 261/1000, Train Loss: 0.04237, lr: 0.000125\n",
      "Val Loss: 0.04607\n",
      "---------\n",
      "Epoch 262/1000, Train Loss: 0.04236, lr: 0.000125\n",
      "Val Loss: 0.04607\n",
      "---------\n",
      "Epoch 263/1000, Train Loss: 0.04235, lr: 0.000125\n",
      "Val Loss: 0.04607\n",
      "---------\n",
      "Epoch 264/1000, Train Loss: 0.04235, lr: 0.000125\n",
      "Val Loss: 0.04607\n",
      "---------\n",
      "Epoch 265/1000, Train Loss: 0.04234, lr: 0.000125\n",
      "Val Loss: 0.04607\n",
      "---------\n",
      "Epoch 266/1000, Train Loss: 0.04233, lr: 0.000125\n",
      "Val Loss: 0.04606\n",
      "---------\n",
      "Epoch 267/1000, Train Loss: 0.04233, lr: 0.000125\n",
      "Val Loss: 0.04606\n",
      "---------\n",
      "Epoch 268/1000, Train Loss: 0.04232, lr: 0.000125\n",
      "Val Loss: 0.04606\n",
      "---------\n",
      "Epoch 269/1000, Train Loss: 0.04231, lr: 0.000125\n",
      "Val Loss: 0.04606\n",
      "---------\n",
      "Epoch 270/1000, Train Loss: 0.04231, lr: 0.000125\n",
      "Val Loss: 0.04606\n",
      "---------\n",
      "Epoch 271/1000, Train Loss: 0.04230, lr: 0.000125\n",
      "Val Loss: 0.04605\n",
      "---------\n",
      "Epoch 272/1000, Train Loss: 0.04229, lr: 0.000125\n",
      "Val Loss: 0.04605\n",
      "---------\n",
      "Epoch 273/1000, Train Loss: 0.04229, lr: 0.000125\n",
      "Val Loss: 0.04605\n",
      "---------\n",
      "Epoch 274/1000, Train Loss: 0.04228, lr: 0.000125\n",
      "Val Loss: 0.04605\n",
      "---------\n",
      "Epoch 275/1000, Train Loss: 0.04227, lr: 0.000125\n",
      "Val Loss: 0.04605\n",
      "---------\n",
      "Epoch 276/1000, Train Loss: 0.04227, lr: 0.000125\n",
      "Val Loss: 0.04605\n",
      "---------\n",
      "Epoch 277/1000, Train Loss: 0.04226, lr: 0.000125\n",
      "Val Loss: 0.04605\n",
      "---------\n",
      "Epoch 278/1000, Train Loss: 0.04225, lr: 0.000125\n",
      "Val Loss: 0.04604\n",
      "---------\n",
      "Epoch 279/1000, Train Loss: 0.04225, lr: 0.000125\n",
      "Val Loss: 0.04604\n",
      "---------\n",
      "Epoch 280/1000, Train Loss: 0.04224, lr: 0.000125\n",
      "Val Loss: 0.04604\n",
      "---------\n",
      "Epoch 281/1000, Train Loss: 0.04223, lr: 0.000125\n",
      "Val Loss: 0.04604\n",
      "---------\n",
      "Epoch 282/1000, Train Loss: 0.04223, lr: 0.000125\n",
      "Val Loss: 0.04604\n",
      "---------\n",
      "Epoch 283/1000, Train Loss: 0.04222, lr: 0.000125\n",
      "Val Loss: 0.04603\n",
      "---------\n",
      "Epoch 284/1000, Train Loss: 0.04221, lr: 0.000125\n",
      "Val Loss: 0.04603\n",
      "---------\n",
      "Epoch 285/1000, Train Loss: 0.04221, lr: 0.000125\n",
      "Val Loss: 0.04603\n",
      "---------\n",
      "Epoch 286/1000, Train Loss: 0.04220, lr: 0.000125\n",
      "Val Loss: 0.04603\n",
      "---------\n",
      "Epoch 287/1000, Train Loss: 0.04219, lr: 0.000125\n",
      "Val Loss: 0.04603\n",
      "---------\n",
      "Epoch 288/1000, Train Loss: 0.04218, lr: 0.000125\n",
      "Val Loss: 0.04602\n",
      "---------\n",
      "Epoch 289/1000, Train Loss: 0.04218, lr: 0.000125\n",
      "Val Loss: 0.04602\n",
      "---------\n",
      "Epoch 290/1000, Train Loss: 0.04217, lr: 0.000125\n",
      "Val Loss: 0.04602\n",
      "---------\n",
      "Epoch 291/1000, Train Loss: 0.04216, lr: 0.000125\n",
      "Val Loss: 0.04602\n",
      "---------\n",
      "Epoch 292/1000, Train Loss: 0.04216, lr: 0.000125\n",
      "Val Loss: 0.04601\n",
      "---------\n",
      "Epoch 293/1000, Train Loss: 0.04215, lr: 0.000125\n",
      "Val Loss: 0.04601\n",
      "---------\n",
      "Epoch 294/1000, Train Loss: 0.04214, lr: 0.000125\n",
      "Val Loss: 0.04601\n",
      "---------\n",
      "Epoch 295/1000, Train Loss: 0.04213, lr: 0.000125\n",
      "Val Loss: 0.04600\n",
      "---------\n",
      "Epoch 296/1000, Train Loss: 0.04213, lr: 0.000125\n",
      "Val Loss: 0.04600\n",
      "---------\n",
      "Epoch 297/1000, Train Loss: 0.04212, lr: 0.000125\n",
      "Val Loss: 0.04600\n",
      "---------\n",
      "Epoch 298/1000, Train Loss: 0.04211, lr: 0.000125\n",
      "Val Loss: 0.04600\n",
      "---------\n",
      "Epoch 299/1000, Train Loss: 0.04210, lr: 0.000125\n",
      "Val Loss: 0.04599\n",
      "---------\n",
      "Epoch 300/1000, Train Loss: 0.04210, lr: 0.000125\n",
      "Val Loss: 0.04599\n",
      "---------\n",
      "Epoch 301/1000, Train Loss: 0.04209, lr: 0.000125\n",
      "Val Loss: 0.04599\n",
      "---------\n",
      "Epoch 302/1000, Train Loss: 0.04208, lr: 0.000125\n",
      "Val Loss: 0.04598\n",
      "---------\n",
      "Epoch 303/1000, Train Loss: 0.04207, lr: 0.000125\n",
      "Val Loss: 0.04597\n",
      "---------\n",
      "Epoch 304/1000, Train Loss: 0.04206, lr: 0.000125\n",
      "Val Loss: 0.04597\n",
      "---------\n",
      "Epoch 305/1000, Train Loss: 0.04205, lr: 0.000125\n",
      "Val Loss: 0.04596\n",
      "---------\n",
      "Epoch 306/1000, Train Loss: 0.04204, lr: 0.000125\n",
      "Val Loss: 0.04596\n",
      "---------\n",
      "Epoch 307/1000, Train Loss: 0.04204, lr: 0.000125\n",
      "Val Loss: 0.04595\n",
      "---------\n",
      "Epoch 308/1000, Train Loss: 0.04203, lr: 0.000125\n",
      "Val Loss: 0.04594\n",
      "---------\n",
      "Epoch 309/1000, Train Loss: 0.04202, lr: 0.000125\n",
      "Val Loss: 0.04594\n",
      "---------\n",
      "Epoch 310/1000, Train Loss: 0.04201, lr: 0.000125\n",
      "Val Loss: 0.04593\n",
      "---------\n",
      "Epoch 311/1000, Train Loss: 0.04200, lr: 0.000125\n",
      "Val Loss: 0.04592\n",
      "---------\n",
      "Epoch 312/1000, Train Loss: 0.04199, lr: 0.000125\n",
      "Val Loss: 0.04591\n",
      "---------\n",
      "Epoch 313/1000, Train Loss: 0.04198, lr: 0.000125\n",
      "Val Loss: 0.04591\n",
      "---------\n",
      "Epoch 314/1000, Train Loss: 0.04196, lr: 0.000125\n",
      "Val Loss: 0.04590\n",
      "---------\n",
      "Epoch 315/1000, Train Loss: 0.04195, lr: 0.000125\n",
      "Val Loss: 0.04589\n",
      "---------\n",
      "Epoch 316/1000, Train Loss: 0.04194, lr: 0.000125\n",
      "Val Loss: 0.04588\n",
      "---------\n",
      "Epoch 317/1000, Train Loss: 0.04193, lr: 0.000125\n",
      "Val Loss: 0.04587\n",
      "---------\n",
      "Epoch 318/1000, Train Loss: 0.04192, lr: 0.000125\n",
      "Val Loss: 0.04586\n",
      "---------\n",
      "Epoch 319/1000, Train Loss: 0.04190, lr: 0.000125\n",
      "Val Loss: 0.04585\n",
      "---------\n",
      "Epoch 320/1000, Train Loss: 0.04189, lr: 0.000125\n",
      "Val Loss: 0.04583\n",
      "---------\n",
      "Epoch 321/1000, Train Loss: 0.04188, lr: 0.000125\n",
      "Val Loss: 0.04582\n",
      "---------\n",
      "Epoch 322/1000, Train Loss: 0.04186, lr: 0.000125\n",
      "Val Loss: 0.04580\n",
      "---------\n",
      "Epoch 323/1000, Train Loss: 0.04184, lr: 0.000125\n",
      "Val Loss: 0.04579\n",
      "---------\n",
      "Epoch 324/1000, Train Loss: 0.04183, lr: 0.000125\n",
      "Val Loss: 0.04577\n",
      "---------\n",
      "Epoch 325/1000, Train Loss: 0.04181, lr: 0.000125\n",
      "Val Loss: 0.04575\n",
      "---------\n",
      "Epoch 326/1000, Train Loss: 0.04179, lr: 0.000125\n",
      "Val Loss: 0.04573\n",
      "---------\n",
      "Epoch 327/1000, Train Loss: 0.04177, lr: 0.000125\n",
      "Val Loss: 0.04572\n",
      "---------\n",
      "Epoch 328/1000, Train Loss: 0.04175, lr: 0.000125\n",
      "Val Loss: 0.04570\n",
      "---------\n",
      "Epoch 329/1000, Train Loss: 0.04173, lr: 0.000125\n",
      "Val Loss: 0.04567\n",
      "---------\n",
      "Epoch 330/1000, Train Loss: 0.04171, lr: 0.000125\n",
      "Val Loss: 0.04565\n",
      "---------\n",
      "Epoch 331/1000, Train Loss: 0.04169, lr: 0.000125\n",
      "Val Loss: 0.04563\n",
      "---------\n",
      "Epoch 332/1000, Train Loss: 0.04167, lr: 0.000125\n",
      "Val Loss: 0.04560\n",
      "---------\n",
      "Epoch 333/1000, Train Loss: 0.04164, lr: 0.000125\n",
      "Val Loss: 0.04558\n",
      "---------\n",
      "Epoch 334/1000, Train Loss: 0.04162, lr: 0.000125\n",
      "Val Loss: 0.04556\n",
      "---------\n",
      "Epoch 335/1000, Train Loss: 0.04159, lr: 0.000125\n",
      "Val Loss: 0.04553\n",
      "---------\n",
      "Epoch 336/1000, Train Loss: 0.04157, lr: 0.000125\n",
      "Val Loss: 0.04551\n",
      "---------\n",
      "Epoch 337/1000, Train Loss: 0.04154, lr: 0.000125\n",
      "Val Loss: 0.04548\n",
      "---------\n",
      "Epoch 338/1000, Train Loss: 0.04151, lr: 0.000125\n",
      "Val Loss: 0.04546\n",
      "---------\n",
      "Epoch 339/1000, Train Loss: 0.04149, lr: 0.000125\n",
      "Val Loss: 0.04543\n",
      "---------\n",
      "Epoch 340/1000, Train Loss: 0.04146, lr: 0.000125\n",
      "Val Loss: 0.04541\n",
      "---------\n",
      "Epoch 341/1000, Train Loss: 0.04144, lr: 0.000125\n",
      "Val Loss: 0.04538\n",
      "---------\n",
      "Epoch 342/1000, Train Loss: 0.04141, lr: 0.000125\n",
      "Val Loss: 0.04536\n",
      "---------\n",
      "Epoch 343/1000, Train Loss: 0.04139, lr: 0.000125\n",
      "Val Loss: 0.04534\n",
      "---------\n",
      "Epoch 344/1000, Train Loss: 0.04136, lr: 0.000125\n",
      "Val Loss: 0.04532\n",
      "---------\n",
      "Epoch 345/1000, Train Loss: 0.04134, lr: 0.000125\n",
      "Val Loss: 0.04530\n",
      "---------\n",
      "Epoch 346/1000, Train Loss: 0.04132, lr: 0.000125\n",
      "Val Loss: 0.04528\n",
      "---------\n",
      "Epoch 347/1000, Train Loss: 0.04130, lr: 0.000125\n",
      "Val Loss: 0.04526\n",
      "---------\n",
      "Epoch 348/1000, Train Loss: 0.04128, lr: 0.000125\n",
      "Val Loss: 0.04524\n",
      "---------\n",
      "Epoch 349/1000, Train Loss: 0.04126, lr: 0.000125\n",
      "Val Loss: 0.04523\n",
      "---------\n",
      "Epoch 350/1000, Train Loss: 0.04124, lr: 0.000125\n",
      "Val Loss: 0.04521\n",
      "---------\n",
      "Epoch 351/1000, Train Loss: 0.04122, lr: 0.000125\n",
      "Val Loss: 0.04519\n",
      "---------\n",
      "Epoch 352/1000, Train Loss: 0.04120, lr: 0.000125\n",
      "Val Loss: 0.04518\n",
      "---------\n",
      "Epoch 353/1000, Train Loss: 0.04119, lr: 0.000125\n",
      "Val Loss: 0.04517\n",
      "---------\n",
      "Epoch 354/1000, Train Loss: 0.04117, lr: 0.000125\n",
      "Val Loss: 0.04515\n",
      "---------\n",
      "Epoch 355/1000, Train Loss: 0.04115, lr: 0.000125\n",
      "Val Loss: 0.04514\n",
      "---------\n",
      "Epoch 356/1000, Train Loss: 0.04114, lr: 0.000125\n",
      "Val Loss: 0.04513\n",
      "---------\n",
      "Epoch 357/1000, Train Loss: 0.04112, lr: 0.000125\n",
      "Val Loss: 0.04512\n",
      "---------\n",
      "Epoch 358/1000, Train Loss: 0.04111, lr: 0.000125\n",
      "Val Loss: 0.04511\n",
      "---------\n",
      "Epoch 359/1000, Train Loss: 0.04109, lr: 0.000125\n",
      "Val Loss: 0.04509\n",
      "---------\n",
      "Epoch 360/1000, Train Loss: 0.04108, lr: 0.000125\n",
      "Val Loss: 0.04509\n",
      "---------\n",
      "Epoch 361/1000, Train Loss: 0.04107, lr: 0.000125\n",
      "Val Loss: 0.04507\n",
      "---------\n",
      "Epoch 362/1000, Train Loss: 0.04106, lr: 0.000125\n",
      "Val Loss: 0.04507\n",
      "---------\n",
      "Epoch 363/1000, Train Loss: 0.04104, lr: 0.000125\n",
      "Val Loss: 0.04506\n",
      "---------\n",
      "Epoch 364/1000, Train Loss: 0.04103, lr: 0.000125\n",
      "Val Loss: 0.04505\n",
      "---------\n",
      "Epoch 365/1000, Train Loss: 0.04102, lr: 0.000125\n",
      "Val Loss: 0.04504\n",
      "---------\n",
      "Epoch 366/1000, Train Loss: 0.04101, lr: 0.000125\n",
      "Val Loss: 0.04504\n",
      "---------\n",
      "Epoch 367/1000, Train Loss: 0.04100, lr: 0.000125\n",
      "Val Loss: 0.04503\n",
      "---------\n",
      "Epoch 368/1000, Train Loss: 0.04099, lr: 0.000125\n",
      "Val Loss: 0.04502\n",
      "---------\n",
      "Epoch 369/1000, Train Loss: 0.04098, lr: 0.000125\n",
      "Val Loss: 0.04502\n",
      "---------\n",
      "Epoch 370/1000, Train Loss: 0.04097, lr: 0.000125\n",
      "Val Loss: 0.04501\n",
      "---------\n",
      "Epoch 371/1000, Train Loss: 0.04095, lr: 0.000125\n",
      "Val Loss: 0.04500\n",
      "---------\n",
      "Epoch 372/1000, Train Loss: 0.04095, lr: 0.000125\n",
      "Val Loss: 0.04499\n",
      "---------\n",
      "Epoch 373/1000, Train Loss: 0.04094, lr: 0.000125\n",
      "Val Loss: 0.04499\n",
      "---------\n",
      "Epoch 374/1000, Train Loss: 0.04093, lr: 0.000125\n",
      "Val Loss: 0.04498\n",
      "---------\n",
      "Epoch 375/1000, Train Loss: 0.04092, lr: 0.000125\n",
      "Val Loss: 0.04498\n",
      "---------\n",
      "Epoch 376/1000, Train Loss: 0.04091, lr: 0.000125\n",
      "Val Loss: 0.04497\n",
      "---------\n",
      "Epoch 377/1000, Train Loss: 0.04090, lr: 0.000125\n",
      "Val Loss: 0.04496\n",
      "---------\n",
      "Epoch 378/1000, Train Loss: 0.04089, lr: 0.000125\n",
      "Val Loss: 0.04496\n",
      "---------\n",
      "Epoch 379/1000, Train Loss: 0.04088, lr: 0.000125\n",
      "Val Loss: 0.04495\n",
      "---------\n",
      "Epoch 380/1000, Train Loss: 0.04087, lr: 0.000125\n",
      "Val Loss: 0.04495\n",
      "---------\n",
      "Epoch 381/1000, Train Loss: 0.04086, lr: 0.000125\n",
      "Val Loss: 0.04494\n",
      "---------\n",
      "Epoch 382/1000, Train Loss: 0.04086, lr: 0.000125\n",
      "Val Loss: 0.04494\n",
      "---------\n",
      "Epoch 383/1000, Train Loss: 0.04085, lr: 0.000125\n",
      "Val Loss: 0.04493\n",
      "---------\n",
      "Epoch 384/1000, Train Loss: 0.04084, lr: 0.000125\n",
      "Val Loss: 0.04493\n",
      "---------\n",
      "Epoch 385/1000, Train Loss: 0.04083, lr: 0.000125\n",
      "Val Loss: 0.04492\n",
      "---------\n",
      "Epoch 386/1000, Train Loss: 0.04082, lr: 0.000125\n",
      "Val Loss: 0.04492\n",
      "---------\n",
      "Epoch 387/1000, Train Loss: 0.04082, lr: 0.000125\n",
      "Val Loss: 0.04491\n",
      "---------\n",
      "Epoch 388/1000, Train Loss: 0.04081, lr: 0.000125\n",
      "Val Loss: 0.04491\n",
      "---------\n",
      "Epoch 389/1000, Train Loss: 0.04080, lr: 0.000125\n",
      "Val Loss: 0.04490\n",
      "---------\n",
      "Epoch 390/1000, Train Loss: 0.04079, lr: 0.000125\n",
      "Val Loss: 0.04489\n",
      "---------\n",
      "Epoch 391/1000, Train Loss: 0.04079, lr: 0.000125\n",
      "Val Loss: 0.04489\n",
      "---------\n",
      "Epoch 392/1000, Train Loss: 0.04078, lr: 0.000125\n",
      "Val Loss: 0.04489\n",
      "---------\n",
      "Epoch 393/1000, Train Loss: 0.04077, lr: 0.000125\n",
      "Val Loss: 0.04489\n",
      "---------\n",
      "Epoch 394/1000, Train Loss: 0.04077, lr: 0.000125\n",
      "Val Loss: 0.04488\n",
      "---------\n",
      "Epoch 395/1000, Train Loss: 0.04076, lr: 0.000125\n",
      "Val Loss: 0.04488\n",
      "---------\n",
      "Epoch 396/1000, Train Loss: 0.04075, lr: 0.000125\n",
      "Val Loss: 0.04488\n",
      "---------\n",
      "Epoch 397/1000, Train Loss: 0.04075, lr: 0.000125\n",
      "Val Loss: 0.04488\n",
      "---------\n",
      "Epoch 398/1000, Train Loss: 0.04074, lr: 0.000125\n",
      "Val Loss: 0.04487\n",
      "---------\n",
      "Epoch 399/1000, Train Loss: 0.04073, lr: 0.000125\n",
      "Val Loss: 0.04487\n",
      "---------\n",
      "Epoch 400/1000, Train Loss: 0.04073, lr: 0.000125\n",
      "Val Loss: 0.04487\n",
      "---------\n",
      "Epoch 401/1000, Train Loss: 0.04072, lr: 0.000125\n",
      "Val Loss: 0.04487\n",
      "---------\n",
      "Epoch 402/1000, Train Loss: 0.04072, lr: 0.000125\n",
      "Val Loss: 0.04486\n",
      "---------\n",
      "Epoch 403/1000, Train Loss: 0.04071, lr: 0.000125\n",
      "Val Loss: 0.04486\n",
      "---------\n",
      "Epoch 404/1000, Train Loss: 0.04070, lr: 0.000125\n",
      "Val Loss: 0.04486\n",
      "---------\n",
      "Epoch 405/1000, Train Loss: 0.04070, lr: 0.000125\n",
      "Val Loss: 0.04486\n",
      "---------\n",
      "Epoch 406/1000, Train Loss: 0.04069, lr: 0.000125\n",
      "Val Loss: 0.04485\n",
      "---------\n",
      "Epoch 407/1000, Train Loss: 0.04069, lr: 0.000125\n",
      "Val Loss: 0.04485\n",
      "---------\n",
      "Epoch 408/1000, Train Loss: 0.04068, lr: 0.000125\n",
      "Val Loss: 0.04485\n",
      "---------\n",
      "Epoch 409/1000, Train Loss: 0.04068, lr: 0.000125\n",
      "Val Loss: 0.04484\n",
      "---------\n",
      "Epoch 410/1000, Train Loss: 0.04067, lr: 0.000125\n",
      "Val Loss: 0.04484\n",
      "---------\n",
      "Epoch 411/1000, Train Loss: 0.04066, lr: 0.000125\n",
      "Val Loss: 0.04484\n",
      "---------\n",
      "Epoch 412/1000, Train Loss: 0.04066, lr: 0.000125\n",
      "Val Loss: 0.04484\n",
      "---------\n",
      "Epoch 413/1000, Train Loss: 0.04065, lr: 0.000125\n",
      "Val Loss: 0.04483\n",
      "---------\n",
      "Epoch 414/1000, Train Loss: 0.04065, lr: 0.000125\n",
      "Val Loss: 0.04483\n",
      "---------\n",
      "Epoch 415/1000, Train Loss: 0.04064, lr: 0.000125\n",
      "Val Loss: 0.04483\n",
      "---------\n",
      "Epoch 416/1000, Train Loss: 0.04064, lr: 0.000125\n",
      "Val Loss: 0.04483\n",
      "---------\n",
      "Epoch 417/1000, Train Loss: 0.04063, lr: 0.000125\n",
      "Val Loss: 0.04483\n",
      "---------\n",
      "Epoch 418/1000, Train Loss: 0.04063, lr: 0.000125\n",
      "Val Loss: 0.04483\n",
      "---------\n",
      "Epoch 419/1000, Train Loss: 0.04062, lr: 0.000125\n",
      "Val Loss: 0.04482\n",
      "---------\n",
      "Epoch 420/1000, Train Loss: 0.04062, lr: 0.000125\n",
      "Val Loss: 0.04482\n",
      "---------\n",
      "Epoch 421/1000, Train Loss: 0.04061, lr: 0.000125\n",
      "Val Loss: 0.04482\n",
      "---------\n",
      "Epoch 422/1000, Train Loss: 0.04061, lr: 0.000125\n",
      "Val Loss: 0.04482\n",
      "---------\n",
      "Epoch 423/1000, Train Loss: 0.04060, lr: 0.000125\n",
      "Val Loss: 0.04482\n",
      "---------\n",
      "Epoch 424/1000, Train Loss: 0.04060, lr: 0.000125\n",
      "Val Loss: 0.04482\n",
      "---------\n",
      "Epoch 425/1000, Train Loss: 0.04059, lr: 0.000125\n",
      "Val Loss: 0.04482\n",
      "---------\n",
      "Epoch 426/1000, Train Loss: 0.04059, lr: 0.000125\n",
      "Val Loss: 0.04481\n",
      "---------\n",
      "Epoch 427/1000, Train Loss: 0.04058, lr: 0.000125\n",
      "Val Loss: 0.04481\n",
      "---------\n",
      "Epoch 428/1000, Train Loss: 0.04058, lr: 0.000125\n",
      "Val Loss: 0.04482\n",
      "---------\n",
      "Epoch 429/1000, Train Loss: 0.04057, lr: 0.000125\n",
      "Val Loss: 0.04481\n",
      "---------\n",
      "Epoch 430/1000, Train Loss: 0.04057, lr: 0.000125\n",
      "Val Loss: 0.04481\n",
      "---------\n",
      "Epoch 431/1000, Train Loss: 0.04056, lr: 0.000125\n",
      "Val Loss: 0.04481\n",
      "---------\n",
      "Epoch 432/1000, Train Loss: 0.04056, lr: 0.000125\n",
      "Val Loss: 0.04481\n",
      "---------\n",
      "Epoch 433/1000, Train Loss: 0.04055, lr: 0.000125\n",
      "Val Loss: 0.04481\n",
      "---------\n",
      "Epoch 434/1000, Train Loss: 0.04055, lr: 0.000125\n",
      "Val Loss: 0.04481\n",
      "---------\n",
      "Epoch 435/1000, Train Loss: 0.04054, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 436/1000, Train Loss: 0.04054, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 437/1000, Train Loss: 0.04053, lr: 0.000125\n",
      "Val Loss: 0.04481\n",
      "---------\n",
      "Epoch 438/1000, Train Loss: 0.04053, lr: 0.000125\n",
      "Val Loss: 0.04481\n",
      "---------\n",
      "Epoch 439/1000, Train Loss: 0.04052, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 440/1000, Train Loss: 0.04052, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 441/1000, Train Loss: 0.04052, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 442/1000, Train Loss: 0.04051, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 443/1000, Train Loss: 0.04051, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 444/1000, Train Loss: 0.04050, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 445/1000, Train Loss: 0.04050, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 446/1000, Train Loss: 0.04049, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 447/1000, Train Loss: 0.04049, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 448/1000, Train Loss: 0.04048, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 449/1000, Train Loss: 0.04048, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 450/1000, Train Loss: 0.04047, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 451/1000, Train Loss: 0.04047, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 452/1000, Train Loss: 0.04047, lr: 0.000125\n",
      "Val Loss: 0.04479\n",
      "---------\n",
      "Epoch 453/1000, Train Loss: 0.04046, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 454/1000, Train Loss: 0.04046, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 455/1000, Train Loss: 0.04045, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 456/1000, Train Loss: 0.04045, lr: 0.000125\n",
      "Val Loss: 0.04479\n",
      "---------\n",
      "Epoch 457/1000, Train Loss: 0.04045, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 458/1000, Train Loss: 0.04044, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 459/1000, Train Loss: 0.04044, lr: 0.000125\n",
      "Val Loss: 0.04479\n",
      "---------\n",
      "Epoch 460/1000, Train Loss: 0.04043, lr: 0.000125\n",
      "Val Loss: 0.04479\n",
      "---------\n",
      "Epoch 461/1000, Train Loss: 0.04043, lr: 0.000125\n",
      "Val Loss: 0.04480\n",
      "---------\n",
      "Epoch 462/1000, Train Loss: 0.04043, lr: 0.000125\n",
      "Val Loss: 0.04479\n",
      "---------\n",
      "Epoch 463/1000, Train Loss: 0.04042, lr: 0.000125\n",
      "Val Loss: 0.04479\n",
      "---------\n",
      "Epoch 464/1000, Train Loss: 0.04042, lr: 0.000125\n",
      "Val Loss: 0.04479\n",
      "---------\n",
      "Epoch 465/1000, Train Loss: 0.04041, lr: 0.000125\n",
      "Val Loss: 0.04479\n",
      "---------\n",
      "Epoch 466/1000, Train Loss: 0.04041, lr: 0.000125\n",
      "Val Loss: 0.04479\n",
      "---------\n",
      "Epoch 467/1000, Train Loss: 0.04040, lr: 0.000125\n",
      "Val Loss: 0.04479\n",
      "---------\n",
      "Epoch 468/1000, Train Loss: 0.04040, lr: 0.000125\n",
      "Val Loss: 0.04479\n",
      "---------\n",
      "Epoch 469/1000, Train Loss: 0.04039, lr: 0.000125\n",
      "Val Loss: 0.04479\n",
      "---------\n",
      "Epoch 470/1000, Train Loss: 0.04039, lr: 0.000125\n",
      "Val Loss: 0.04479\n",
      "---------\n",
      "Epoch 471/1000, Train Loss: 0.04039, lr: 0.000125\n",
      "Val Loss: 0.04479\n",
      "---------\n",
      "Epoch 472/1000, Train Loss: 0.04038, lr: 0.000125\n",
      "Val Loss: 0.04478\n",
      "---------\n",
      "Epoch 473/1000, Train Loss: 0.04038, lr: 0.000125\n",
      "Val Loss: 0.04478\n",
      "---------\n",
      "Epoch 474/1000, Train Loss: 0.04037, lr: 0.000125\n",
      "Val Loss: 0.04478\n",
      "---------\n",
      "Epoch 475/1000, Train Loss: 0.04037, lr: 0.000125\n",
      "Val Loss: 0.04478\n",
      "---------\n",
      "Epoch 476/1000, Train Loss: 0.04037, lr: 0.000125\n",
      "Val Loss: 0.04478\n",
      "---------\n",
      "Epoch 477/1000, Train Loss: 0.04036, lr: 0.000125\n",
      "Val Loss: 0.04478\n",
      "---------\n",
      "Epoch 478/1000, Train Loss: 0.04036, lr: 0.000125\n",
      "Val Loss: 0.04478\n",
      "---------\n",
      "Epoch 479/1000, Train Loss: 0.04036, lr: 0.000125\n",
      "Val Loss: 0.04478\n",
      "---------\n",
      "Epoch 480/1000, Train Loss: 0.04035, lr: 0.000125\n",
      "Val Loss: 0.04477\n",
      "---------\n",
      "Epoch 481/1000, Train Loss: 0.04035, lr: 0.000125\n",
      "Val Loss: 0.04478\n",
      "---------\n",
      "Epoch 482/1000, Train Loss: 0.04034, lr: 0.000125\n",
      "Val Loss: 0.04477\n",
      "---------\n",
      "Epoch 483/1000, Train Loss: 0.04034, lr: 0.000125\n",
      "Val Loss: 0.04477\n",
      "---------\n",
      "Epoch 484/1000, Train Loss: 0.04034, lr: 0.000125\n",
      "Val Loss: 0.04477\n",
      "---------\n",
      "Epoch 485/1000, Train Loss: 0.04033, lr: 0.000125\n",
      "Val Loss: 0.04478\n",
      "---------\n",
      "Epoch 486/1000, Train Loss: 0.04033, lr: 0.000125\n",
      "Val Loss: 0.04477\n",
      "---------\n",
      "Epoch 487/1000, Train Loss: 0.04032, lr: 0.000125\n",
      "Val Loss: 0.04477\n",
      "---------\n",
      "Epoch 488/1000, Train Loss: 0.04032, lr: 0.000125\n",
      "Val Loss: 0.04477\n",
      "---------\n",
      "Epoch 489/1000, Train Loss: 0.04032, lr: 0.000125\n",
      "Val Loss: 0.04477\n",
      "---------\n",
      "Epoch 490/1000, Train Loss: 0.04031, lr: 0.000125\n",
      "Val Loss: 0.04477\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 491/1000, Train Loss: 0.04025, lr: 6.25e-05\n",
      "Val Loss: 0.04453\n",
      "---------\n",
      "Epoch 492/1000, Train Loss: 0.04024, lr: 6.25e-05\n",
      "Val Loss: 0.04453\n",
      "---------\n",
      "Epoch 493/1000, Train Loss: 0.04023, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 494/1000, Train Loss: 0.04022, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 495/1000, Train Loss: 0.04021, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 496/1000, Train Loss: 0.04021, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 497/1000, Train Loss: 0.04020, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 498/1000, Train Loss: 0.04020, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 499/1000, Train Loss: 0.04019, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 500/1000, Train Loss: 0.04019, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 501/1000, Train Loss: 0.04019, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 502/1000, Train Loss: 0.04018, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 503/1000, Train Loss: 0.04018, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 504/1000, Train Loss: 0.04017, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 505/1000, Train Loss: 0.04017, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 506/1000, Train Loss: 0.04017, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 507/1000, Train Loss: 0.04017, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 508/1000, Train Loss: 0.04016, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 509/1000, Train Loss: 0.04016, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 510/1000, Train Loss: 0.04016, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 511/1000, Train Loss: 0.04015, lr: 6.25e-05\n",
      "Val Loss: 0.04452\n",
      "---------\n",
      "Epoch 512/1000, Train Loss: 0.04015, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 513/1000, Train Loss: 0.04015, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 514/1000, Train Loss: 0.04014, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 515/1000, Train Loss: 0.04014, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 516/1000, Train Loss: 0.04014, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 517/1000, Train Loss: 0.04013, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 518/1000, Train Loss: 0.04013, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 519/1000, Train Loss: 0.04013, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 520/1000, Train Loss: 0.04013, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 521/1000, Train Loss: 0.04012, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 522/1000, Train Loss: 0.04012, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 523/1000, Train Loss: 0.04012, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 524/1000, Train Loss: 0.04012, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 525/1000, Train Loss: 0.04011, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 526/1000, Train Loss: 0.04011, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 527/1000, Train Loss: 0.04011, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 528/1000, Train Loss: 0.04011, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 529/1000, Train Loss: 0.04010, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 530/1000, Train Loss: 0.04010, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 531/1000, Train Loss: 0.04010, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 532/1000, Train Loss: 0.04010, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 533/1000, Train Loss: 0.04009, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 534/1000, Train Loss: 0.04009, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 535/1000, Train Loss: 0.04009, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 536/1000, Train Loss: 0.04009, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 537/1000, Train Loss: 0.04008, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 538/1000, Train Loss: 0.04008, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 539/1000, Train Loss: 0.04008, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 540/1000, Train Loss: 0.04008, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 541/1000, Train Loss: 0.04007, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 542/1000, Train Loss: 0.04007, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 543/1000, Train Loss: 0.04007, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 544/1000, Train Loss: 0.04007, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 545/1000, Train Loss: 0.04006, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 546/1000, Train Loss: 0.04006, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 547/1000, Train Loss: 0.04006, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 548/1000, Train Loss: 0.04006, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 549/1000, Train Loss: 0.04005, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 550/1000, Train Loss: 0.04005, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 551/1000, Train Loss: 0.04005, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 552/1000, Train Loss: 0.04005, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 553/1000, Train Loss: 0.04004, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 554/1000, Train Loss: 0.04004, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 555/1000, Train Loss: 0.04004, lr: 6.25e-05\n",
      "Val Loss: 0.04451\n",
      "---------\n",
      "Epoch 556/1000, Train Loss: 0.04004, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 557/1000, Train Loss: 0.04004, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 558/1000, Train Loss: 0.04003, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 559/1000, Train Loss: 0.04003, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 560/1000, Train Loss: 0.04003, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 561/1000, Train Loss: 0.04003, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 562/1000, Train Loss: 0.04002, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 563/1000, Train Loss: 0.04002, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 564/1000, Train Loss: 0.04002, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 565/1000, Train Loss: 0.04002, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 566/1000, Train Loss: 0.04002, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 567/1000, Train Loss: 0.04001, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 568/1000, Train Loss: 0.04001, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 569/1000, Train Loss: 0.04001, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 570/1000, Train Loss: 0.04001, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 571/1000, Train Loss: 0.04001, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 572/1000, Train Loss: 0.04000, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 573/1000, Train Loss: 0.04000, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "Epoch 574/1000, Train Loss: 0.04000, lr: 6.25e-05\n",
      "Val Loss: 0.04450\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 575/1000, Train Loss: 0.03994, lr: 3.125e-05\n",
      "Val Loss: 0.04446\n",
      "---------\n",
      "Epoch 576/1000, Train Loss: 0.03992, lr: 3.125e-05\n",
      "Val Loss: 0.04446\n",
      "---------\n",
      "Epoch 577/1000, Train Loss: 0.03992, lr: 3.125e-05\n",
      "Val Loss: 0.04446\n",
      "---------\n",
      "Epoch 578/1000, Train Loss: 0.03992, lr: 3.125e-05\n",
      "Val Loss: 0.04446\n",
      "---------\n",
      "Epoch 579/1000, Train Loss: 0.03992, lr: 3.125e-05\n",
      "Val Loss: 0.04446\n",
      "---------\n",
      "Epoch 580/1000, Train Loss: 0.03991, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 581/1000, Train Loss: 0.03991, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 582/1000, Train Loss: 0.03991, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 583/1000, Train Loss: 0.03991, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 584/1000, Train Loss: 0.03991, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 585/1000, Train Loss: 0.03990, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 586/1000, Train Loss: 0.03990, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 587/1000, Train Loss: 0.03990, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 588/1000, Train Loss: 0.03990, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 589/1000, Train Loss: 0.03990, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 590/1000, Train Loss: 0.03990, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 591/1000, Train Loss: 0.03989, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 592/1000, Train Loss: 0.03989, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 593/1000, Train Loss: 0.03989, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 594/1000, Train Loss: 0.03989, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 595/1000, Train Loss: 0.03989, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 596/1000, Train Loss: 0.03989, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 597/1000, Train Loss: 0.03989, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 598/1000, Train Loss: 0.03988, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 599/1000, Train Loss: 0.03988, lr: 3.125e-05\n",
      "Val Loss: 0.04445\n",
      "---------\n",
      "Epoch 600/1000, Train Loss: 0.03988, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 601/1000, Train Loss: 0.03988, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 602/1000, Train Loss: 0.03988, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 603/1000, Train Loss: 0.03988, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 604/1000, Train Loss: 0.03988, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 605/1000, Train Loss: 0.03987, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 606/1000, Train Loss: 0.03987, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 607/1000, Train Loss: 0.03987, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 608/1000, Train Loss: 0.03987, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 609/1000, Train Loss: 0.03987, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 610/1000, Train Loss: 0.03987, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 611/1000, Train Loss: 0.03987, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 612/1000, Train Loss: 0.03987, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 613/1000, Train Loss: 0.03986, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 614/1000, Train Loss: 0.03986, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 615/1000, Train Loss: 0.03986, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 616/1000, Train Loss: 0.03986, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 617/1000, Train Loss: 0.03986, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 618/1000, Train Loss: 0.03986, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 619/1000, Train Loss: 0.03986, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 620/1000, Train Loss: 0.03986, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 621/1000, Train Loss: 0.03985, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 622/1000, Train Loss: 0.03985, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 623/1000, Train Loss: 0.03985, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 624/1000, Train Loss: 0.03985, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 625/1000, Train Loss: 0.03985, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 626/1000, Train Loss: 0.03985, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 627/1000, Train Loss: 0.03985, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 628/1000, Train Loss: 0.03985, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 629/1000, Train Loss: 0.03984, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 630/1000, Train Loss: 0.03984, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 631/1000, Train Loss: 0.03984, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 632/1000, Train Loss: 0.03984, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 633/1000, Train Loss: 0.03984, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 634/1000, Train Loss: 0.03984, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 635/1000, Train Loss: 0.03984, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 636/1000, Train Loss: 0.03984, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 637/1000, Train Loss: 0.03983, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 638/1000, Train Loss: 0.03983, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 639/1000, Train Loss: 0.03983, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 640/1000, Train Loss: 0.03983, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 641/1000, Train Loss: 0.03983, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 642/1000, Train Loss: 0.03983, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 643/1000, Train Loss: 0.03983, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 644/1000, Train Loss: 0.03983, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 645/1000, Train Loss: 0.03982, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 646/1000, Train Loss: 0.03982, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 647/1000, Train Loss: 0.03982, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 648/1000, Train Loss: 0.03982, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 649/1000, Train Loss: 0.03982, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 650/1000, Train Loss: 0.03982, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 651/1000, Train Loss: 0.03982, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 652/1000, Train Loss: 0.03982, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 653/1000, Train Loss: 0.03982, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 654/1000, Train Loss: 0.03981, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 655/1000, Train Loss: 0.03981, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 656/1000, Train Loss: 0.03981, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 657/1000, Train Loss: 0.03981, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 658/1000, Train Loss: 0.03981, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 659/1000, Train Loss: 0.03981, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "Epoch 660/1000, Train Loss: 0.03981, lr: 3.125e-05\n",
      "Val Loss: 0.04444\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 661/1000, Train Loss: 0.03976, lr: 1.5625e-05\n",
      "Val Loss: 0.04438\n",
      "---------\n",
      "Epoch 662/1000, Train Loss: 0.03976, lr: 1.5625e-05\n",
      "Val Loss: 0.04438\n",
      "---------\n",
      "Epoch 663/1000, Train Loss: 0.03976, lr: 1.5625e-05\n",
      "Val Loss: 0.04438\n",
      "---------\n",
      "Epoch 664/1000, Train Loss: 0.03976, lr: 1.5625e-05\n",
      "Val Loss: 0.04438\n",
      "---------\n",
      "Epoch 665/1000, Train Loss: 0.03975, lr: 1.5625e-05\n",
      "Val Loss: 0.04438\n",
      "---------\n",
      "Epoch 666/1000, Train Loss: 0.03975, lr: 1.5625e-05\n",
      "Val Loss: 0.04438\n",
      "---------\n",
      "Epoch 667/1000, Train Loss: 0.03975, lr: 1.5625e-05\n",
      "Val Loss: 0.04438\n",
      "---------\n",
      "Epoch 668/1000, Train Loss: 0.03975, lr: 1.5625e-05\n",
      "Val Loss: 0.04438\n",
      "---------\n",
      "Epoch 669/1000, Train Loss: 0.03975, lr: 1.5625e-05\n",
      "Val Loss: 0.04438\n",
      "---------\n",
      "Epoch 670/1000, Train Loss: 0.03975, lr: 1.5625e-05\n",
      "Val Loss: 0.04438\n",
      "---------\n",
      "Epoch 671/1000, Train Loss: 0.03975, lr: 1.5625e-05\n",
      "Val Loss: 0.04438\n",
      "---------\n",
      "Epoch 672/1000, Train Loss: 0.03975, lr: 1.5625e-05\n",
      "Val Loss: 0.04438\n",
      "---------\n",
      "Epoch 673/1000, Train Loss: 0.03975, lr: 1.5625e-05\n",
      "Val Loss: 0.04438\n",
      "---------\n",
      "Epoch 674/1000, Train Loss: 0.03975, lr: 1.5625e-05\n",
      "Val Loss: 0.04438\n",
      "---------\n",
      "Epoch 675/1000, Train Loss: 0.03974, lr: 1.5625e-05\n",
      "Val Loss: 0.04438\n",
      "---------\n",
      "Epoch 676/1000, Train Loss: 0.03974, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 677/1000, Train Loss: 0.03974, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 678/1000, Train Loss: 0.03974, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 679/1000, Train Loss: 0.03974, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 680/1000, Train Loss: 0.03974, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 681/1000, Train Loss: 0.03974, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 682/1000, Train Loss: 0.03974, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 683/1000, Train Loss: 0.03974, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 684/1000, Train Loss: 0.03974, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 685/1000, Train Loss: 0.03974, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 686/1000, Train Loss: 0.03974, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 687/1000, Train Loss: 0.03973, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 688/1000, Train Loss: 0.03973, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 689/1000, Train Loss: 0.03973, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 690/1000, Train Loss: 0.03973, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 691/1000, Train Loss: 0.03973, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 692/1000, Train Loss: 0.03973, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 693/1000, Train Loss: 0.03973, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "Epoch 694/1000, Train Loss: 0.03973, lr: 1.5625e-05\n",
      "Val Loss: 0.04437\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 695/1000, Train Loss: 0.03970, lr: 7.8125e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 696/1000, Train Loss: 0.03969, lr: 7.8125e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 697/1000, Train Loss: 0.03969, lr: 7.8125e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 698/1000, Train Loss: 0.03969, lr: 7.8125e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 699/1000, Train Loss: 0.03969, lr: 7.8125e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 700/1000, Train Loss: 0.03969, lr: 7.8125e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 701/1000, Train Loss: 0.03969, lr: 7.8125e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 702/1000, Train Loss: 0.03969, lr: 7.8125e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 703/1000, Train Loss: 0.03969, lr: 7.8125e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 704/1000, Train Loss: 0.03969, lr: 7.8125e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 705/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 706/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 707/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 708/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 709/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 710/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 711/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 712/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 713/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 714/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 715/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 716/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 717/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 718/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 719/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 720/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 721/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 722/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 723/1000, Train Loss: 0.03967, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 724/1000, Train Loss: 0.03966, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 725/1000, Train Loss: 0.03966, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 726/1000, Train Loss: 0.03966, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 727/1000, Train Loss: 0.03966, lr: 3.90625e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 728/1000, Train Loss: 0.03965, lr: 1.953125e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 729/1000, Train Loss: 0.03965, lr: 1.953125e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 730/1000, Train Loss: 0.03965, lr: 1.953125e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 731/1000, Train Loss: 0.03965, lr: 1.953125e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 732/1000, Train Loss: 0.03965, lr: 1.953125e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 733/1000, Train Loss: 0.03965, lr: 1.953125e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 734/1000, Train Loss: 0.03965, lr: 1.953125e-06\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 735/1000, Train Loss: 0.03965, lr: 9.765625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 736/1000, Train Loss: 0.03965, lr: 9.765625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 737/1000, Train Loss: 0.03965, lr: 9.765625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 738/1000, Train Loss: 0.03965, lr: 9.765625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 739/1000, Train Loss: 0.03965, lr: 9.765625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 740/1000, Train Loss: 0.03965, lr: 9.765625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 741/1000, Train Loss: 0.03965, lr: 9.765625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 742/1000, Train Loss: 0.03965, lr: 9.765625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 743/1000, Train Loss: 0.03965, lr: 9.765625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 744/1000, Train Loss: 0.03964, lr: 4.8828125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 745/1000, Train Loss: 0.03964, lr: 4.8828125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 746/1000, Train Loss: 0.03964, lr: 4.8828125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 747/1000, Train Loss: 0.03964, lr: 4.8828125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 748/1000, Train Loss: 0.03964, lr: 4.8828125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 749/1000, Train Loss: 0.03964, lr: 4.8828125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 750/1000, Train Loss: 0.03964, lr: 4.8828125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 751/1000, Train Loss: 0.03964, lr: 4.8828125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 752/1000, Train Loss: 0.03964, lr: 4.8828125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 753/1000, Train Loss: 0.03964, lr: 4.8828125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 754/1000, Train Loss: 0.03964, lr: 4.8828125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 755/1000, Train Loss: 0.03964, lr: 4.8828125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 756/1000, Train Loss: 0.03964, lr: 4.8828125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 757/1000, Train Loss: 0.03964, lr: 2.44140625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 758/1000, Train Loss: 0.03964, lr: 2.44140625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 759/1000, Train Loss: 0.03964, lr: 2.44140625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 760/1000, Train Loss: 0.03964, lr: 2.44140625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 761/1000, Train Loss: 0.03964, lr: 2.44140625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 762/1000, Train Loss: 0.03964, lr: 2.44140625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 763/1000, Train Loss: 0.03964, lr: 2.44140625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 764/1000, Train Loss: 0.03964, lr: 2.44140625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 765/1000, Train Loss: 0.03964, lr: 2.44140625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 766/1000, Train Loss: 0.03964, lr: 2.44140625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 767/1000, Train Loss: 0.03964, lr: 2.44140625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 768/1000, Train Loss: 0.03964, lr: 2.44140625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 769/1000, Train Loss: 0.03964, lr: 2.44140625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 770/1000, Train Loss: 0.03964, lr: 2.44140625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 771/1000, Train Loss: 0.03964, lr: 2.44140625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 772/1000, Train Loss: 0.03964, lr: 2.44140625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 773/1000, Train Loss: 0.03964, lr: 2.44140625e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 774/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 775/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 776/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 777/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 778/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 779/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 780/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 781/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 782/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 783/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 784/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 785/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 786/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 787/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 788/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 789/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 790/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 791/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 792/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 793/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 794/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 795/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 796/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 797/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 798/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 799/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 800/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 801/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 802/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 803/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 804/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 805/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 806/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 807/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 808/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 809/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 810/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 811/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 812/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 813/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 814/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 815/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 816/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 817/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 818/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 819/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 820/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 821/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 822/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 823/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 824/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 825/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 826/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 827/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 828/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 829/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 830/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 831/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 832/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 833/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 834/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 835/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 836/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 837/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 838/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 839/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 840/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 841/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 842/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 843/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 844/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 845/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 846/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 847/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 848/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 849/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 850/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 851/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 852/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 853/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 854/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 855/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 856/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 857/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 858/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 859/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 860/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 861/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 862/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 863/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 864/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 865/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 866/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 867/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 868/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 869/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 870/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 871/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 872/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 873/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 874/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 875/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 876/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 877/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 878/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 879/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 880/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 881/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 882/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 883/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 884/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 885/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 886/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 887/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 888/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 889/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 890/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 891/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 892/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 893/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 894/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 895/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 896/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 897/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 898/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 899/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 900/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 901/1000, Train Loss: 0.03964, lr: 1.220703125e-07\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 902/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 903/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 904/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 905/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 906/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 907/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 908/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 909/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 910/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 911/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 912/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 913/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 914/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 915/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 916/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 917/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 918/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 919/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 920/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 921/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 922/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 923/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 924/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 925/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 926/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 927/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 928/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 929/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 930/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 931/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 932/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 933/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 934/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 935/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 936/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 937/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 938/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 939/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 940/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 941/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 942/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 943/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 944/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 945/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 946/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 947/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 948/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 949/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 950/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 951/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 952/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 953/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 954/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 955/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 956/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 957/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 958/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 959/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 960/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 961/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 962/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 963/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 964/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 965/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 966/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 967/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 968/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 969/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 970/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 971/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 972/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 973/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 974/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 975/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 976/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 977/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 978/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 979/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 980/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 981/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 982/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 983/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 984/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 985/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 986/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 987/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 988/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 989/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 990/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 991/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 992/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 993/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 994/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 995/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 996/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 997/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 998/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 999/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "Epoch 1000/1000, Train Loss: 0.03964, lr: 6.103515625e-08\n",
      "Val Loss: 0.04436\n",
      "---------\n",
      "6710.678318977356 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 3)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 3)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 3)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 3)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 3)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 3)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 3)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 3)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 3)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 3)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 3)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 3)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 3)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 3)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 3)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 3)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 3)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 3)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 3)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 3)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 3)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 3)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 3)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 3)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 3)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 3)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 3)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 3)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 3)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 3)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 3)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 3)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 3)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 3)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 3)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 3)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 3)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 3)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 3)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 3)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 3)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 3)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 3)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 3)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 3)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 3)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 3)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 3)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 3)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 3)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 3)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 3)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 3)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 3)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 3)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 3)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 3)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 3)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 3)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 3)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 3)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 3)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 3)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 3)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 3)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 3)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 3)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 3)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 3)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 3)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 3)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 3)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 3)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 3)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 3)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 3)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 3)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 3)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 3)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 3)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 3)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 3)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 3)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 3)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 3)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 3)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 3)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 3)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 3)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 3)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 3)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 3)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 3)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 3)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 3)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 3)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 3)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 3)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 3)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 3)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 3)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 3)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 3)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 3)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 3)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 3)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 3)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 3)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 3)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 3)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 3)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 3)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 3)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 3)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 3)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 3)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 3)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 3)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 3)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 3)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 3)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 3)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 3)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 3)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 3)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 3)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 3)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 3)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 3)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 3)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 3)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 3)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 3)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 3)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 3)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 3)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 3)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 3)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 3)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 3)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 3)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 3)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 3)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 3)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 3)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 3)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 3)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 3)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 3)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 3)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 3)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 3)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 3)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 3)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 3)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 3)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 3)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 3)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 3)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 3)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 3)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 3)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 3)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 3)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 3)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 3)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 3)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 3)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 3)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 3)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 3)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 3)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 3)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 3)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 3)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 3)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 3)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 3)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 3)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 3)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 3)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 3)\n",
      "182\n",
      "y_hat: (1460, 32, 48, 6, 3), y_hat_i: (4, 32, 48, 6, 3), y_i: (4, 32, 48, 6, 3), batch.x: torch.Size([128, 48, 6, 6]), y: (1460, 32, 48, 6, 3)\n",
      "RMSE for t2m: 2.5194715978261035; MAE for t2m: 1.91450865170485;\n",
      "RMSE for sp: 3.0141489081595383; MAE for sp: 2.139236710005034;\n",
      "RMSE for tcc: 0.3370161432650224; MAE for tcc: 0.2406498284325559;\n",
      "RMSE for u10: 1.817782680787341; MAE for u10: 1.3341112625369345;\n",
      "RMSE for v10: 1.799304559578151; MAE for v10: 1.3094555798240035;\n",
      "RMSE for tp: 0.31519590824031146; MAE for tp: 0.08436556715642529;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 3)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 3)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 3)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 3)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 3)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 3)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 3)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 3)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 3)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 3)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 3)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 3)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 3)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 3)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 3)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 3)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 3)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 3)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 3)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 3)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 3)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 3)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 3)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 3)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 3)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 3)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 3)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 3)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 3)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 3)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 3)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 3)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 3)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 3)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 3)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 3)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 3)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 3)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 3)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 3)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 3)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 3)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 3)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 3)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 3)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 3)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 3)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 3)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 3)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 3)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 3)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 3)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 3)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 3)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 3)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 3)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 3)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 3)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 3)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 3)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 3)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 3)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 3)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 3)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 3)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 3)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 3)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 3)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 3)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 3)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 3)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 3)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 3)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 3)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 3)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 3)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 3)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 3)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 3)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 3)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 3)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 3)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 3)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 3)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 3)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 3)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 3)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 3)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 3)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 3)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 3)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 3)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 3)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 3)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 3)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 3)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 3)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 3)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 3)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 3)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 3)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 3)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 3)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 3)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 3)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 3)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 3)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 3)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 3)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 3)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 3)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 3)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 3)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 3)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 3)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 3)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 3)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 3)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 3)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 3)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 3)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 3)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 3)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 3)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 3)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 3)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 3)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 3)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 3)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 3)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 3)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 3)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 3)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 3)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 3)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 3)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 3)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 3)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 3)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 3)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 3)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 3)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 3)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 3)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 3)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 3)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 3)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 3)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 3)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 3)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 3)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 3)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 3)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 3)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 3)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 3)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 3)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 3)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 3)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 3)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 3)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 3)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 3)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 3)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 3)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 3)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 3)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 3)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 3)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 3)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 3)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 3)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 3)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 3)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 3)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 3)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 3)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 3)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 3)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 3)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 3)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 3), y_hat_i: (8, 32, 48, 6, 3), y_i: (8, 32, 48, 6, 3), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 3)\n",
      "182\n",
      "y_hat: (1460, 32, 48, 6, 3), y_hat_i: (4, 32, 48, 6, 3), y_i: (4, 32, 48, 6, 3), batch.x: torch.Size([128, 48, 6, 6]), y: (1460, 32, 48, 6, 3)\n",
      "RMSE for t2m: 2.5194715978261035; MAE for t2m: 1.91450865170485;\n",
      "RMSE for sp: 3.0141489081595383; MAE for sp: 2.139236710005034;\n",
      "RMSE for tcc: 0.33647984975521616; MAE for tcc: 0.23975318260243578;\n",
      "RMSE for u10: 1.817782680787341; MAE for u10: 1.3341112625369345;\n",
      "RMSE for v10: 1.799304559578151; MAE for v10: 1.3094555798240035;\n",
      "RMSE for tp: 0.31519590824031146; MAE for tp: 0.08436556715642529;\n",
      "Epoch 1/1000, Train Loss: 0.07979, lr: 0.001------------------------------------| 27.3% Complete\n",
      "Val Loss: 0.06863\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.06695, lr: 0.001\n",
      "Val Loss: 0.06564\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.06538, lr: 0.001\n",
      "Val Loss: 0.06524\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.06480, lr: 0.001\n",
      "Val Loss: 0.06490\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.06341, lr: 0.001\n",
      "Val Loss: 0.06294\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.06062, lr: 0.001\n",
      "Val Loss: 0.06157\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.05951, lr: 0.001\n",
      "Val Loss: 0.06139\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.05899, lr: 0.001\n",
      "Val Loss: 0.06082\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.05888, lr: 0.001\n",
      "Val Loss: 0.06127\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.05869, lr: 0.001\n",
      "Val Loss: 0.06044\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.05834, lr: 0.001\n",
      "Val Loss: 0.06024\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.05811, lr: 0.001\n",
      "Val Loss: 0.05986\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.05788, lr: 0.001\n",
      "Val Loss: 0.05949\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.05760, lr: 0.001\n",
      "Val Loss: 0.05948\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.05696, lr: 0.001\n",
      "Val Loss: 0.05874\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.05540, lr: 0.001\n",
      "Val Loss: 0.05705\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.05419, lr: 0.001\n",
      "Val Loss: 0.05584\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.05352, lr: 0.001\n",
      "Val Loss: 0.05521\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.05311, lr: 0.001\n",
      "Val Loss: 0.05480\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.05279, lr: 0.001\n",
      "Val Loss: 0.05447\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.05253, lr: 0.001\n",
      "Val Loss: 0.05429\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.05228, lr: 0.001\n",
      "Val Loss: 0.05413\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.05197, lr: 0.001\n",
      "Val Loss: 0.05400\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.05165, lr: 0.001\n",
      "Val Loss: 0.05355\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.05130, lr: 0.001\n",
      "Val Loss: 0.05339\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.05103, lr: 0.001\n",
      "Val Loss: 0.05308\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.05082, lr: 0.001\n",
      "Val Loss: 0.05287\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.05064, lr: 0.001\n",
      "Val Loss: 0.05274\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.05049, lr: 0.001\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.05036, lr: 0.001\n",
      "Val Loss: 0.05255\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.05025, lr: 0.001\n",
      "Val Loss: 0.05243\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.05013, lr: 0.001\n",
      "Val Loss: 0.05237\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.05005, lr: 0.001\n",
      "Val Loss: 0.05234\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.04998, lr: 0.001\n",
      "Val Loss: 0.05229\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.04989, lr: 0.001\n",
      "Val Loss: 0.05224\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.04982, lr: 0.001\n",
      "Val Loss: 0.05220\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.04975, lr: 0.001\n",
      "Val Loss: 0.05216\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.04967, lr: 0.001\n",
      "Val Loss: 0.05211\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.04962, lr: 0.001\n",
      "Val Loss: 0.05211\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.04956, lr: 0.001\n",
      "Val Loss: 0.05211\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.04951, lr: 0.001\n",
      "Val Loss: 0.05210\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.04946, lr: 0.001\n",
      "Val Loss: 0.05213\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.04941, lr: 0.001\n",
      "Val Loss: 0.05211\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.04935, lr: 0.001\n",
      "Val Loss: 0.05208\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.04931, lr: 0.001\n",
      "Val Loss: 0.05205\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.04926, lr: 0.001\n",
      "Val Loss: 0.05206\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.04921, lr: 0.001\n",
      "Val Loss: 0.05209\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.04917, lr: 0.001\n",
      "Val Loss: 0.05214\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.04913, lr: 0.001\n",
      "Val Loss: 0.05214\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.04909, lr: 0.001\n",
      "Val Loss: 0.05219\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.04904, lr: 0.001\n",
      "Val Loss: 0.05223\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.04900, lr: 0.001\n",
      "Val Loss: 0.05223\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 53/1000, Train Loss: 0.04834, lr: 0.0005\n",
      "Val Loss: 0.05034\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.04817, lr: 0.0005\n",
      "Val Loss: 0.05033\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.04812, lr: 0.0005\n",
      "Val Loss: 0.05032\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.04808, lr: 0.0005\n",
      "Val Loss: 0.05031\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.04805, lr: 0.0005\n",
      "Val Loss: 0.05031\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.04802, lr: 0.0005\n",
      "Val Loss: 0.05030\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.04799, lr: 0.0005\n",
      "Val Loss: 0.05031\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.04796, lr: 0.0005\n",
      "Val Loss: 0.05030\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.04793, lr: 0.0005\n",
      "Val Loss: 0.05032\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.04790, lr: 0.0005\n",
      "Val Loss: 0.05031\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.04788, lr: 0.0005\n",
      "Val Loss: 0.05033\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.04785, lr: 0.0005\n",
      "Val Loss: 0.05033\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.04782, lr: 0.0005\n",
      "Val Loss: 0.05034\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.04780, lr: 0.0005\n",
      "Val Loss: 0.05034\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.04777, lr: 0.0005\n",
      "Val Loss: 0.05034\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 68/1000, Train Loss: 0.04741, lr: 0.00025\n",
      "Val Loss: 0.04979\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.04737, lr: 0.00025\n",
      "Val Loss: 0.04979\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.04735, lr: 0.00025\n",
      "Val Loss: 0.04978\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.04733, lr: 0.00025\n",
      "Val Loss: 0.04978\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.04731, lr: 0.00025\n",
      "Val Loss: 0.04978\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.04729, lr: 0.00025\n",
      "Val Loss: 0.04977\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.04727, lr: 0.00025\n",
      "Val Loss: 0.04977\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.04726, lr: 0.00025\n",
      "Val Loss: 0.04977\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.04724, lr: 0.00025\n",
      "Val Loss: 0.04977\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.04723, lr: 0.00025\n",
      "Val Loss: 0.04977\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.04721, lr: 0.00025\n",
      "Val Loss: 0.04976\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.04720, lr: 0.00025\n",
      "Val Loss: 0.04976\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.04718, lr: 0.00025\n",
      "Val Loss: 0.04976\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.04717, lr: 0.00025\n",
      "Val Loss: 0.04976\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.04715, lr: 0.00025\n",
      "Val Loss: 0.04976\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.04714, lr: 0.00025\n",
      "Val Loss: 0.04976\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.04713, lr: 0.00025\n",
      "Val Loss: 0.04975\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.04711, lr: 0.00025\n",
      "Val Loss: 0.04975\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.04710, lr: 0.00025\n",
      "Val Loss: 0.04974\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.04709, lr: 0.00025\n",
      "Val Loss: 0.04975\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.04707, lr: 0.00025\n",
      "Val Loss: 0.04974\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.04706, lr: 0.00025\n",
      "Val Loss: 0.04974\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.04704, lr: 0.00025\n",
      "Val Loss: 0.04974\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.04703, lr: 0.00025\n",
      "Val Loss: 0.04974\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.04702, lr: 0.00025\n",
      "Val Loss: 0.04974\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.04701, lr: 0.00025\n",
      "Val Loss: 0.04974\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.04699, lr: 0.00025\n",
      "Val Loss: 0.04974\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.04698, lr: 0.00025\n",
      "Val Loss: 0.04974\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.04696, lr: 0.00025\n",
      "Val Loss: 0.04974\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.04695, lr: 0.00025\n",
      "Val Loss: 0.04974\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 98/1000, Train Loss: 0.04675, lr: 0.000125\n",
      "Val Loss: 0.04950\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.04672, lr: 0.000125\n",
      "Val Loss: 0.04950\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.04671, lr: 0.000125\n",
      "Val Loss: 0.04950\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.04670, lr: 0.000125\n",
      "Val Loss: 0.04950\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.04669, lr: 0.000125\n",
      "Val Loss: 0.04950\n",
      "---------\n",
      "Epoch 103/1000, Train Loss: 0.04668, lr: 0.000125\n",
      "Val Loss: 0.04950\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.04667, lr: 0.000125\n",
      "Val Loss: 0.04951\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.04666, lr: 0.000125\n",
      "Val Loss: 0.04951\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.04665, lr: 0.000125\n",
      "Val Loss: 0.04951\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.04664, lr: 0.000125\n",
      "Val Loss: 0.04950\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 108/1000, Train Loss: 0.04653, lr: 6.25e-05\n",
      "Val Loss: 0.04941\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.04650, lr: 6.25e-05\n",
      "Val Loss: 0.04941\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.04649, lr: 6.25e-05\n",
      "Val Loss: 0.04941\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.04649, lr: 6.25e-05\n",
      "Val Loss: 0.04941\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.04648, lr: 6.25e-05\n",
      "Val Loss: 0.04941\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.04648, lr: 6.25e-05\n",
      "Val Loss: 0.04941\n",
      "---------\n",
      "Epoch 114/1000, Train Loss: 0.04647, lr: 6.25e-05\n",
      "Val Loss: 0.04941\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.04647, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.04646, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.04646, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.04645, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.04645, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.04644, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 121/1000, Train Loss: 0.04644, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.04643, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.04643, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.04642, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.04642, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.04641, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 127/1000, Train Loss: 0.04641, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 128/1000, Train Loss: 0.04640, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.04640, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.04640, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.04639, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.04639, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.04638, lr: 6.25e-05\n",
      "Val Loss: 0.04940\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 134/1000, Train Loss: 0.04632, lr: 3.125e-05\n",
      "Val Loss: 0.04937\n",
      "---------\n",
      "Epoch 135/1000, Train Loss: 0.04631, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 136/1000, Train Loss: 0.04631, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.04630, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.04630, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.04630, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.04630, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 141/1000, Train Loss: 0.04629, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 142/1000, Train Loss: 0.04629, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.04629, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.04629, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 145/1000, Train Loss: 0.04628, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 146/1000, Train Loss: 0.04628, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 147/1000, Train Loss: 0.04628, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 148/1000, Train Loss: 0.04628, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 149/1000, Train Loss: 0.04627, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 150/1000, Train Loss: 0.04627, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 151/1000, Train Loss: 0.04627, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 152/1000, Train Loss: 0.04627, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 153/1000, Train Loss: 0.04626, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 154/1000, Train Loss: 0.04626, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 155/1000, Train Loss: 0.04626, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 156/1000, Train Loss: 0.04626, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 157/1000, Train Loss: 0.04626, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 158/1000, Train Loss: 0.04625, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 159/1000, Train Loss: 0.04625, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 160/1000, Train Loss: 0.04625, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 161/1000, Train Loss: 0.04625, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 162/1000, Train Loss: 0.04624, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 163/1000, Train Loss: 0.04624, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 164/1000, Train Loss: 0.04624, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 165/1000, Train Loss: 0.04624, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 166/1000, Train Loss: 0.04624, lr: 3.125e-05\n",
      "Val Loss: 0.04936\n",
      "---------\n",
      "Epoch 167/1000, Train Loss: 0.04623, lr: 3.125e-05\n",
      "Val Loss: 0.04935\n",
      "---------\n",
      "Epoch 168/1000, Train Loss: 0.04623, lr: 3.125e-05\n",
      "Val Loss: 0.04935\n",
      "---------\n",
      "Epoch 169/1000, Train Loss: 0.04623, lr: 3.125e-05\n",
      "Val Loss: 0.04935\n",
      "---------\n",
      "Epoch 170/1000, Train Loss: 0.04623, lr: 3.125e-05\n",
      "Val Loss: 0.04935\n",
      "---------\n",
      "Epoch 171/1000, Train Loss: 0.04623, lr: 3.125e-05\n",
      "Val Loss: 0.04935\n",
      "---------\n",
      "Epoch 172/1000, Train Loss: 0.04622, lr: 3.125e-05\n",
      "Val Loss: 0.04935\n",
      "---------\n",
      "Epoch 173/1000, Train Loss: 0.04622, lr: 3.125e-05\n",
      "Val Loss: 0.04935\n",
      "---------\n",
      "Epoch 174/1000, Train Loss: 0.04622, lr: 3.125e-05\n",
      "Val Loss: 0.04935\n",
      "---------\n",
      "Epoch 175/1000, Train Loss: 0.04622, lr: 3.125e-05\n",
      "Val Loss: 0.04935\n",
      "---------\n",
      "Epoch 176/1000, Train Loss: 0.04622, lr: 3.125e-05\n",
      "Val Loss: 0.04935\n",
      "---------\n",
      "Epoch 177/1000, Train Loss: 0.04621, lr: 3.125e-05\n",
      "Val Loss: 0.04935\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 178/1000, Train Loss: 0.04618, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 179/1000, Train Loss: 0.04617, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 180/1000, Train Loss: 0.04617, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 181/1000, Train Loss: 0.04617, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 182/1000, Train Loss: 0.04617, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 183/1000, Train Loss: 0.04616, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 184/1000, Train Loss: 0.04616, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 185/1000, Train Loss: 0.04616, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 186/1000, Train Loss: 0.04616, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 187/1000, Train Loss: 0.04616, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 188/1000, Train Loss: 0.04616, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 189/1000, Train Loss: 0.04616, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 190/1000, Train Loss: 0.04616, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 191/1000, Train Loss: 0.04615, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 192/1000, Train Loss: 0.04615, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 193/1000, Train Loss: 0.04615, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 194/1000, Train Loss: 0.04615, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 195/1000, Train Loss: 0.04615, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 196/1000, Train Loss: 0.04615, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 197/1000, Train Loss: 0.04615, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 198/1000, Train Loss: 0.04615, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 199/1000, Train Loss: 0.04615, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 200/1000, Train Loss: 0.04614, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 201/1000, Train Loss: 0.04614, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 202/1000, Train Loss: 0.04614, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 203/1000, Train Loss: 0.04614, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 204/1000, Train Loss: 0.04614, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 205/1000, Train Loss: 0.04614, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 206/1000, Train Loss: 0.04614, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 207/1000, Train Loss: 0.04614, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 208/1000, Train Loss: 0.04614, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 209/1000, Train Loss: 0.04613, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 210/1000, Train Loss: 0.04613, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 211/1000, Train Loss: 0.04613, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 212/1000, Train Loss: 0.04613, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 213/1000, Train Loss: 0.04613, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 214/1000, Train Loss: 0.04613, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 215/1000, Train Loss: 0.04613, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 216/1000, Train Loss: 0.04613, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 217/1000, Train Loss: 0.04613, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 218/1000, Train Loss: 0.04612, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 219/1000, Train Loss: 0.04612, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 220/1000, Train Loss: 0.04612, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 221/1000, Train Loss: 0.04612, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 222/1000, Train Loss: 0.04612, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 223/1000, Train Loss: 0.04612, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 224/1000, Train Loss: 0.04612, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 225/1000, Train Loss: 0.04612, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 226/1000, Train Loss: 0.04612, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 227/1000, Train Loss: 0.04611, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 228/1000, Train Loss: 0.04611, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 229/1000, Train Loss: 0.04611, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 230/1000, Train Loss: 0.04611, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 231/1000, Train Loss: 0.04611, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 232/1000, Train Loss: 0.04611, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 233/1000, Train Loss: 0.04611, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 234/1000, Train Loss: 0.04611, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 235/1000, Train Loss: 0.04611, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 236/1000, Train Loss: 0.04611, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 237/1000, Train Loss: 0.04610, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 238/1000, Train Loss: 0.04610, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 239/1000, Train Loss: 0.04610, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 240/1000, Train Loss: 0.04610, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 241/1000, Train Loss: 0.04610, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 242/1000, Train Loss: 0.04610, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 243/1000, Train Loss: 0.04610, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 244/1000, Train Loss: 0.04610, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 245/1000, Train Loss: 0.04610, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 246/1000, Train Loss: 0.04609, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 247/1000, Train Loss: 0.04609, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 248/1000, Train Loss: 0.04609, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 249/1000, Train Loss: 0.04609, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 250/1000, Train Loss: 0.04609, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 251/1000, Train Loss: 0.04609, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 252/1000, Train Loss: 0.04609, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 253/1000, Train Loss: 0.04609, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 254/1000, Train Loss: 0.04609, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 255/1000, Train Loss: 0.04609, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 256/1000, Train Loss: 0.04608, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 257/1000, Train Loss: 0.04608, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 258/1000, Train Loss: 0.04608, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 259/1000, Train Loss: 0.04608, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 260/1000, Train Loss: 0.04608, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 261/1000, Train Loss: 0.04608, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 262/1000, Train Loss: 0.04608, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 263/1000, Train Loss: 0.04608, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 264/1000, Train Loss: 0.04608, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 265/1000, Train Loss: 0.04608, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 266/1000, Train Loss: 0.04607, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 267/1000, Train Loss: 0.04607, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 268/1000, Train Loss: 0.04607, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 269/1000, Train Loss: 0.04607, lr: 1.5625e-05\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 270/1000, Train Loss: 0.04607, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 271/1000, Train Loss: 0.04607, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 272/1000, Train Loss: 0.04607, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 273/1000, Train Loss: 0.04607, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 274/1000, Train Loss: 0.04607, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 275/1000, Train Loss: 0.04606, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 276/1000, Train Loss: 0.04606, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 277/1000, Train Loss: 0.04606, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 278/1000, Train Loss: 0.04606, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 279/1000, Train Loss: 0.04606, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 280/1000, Train Loss: 0.04606, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 281/1000, Train Loss: 0.04606, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 282/1000, Train Loss: 0.04606, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 283/1000, Train Loss: 0.04606, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 284/1000, Train Loss: 0.04606, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 285/1000, Train Loss: 0.04605, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 286/1000, Train Loss: 0.04605, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 287/1000, Train Loss: 0.04605, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 288/1000, Train Loss: 0.04605, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 289/1000, Train Loss: 0.04605, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 290/1000, Train Loss: 0.04605, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 291/1000, Train Loss: 0.04605, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 292/1000, Train Loss: 0.04605, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 293/1000, Train Loss: 0.04605, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 294/1000, Train Loss: 0.04605, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 295/1000, Train Loss: 0.04604, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 296/1000, Train Loss: 0.04604, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 297/1000, Train Loss: 0.04604, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 298/1000, Train Loss: 0.04604, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 299/1000, Train Loss: 0.04604, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 300/1000, Train Loss: 0.04604, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 301/1000, Train Loss: 0.04604, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 302/1000, Train Loss: 0.04604, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 303/1000, Train Loss: 0.04604, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 304/1000, Train Loss: 0.04604, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 305/1000, Train Loss: 0.04603, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 306/1000, Train Loss: 0.04603, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 307/1000, Train Loss: 0.04603, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 308/1000, Train Loss: 0.04603, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 309/1000, Train Loss: 0.04603, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 310/1000, Train Loss: 0.04603, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 311/1000, Train Loss: 0.04603, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 312/1000, Train Loss: 0.04603, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 313/1000, Train Loss: 0.04603, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 314/1000, Train Loss: 0.04603, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 315/1000, Train Loss: 0.04603, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 316/1000, Train Loss: 0.04602, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 317/1000, Train Loss: 0.04602, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 318/1000, Train Loss: 0.04602, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 319/1000, Train Loss: 0.04602, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 320/1000, Train Loss: 0.04602, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 321/1000, Train Loss: 0.04602, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 322/1000, Train Loss: 0.04602, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 323/1000, Train Loss: 0.04602, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 324/1000, Train Loss: 0.04602, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 325/1000, Train Loss: 0.04602, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 326/1000, Train Loss: 0.04601, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 327/1000, Train Loss: 0.04601, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 328/1000, Train Loss: 0.04601, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 329/1000, Train Loss: 0.04601, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 330/1000, Train Loss: 0.04601, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 331/1000, Train Loss: 0.04601, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 332/1000, Train Loss: 0.04601, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 333/1000, Train Loss: 0.04601, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 334/1000, Train Loss: 0.04601, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 335/1000, Train Loss: 0.04601, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 336/1000, Train Loss: 0.04601, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 337/1000, Train Loss: 0.04600, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 338/1000, Train Loss: 0.04600, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 339/1000, Train Loss: 0.04600, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 340/1000, Train Loss: 0.04600, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 341/1000, Train Loss: 0.04600, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 342/1000, Train Loss: 0.04600, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 343/1000, Train Loss: 0.04600, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 344/1000, Train Loss: 0.04600, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 345/1000, Train Loss: 0.04600, lr: 1.5625e-05\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 346/1000, Train Loss: 0.04598, lr: 7.8125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 347/1000, Train Loss: 0.04597, lr: 7.8125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 348/1000, Train Loss: 0.04597, lr: 7.8125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 349/1000, Train Loss: 0.04597, lr: 7.8125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 350/1000, Train Loss: 0.04597, lr: 7.8125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 351/1000, Train Loss: 0.04597, lr: 7.8125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 352/1000, Train Loss: 0.04597, lr: 7.8125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 353/1000, Train Loss: 0.04597, lr: 7.8125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 354/1000, Train Loss: 0.04597, lr: 7.8125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 355/1000, Train Loss: 0.04597, lr: 7.8125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 356/1000, Train Loss: 0.04597, lr: 7.8125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 357/1000, Train Loss: 0.04597, lr: 7.8125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 358/1000, Train Loss: 0.04597, lr: 7.8125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 359/1000, Train Loss: 0.04597, lr: 7.8125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 360/1000, Train Loss: 0.04596, lr: 7.8125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 361/1000, Train Loss: 0.04596, lr: 7.8125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 362/1000, Train Loss: 0.04595, lr: 3.90625e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 363/1000, Train Loss: 0.04595, lr: 3.90625e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 364/1000, Train Loss: 0.04595, lr: 3.90625e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 365/1000, Train Loss: 0.04595, lr: 3.90625e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 366/1000, Train Loss: 0.04595, lr: 3.90625e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 367/1000, Train Loss: 0.04595, lr: 3.90625e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 368/1000, Train Loss: 0.04595, lr: 3.90625e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 369/1000, Train Loss: 0.04594, lr: 1.953125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 370/1000, Train Loss: 0.04594, lr: 1.953125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 371/1000, Train Loss: 0.04594, lr: 1.953125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 372/1000, Train Loss: 0.04594, lr: 1.953125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 373/1000, Train Loss: 0.04594, lr: 1.953125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 374/1000, Train Loss: 0.04594, lr: 1.953125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 375/1000, Train Loss: 0.04594, lr: 1.953125e-06\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 376/1000, Train Loss: 0.04594, lr: 9.765625e-07\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 377/1000, Train Loss: 0.04594, lr: 9.765625e-07\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 378/1000, Train Loss: 0.04594, lr: 9.765625e-07\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 379/1000, Train Loss: 0.04594, lr: 9.765625e-07\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 380/1000, Train Loss: 0.04594, lr: 9.765625e-07\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 381/1000, Train Loss: 0.04594, lr: 9.765625e-07\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 382/1000, Train Loss: 0.04594, lr: 9.765625e-07\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 383/1000, Train Loss: 0.04593, lr: 4.8828125e-07\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 384/1000, Train Loss: 0.04593, lr: 4.8828125e-07\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 385/1000, Train Loss: 0.04593, lr: 4.8828125e-07\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 386/1000, Train Loss: 0.04593, lr: 4.8828125e-07\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 387/1000, Train Loss: 0.04593, lr: 4.8828125e-07\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 388/1000, Train Loss: 0.04593, lr: 4.8828125e-07\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 389/1000, Train Loss: 0.04593, lr: 4.8828125e-07\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 390/1000, Train Loss: 0.04593, lr: 2.44140625e-07\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 391/1000, Train Loss: 0.04593, lr: 2.44140625e-07\n",
      "Val Loss: 0.04933\n",
      "---------\n",
      "Epoch 392/1000, Train Loss: 0.04593, lr: 2.44140625e-07\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 393/1000, Train Loss: 0.04593, lr: 2.44140625e-07\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Epoch 394/1000, Train Loss: 0.04593, lr: 2.44140625e-07\n",
      "Val Loss: 0.04934\n",
      "---------\n",
      "Early stopping ....\n",
      "2494.9681193828583 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 4)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 4)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 4)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 4)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 4)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 4)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 4)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 4)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 4)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 4)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 4)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 4)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 4)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 4)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 4)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 4)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 4)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 4)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 4)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 4)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 4)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 4)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 4)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 4)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 4)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 4)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 4)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 4)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 4)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 4)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 4)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 4)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 4)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 4)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 4)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 4)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 4)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 4)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 4)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 4)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 4)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 4)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 4)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 4)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 4)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 4)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 4)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 4)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 4)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 4)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 4)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 4)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 4)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 4)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 4)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 4)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 4)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 4)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 4)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 4)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 4)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 4)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 4)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 4)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 4)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 4)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 4)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 4)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 4)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 4)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 4)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 4)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 4)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 4)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 4)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 4)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 4)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 4)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 4)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 4)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 4)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 4)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 4)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 4)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 4)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 4)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 4)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 4)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 4)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 4)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 4)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 4)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 4)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 4)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 4)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 4)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 4)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 4)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 4)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 4)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 4)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 4)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 4)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 4)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 4)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 4)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 4)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 4)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 4)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 4)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 4)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 4)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 4)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 4)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 4)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 4)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 4)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 4)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 4)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 4)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 4)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 4)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 4)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 4)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 4)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 4)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 4)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 4)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 4)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 4)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 4)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 4)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 4)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 4)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 4)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 4)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 4)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 4)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 4)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 4)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 4)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 4)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 4)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 4)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 4)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 4)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 4)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 4)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 4)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 4)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 4)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 4)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 4)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 4)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 4)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 4)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 4)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 4)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 4)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 4)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 4)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 4)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 4)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 4)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 4)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 4)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 4)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 4)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 4)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 4)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 4)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 4)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 4)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 4)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 4)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 4)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 4)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 4)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 4)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 4)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 4)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 4)\n",
      "182\n",
      "y_hat: (1459, 32, 48, 6, 4), y_hat_i: (3, 32, 48, 6, 4), y_i: (3, 32, 48, 6, 4), batch.x: torch.Size([96, 48, 6, 6]), y: (1459, 32, 48, 6, 4)\n",
      "RMSE for t2m: 3.331505262926854; MAE for t2m: 2.560590427269721;\n",
      "RMSE for sp: 3.828486853941654; MAE for sp: 2.746421681924201;\n",
      "RMSE for tcc: 0.3446257372181257; MAE for tcc: 0.25402682600077436;\n",
      "RMSE for u10: 2.017230892481923; MAE for u10: 1.486937820416798;\n",
      "RMSE for v10: 1.992192312708461; MAE for v10: 1.466825246783192;\n",
      "RMSE for tp: 0.3178089333507821; MAE for tp: 0.08473666008090945;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 4)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 4)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 4)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 4)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 4)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 4)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 4)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 4)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 4)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 4)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 4)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 4)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 4)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 4)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 4)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 4)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 4)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 4)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 4)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 4)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 4)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 4)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 4)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 4)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 4)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 4)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 4)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 4)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 4)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 4)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 4)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 4)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 4)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 4)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 4)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 4)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 4)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 4)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 4)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 4)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 4)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 4)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 4)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 4)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 4)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 4)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 4)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 4)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 4)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 4)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 4)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 4)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 4)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 4)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 4)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 4)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 4)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 4)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 4)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 4)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 4)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 4)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 4)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 4)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 4)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 4)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 4)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 4)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 4)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 4)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 4)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 4)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 4)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 4)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 4)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 4)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 4)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 4)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 4)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 4)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 4)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 4)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 4)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 4)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 4)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 4)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 4)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 4)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 4)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 4)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 4)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 4)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 4)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 4)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 4)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 4)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 4)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 4)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 4)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 4)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 4)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 4)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 4)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 4)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 4)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 4)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 4)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 4)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 4)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 4)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 4)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 4)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 4)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 4)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 4)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 4)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 4)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 4)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 4)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 4)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 4)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 4)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 4)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 4)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 4)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 4)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 4)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 4)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 4)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 4)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 4)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 4)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 4)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 4)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 4)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 4)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 4)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 4)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 4)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 4)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 4)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 4)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 4)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 4)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 4)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 4)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 4)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 4)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 4)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 4)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 4)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 4)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 4)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 4)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 4)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 4)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 4)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 4)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 4)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 4)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 4)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 4)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 4)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 4)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 4)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 4)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 4)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 4)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 4)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 4)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 4)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 4)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 4)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 4)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 4)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 4)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 4)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 4)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 4)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 4)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 4)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 4), y_hat_i: (8, 32, 48, 6, 4), y_i: (8, 32, 48, 6, 4), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 4)\n",
      "182\n",
      "y_hat: (1459, 32, 48, 6, 4), y_hat_i: (3, 32, 48, 6, 4), y_i: (3, 32, 48, 6, 4), batch.x: torch.Size([96, 48, 6, 6]), y: (1459, 32, 48, 6, 4)\n",
      "RMSE for t2m: 3.331505262926854; MAE for t2m: 2.560590427269721;\n",
      "RMSE for sp: 3.828486853941654; MAE for sp: 2.746421681924201;\n",
      "RMSE for tcc: 0.34419975687611853; MAE for tcc: 0.2532118801317655;\n",
      "RMSE for u10: 2.017230892481923; MAE for u10: 1.486937820416798;\n",
      "RMSE for v10: 1.992192312708461; MAE for v10: 1.466825246783192;\n",
      "RMSE for tp: 0.3178089333507821; MAE for tp: 0.08473666008090945;\n",
      "Epoch 1/1000, Train Loss: 0.08077, lr: 0.001--------------------------------| 36.4% Complete\n",
      "Val Loss: 0.07152\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.06884, lr: 0.001\n",
      "Val Loss: 0.06703\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.06604, lr: 0.001\n",
      "Val Loss: 0.06573\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.06539, lr: 0.001\n",
      "Val Loss: 0.06493\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.06434, lr: 0.001\n",
      "Val Loss: 0.06464\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.06169, lr: 0.001\n",
      "Val Loss: 0.06339\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.06052, lr: 0.001\n",
      "Val Loss: 0.06250\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.06000, lr: 0.001\n",
      "Val Loss: 0.06198\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.05967, lr: 0.001\n",
      "Val Loss: 0.06161\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.05938, lr: 0.001\n",
      "Val Loss: 0.06133\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.05893, lr: 0.001\n",
      "Val Loss: 0.06098\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.05813, lr: 0.001\n",
      "Val Loss: 0.05969\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.05730, lr: 0.001\n",
      "Val Loss: 0.05897\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.05668, lr: 0.001\n",
      "Val Loss: 0.05836\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.05622, lr: 0.001\n",
      "Val Loss: 0.05797\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.05591, lr: 0.001\n",
      "Val Loss: 0.05770\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.05568, lr: 0.001\n",
      "Val Loss: 0.05746\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.05552, lr: 0.001\n",
      "Val Loss: 0.05736\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.05539, lr: 0.001\n",
      "Val Loss: 0.05728\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.05528, lr: 0.001\n",
      "Val Loss: 0.05724\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.05517, lr: 0.001\n",
      "Val Loss: 0.05717\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.05507, lr: 0.001\n",
      "Val Loss: 0.05706\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.05498, lr: 0.001\n",
      "Val Loss: 0.05696\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.05490, lr: 0.001\n",
      "Val Loss: 0.05685\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.05482, lr: 0.001\n",
      "Val Loss: 0.05674\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.05475, lr: 0.001\n",
      "Val Loss: 0.05666\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.05468, lr: 0.001\n",
      "Val Loss: 0.05659\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.05461, lr: 0.001\n",
      "Val Loss: 0.05653\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.05454, lr: 0.001\n",
      "Val Loss: 0.05647\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.05447, lr: 0.001\n",
      "Val Loss: 0.05643\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.05440, lr: 0.001\n",
      "Val Loss: 0.05640\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.05432, lr: 0.001\n",
      "Val Loss: 0.05640\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.05427, lr: 0.001\n",
      "Val Loss: 0.05638\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.05420, lr: 0.001\n",
      "Val Loss: 0.05634\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.05412, lr: 0.001\n",
      "Val Loss: 0.05628\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.05401, lr: 0.001\n",
      "Val Loss: 0.05610\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.05385, lr: 0.001\n",
      "Val Loss: 0.05589\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.05366, lr: 0.001\n",
      "Val Loss: 0.05571\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.05350, lr: 0.001\n",
      "Val Loss: 0.05556\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.05336, lr: 0.001\n",
      "Val Loss: 0.05545\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.05325, lr: 0.001\n",
      "Val Loss: 0.05538\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.05316, lr: 0.001\n",
      "Val Loss: 0.05528\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.05307, lr: 0.001\n",
      "Val Loss: 0.05519\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.05300, lr: 0.001\n",
      "Val Loss: 0.05512\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.05294, lr: 0.001\n",
      "Val Loss: 0.05507\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.05288, lr: 0.001\n",
      "Val Loss: 0.05502\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.05283, lr: 0.001\n",
      "Val Loss: 0.05498\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.05279, lr: 0.001\n",
      "Val Loss: 0.05495\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.05274, lr: 0.001\n",
      "Val Loss: 0.05493\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.05271, lr: 0.001\n",
      "Val Loss: 0.05490\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.05267, lr: 0.001\n",
      "Val Loss: 0.05488\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.05263, lr: 0.001\n",
      "Val Loss: 0.05488\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.05260, lr: 0.001\n",
      "Val Loss: 0.05487\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.05257, lr: 0.001\n",
      "Val Loss: 0.05486\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.05254, lr: 0.001\n",
      "Val Loss: 0.05485\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.05251, lr: 0.001\n",
      "Val Loss: 0.05484\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.05247, lr: 0.001\n",
      "Val Loss: 0.05483\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.05244, lr: 0.001\n",
      "Val Loss: 0.05482\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.05242, lr: 0.001\n",
      "Val Loss: 0.05481\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.05239, lr: 0.001\n",
      "Val Loss: 0.05480\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.05236, lr: 0.001\n",
      "Val Loss: 0.05478\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.05233, lr: 0.001\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.05231, lr: 0.001\n",
      "Val Loss: 0.05474\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.05228, lr: 0.001\n",
      "Val Loss: 0.05473\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.05226, lr: 0.001\n",
      "Val Loss: 0.05470\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.05223, lr: 0.001\n",
      "Val Loss: 0.05467\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.05221, lr: 0.001\n",
      "Val Loss: 0.05465\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.05219, lr: 0.001\n",
      "Val Loss: 0.05462\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.05216, lr: 0.001\n",
      "Val Loss: 0.05460\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.05214, lr: 0.001\n",
      "Val Loss: 0.05457\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.05212, lr: 0.001\n",
      "Val Loss: 0.05455\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.05210, lr: 0.001\n",
      "Val Loss: 0.05454\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.05208, lr: 0.001\n",
      "Val Loss: 0.05453\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.05206, lr: 0.001\n",
      "Val Loss: 0.05451\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.05204, lr: 0.001\n",
      "Val Loss: 0.05449\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.05202, lr: 0.001\n",
      "Val Loss: 0.05448\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.05200, lr: 0.001\n",
      "Val Loss: 0.05447\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.05198, lr: 0.001\n",
      "Val Loss: 0.05445\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.05196, lr: 0.001\n",
      "Val Loss: 0.05444\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.05194, lr: 0.001\n",
      "Val Loss: 0.05443\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.05193, lr: 0.001\n",
      "Val Loss: 0.05442\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.05190, lr: 0.001\n",
      "Val Loss: 0.05442\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.05188, lr: 0.001\n",
      "Val Loss: 0.05442\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.05186, lr: 0.001\n",
      "Val Loss: 0.05442\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.05185, lr: 0.001\n",
      "Val Loss: 0.05442\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.05183, lr: 0.001\n",
      "Val Loss: 0.05443\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.05181, lr: 0.001\n",
      "Val Loss: 0.05443\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.05179, lr: 0.001\n",
      "Val Loss: 0.05443\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.05178, lr: 0.001\n",
      "Val Loss: 0.05444\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.05176, lr: 0.001\n",
      "Val Loss: 0.05445\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 91/1000, Train Loss: 0.05117, lr: 0.0005\n",
      "Val Loss: 0.05332\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.05110, lr: 0.0005\n",
      "Val Loss: 0.05332\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.05109, lr: 0.0005\n",
      "Val Loss: 0.05332\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.05107, lr: 0.0005\n",
      "Val Loss: 0.05333\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.05106, lr: 0.0005\n",
      "Val Loss: 0.05333\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.05105, lr: 0.0005\n",
      "Val Loss: 0.05333\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.05104, lr: 0.0005\n",
      "Val Loss: 0.05333\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.05103, lr: 0.0005\n",
      "Val Loss: 0.05333\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.05102, lr: 0.0005\n",
      "Val Loss: 0.05333\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 100/1000, Train Loss: 0.05068, lr: 0.00025\n",
      "Val Loss: 0.05297\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.05065, lr: 0.00025\n",
      "Val Loss: 0.05296\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.05064, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 103/1000, Train Loss: 0.05064, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.05063, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.05062, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.05062, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.05061, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.05060, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.05060, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.05059, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.05059, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.05058, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.05057, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 114/1000, Train Loss: 0.05057, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.05056, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.05056, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.05055, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.05054, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.05054, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.05053, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 121/1000, Train Loss: 0.05053, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.05052, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.05052, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.05051, lr: 0.00025\n",
      "Val Loss: 0.05295\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 125/1000, Train Loss: 0.05031, lr: 0.000125\n",
      "Val Loss: 0.05282\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.05028, lr: 0.000125\n",
      "Val Loss: 0.05282\n",
      "---------\n",
      "Epoch 127/1000, Train Loss: 0.05028, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 128/1000, Train Loss: 0.05027, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.05027, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.05027, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.05026, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.05026, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.05026, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 134/1000, Train Loss: 0.05025, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 135/1000, Train Loss: 0.05025, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 136/1000, Train Loss: 0.05025, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.05024, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.05024, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.05024, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.05024, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 141/1000, Train Loss: 0.05023, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 142/1000, Train Loss: 0.05023, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.05023, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.05022, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 145/1000, Train Loss: 0.05022, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 146/1000, Train Loss: 0.05022, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 147/1000, Train Loss: 0.05021, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 148/1000, Train Loss: 0.05021, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 149/1000, Train Loss: 0.05021, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 150/1000, Train Loss: 0.05020, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 151/1000, Train Loss: 0.05020, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 152/1000, Train Loss: 0.05020, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 153/1000, Train Loss: 0.05020, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "Epoch 154/1000, Train Loss: 0.05019, lr: 0.000125\n",
      "Val Loss: 0.05281\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 155/1000, Train Loss: 0.05009, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 156/1000, Train Loss: 0.05008, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 157/1000, Train Loss: 0.05008, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 158/1000, Train Loss: 0.05007, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 159/1000, Train Loss: 0.05007, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 160/1000, Train Loss: 0.05007, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 161/1000, Train Loss: 0.05007, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 162/1000, Train Loss: 0.05007, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 163/1000, Train Loss: 0.05006, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 164/1000, Train Loss: 0.05006, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 165/1000, Train Loss: 0.05006, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 166/1000, Train Loss: 0.05006, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 167/1000, Train Loss: 0.05006, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 168/1000, Train Loss: 0.05006, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 169/1000, Train Loss: 0.05005, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 170/1000, Train Loss: 0.05005, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 171/1000, Train Loss: 0.05005, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 172/1000, Train Loss: 0.05005, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 173/1000, Train Loss: 0.05005, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 174/1000, Train Loss: 0.05005, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 175/1000, Train Loss: 0.05005, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 176/1000, Train Loss: 0.05004, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 177/1000, Train Loss: 0.05004, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "Epoch 178/1000, Train Loss: 0.05004, lr: 6.25e-05\n",
      "Val Loss: 0.05275\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 179/1000, Train Loss: 0.04999, lr: 3.125e-05\n",
      "Val Loss: 0.05270\n",
      "---------\n",
      "Epoch 180/1000, Train Loss: 0.04998, lr: 3.125e-05\n",
      "Val Loss: 0.05270\n",
      "---------\n",
      "Epoch 181/1000, Train Loss: 0.04998, lr: 3.125e-05\n",
      "Val Loss: 0.05270\n",
      "---------\n",
      "Epoch 182/1000, Train Loss: 0.04998, lr: 3.125e-05\n",
      "Val Loss: 0.05270\n",
      "---------\n",
      "Epoch 183/1000, Train Loss: 0.04998, lr: 3.125e-05\n",
      "Val Loss: 0.05270\n",
      "---------\n",
      "Epoch 184/1000, Train Loss: 0.04998, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 185/1000, Train Loss: 0.04998, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 186/1000, Train Loss: 0.04997, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 187/1000, Train Loss: 0.04997, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 188/1000, Train Loss: 0.04997, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 189/1000, Train Loss: 0.04997, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 190/1000, Train Loss: 0.04997, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 191/1000, Train Loss: 0.04997, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 192/1000, Train Loss: 0.04997, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 193/1000, Train Loss: 0.04997, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 194/1000, Train Loss: 0.04997, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 195/1000, Train Loss: 0.04997, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 196/1000, Train Loss: 0.04997, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 197/1000, Train Loss: 0.04997, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 198/1000, Train Loss: 0.04996, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 199/1000, Train Loss: 0.04996, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 200/1000, Train Loss: 0.04996, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 201/1000, Train Loss: 0.04996, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 202/1000, Train Loss: 0.04996, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 203/1000, Train Loss: 0.04996, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 204/1000, Train Loss: 0.04996, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 205/1000, Train Loss: 0.04996, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 206/1000, Train Loss: 0.04996, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 207/1000, Train Loss: 0.04996, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 208/1000, Train Loss: 0.04996, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 209/1000, Train Loss: 0.04996, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 210/1000, Train Loss: 0.04996, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 211/1000, Train Loss: 0.04995, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 212/1000, Train Loss: 0.04995, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 213/1000, Train Loss: 0.04995, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 214/1000, Train Loss: 0.04995, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 215/1000, Train Loss: 0.04995, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 216/1000, Train Loss: 0.04995, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 217/1000, Train Loss: 0.04995, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 218/1000, Train Loss: 0.04995, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 219/1000, Train Loss: 0.04995, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 220/1000, Train Loss: 0.04995, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 221/1000, Train Loss: 0.04995, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 222/1000, Train Loss: 0.04995, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 223/1000, Train Loss: 0.04995, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 224/1000, Train Loss: 0.04994, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 225/1000, Train Loss: 0.04994, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 226/1000, Train Loss: 0.04994, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 227/1000, Train Loss: 0.04994, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 228/1000, Train Loss: 0.04994, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 229/1000, Train Loss: 0.04994, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 230/1000, Train Loss: 0.04994, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 231/1000, Train Loss: 0.04994, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 232/1000, Train Loss: 0.04994, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 233/1000, Train Loss: 0.04994, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 234/1000, Train Loss: 0.04994, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 235/1000, Train Loss: 0.04994, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 236/1000, Train Loss: 0.04994, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 237/1000, Train Loss: 0.04994, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 238/1000, Train Loss: 0.04993, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 239/1000, Train Loss: 0.04993, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 240/1000, Train Loss: 0.04993, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 241/1000, Train Loss: 0.04993, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 242/1000, Train Loss: 0.04993, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 243/1000, Train Loss: 0.04993, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 244/1000, Train Loss: 0.04993, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 245/1000, Train Loss: 0.04993, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 246/1000, Train Loss: 0.04993, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 247/1000, Train Loss: 0.04993, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 248/1000, Train Loss: 0.04993, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 249/1000, Train Loss: 0.04993, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 250/1000, Train Loss: 0.04993, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 251/1000, Train Loss: 0.04992, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 252/1000, Train Loss: 0.04992, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 253/1000, Train Loss: 0.04992, lr: 3.125e-05\n",
      "Val Loss: 0.05269\n",
      "---------\n",
      "Epoch 254/1000, Train Loss: 0.04992, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 255/1000, Train Loss: 0.04992, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 256/1000, Train Loss: 0.04992, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 257/1000, Train Loss: 0.04992, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 258/1000, Train Loss: 0.04992, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 259/1000, Train Loss: 0.04992, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 260/1000, Train Loss: 0.04992, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 261/1000, Train Loss: 0.04992, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 262/1000, Train Loss: 0.04992, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 263/1000, Train Loss: 0.04992, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 264/1000, Train Loss: 0.04992, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 265/1000, Train Loss: 0.04992, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 266/1000, Train Loss: 0.04991, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 267/1000, Train Loss: 0.04991, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 268/1000, Train Loss: 0.04991, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 269/1000, Train Loss: 0.04991, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 270/1000, Train Loss: 0.04991, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 271/1000, Train Loss: 0.04991, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 272/1000, Train Loss: 0.04991, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 273/1000, Train Loss: 0.04991, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 274/1000, Train Loss: 0.04991, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 275/1000, Train Loss: 0.04991, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 276/1000, Train Loss: 0.04991, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 277/1000, Train Loss: 0.04991, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 278/1000, Train Loss: 0.04991, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 279/1000, Train Loss: 0.04991, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 280/1000, Train Loss: 0.04990, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 281/1000, Train Loss: 0.04990, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 282/1000, Train Loss: 0.04990, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 283/1000, Train Loss: 0.04990, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 284/1000, Train Loss: 0.04990, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 285/1000, Train Loss: 0.04990, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 286/1000, Train Loss: 0.04990, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 287/1000, Train Loss: 0.04990, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 288/1000, Train Loss: 0.04990, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 289/1000, Train Loss: 0.04990, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 290/1000, Train Loss: 0.04990, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 291/1000, Train Loss: 0.04990, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 292/1000, Train Loss: 0.04990, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 293/1000, Train Loss: 0.04990, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 294/1000, Train Loss: 0.04990, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 295/1000, Train Loss: 0.04989, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 296/1000, Train Loss: 0.04989, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 297/1000, Train Loss: 0.04989, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 298/1000, Train Loss: 0.04989, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 299/1000, Train Loss: 0.04989, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 300/1000, Train Loss: 0.04989, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 301/1000, Train Loss: 0.04989, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 302/1000, Train Loss: 0.04989, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 303/1000, Train Loss: 0.04989, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 304/1000, Train Loss: 0.04989, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 305/1000, Train Loss: 0.04989, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 306/1000, Train Loss: 0.04989, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 307/1000, Train Loss: 0.04989, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 308/1000, Train Loss: 0.04989, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 309/1000, Train Loss: 0.04989, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 310/1000, Train Loss: 0.04989, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 311/1000, Train Loss: 0.04988, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 312/1000, Train Loss: 0.04988, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 313/1000, Train Loss: 0.04988, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 314/1000, Train Loss: 0.04988, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 315/1000, Train Loss: 0.04988, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 316/1000, Train Loss: 0.04988, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 317/1000, Train Loss: 0.04988, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 318/1000, Train Loss: 0.04988, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 319/1000, Train Loss: 0.04988, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 320/1000, Train Loss: 0.04988, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 321/1000, Train Loss: 0.04988, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 322/1000, Train Loss: 0.04988, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 323/1000, Train Loss: 0.04988, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 324/1000, Train Loss: 0.04988, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 325/1000, Train Loss: 0.04988, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 326/1000, Train Loss: 0.04988, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 327/1000, Train Loss: 0.04987, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 328/1000, Train Loss: 0.04987, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 329/1000, Train Loss: 0.04987, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 330/1000, Train Loss: 0.04987, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 331/1000, Train Loss: 0.04987, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 332/1000, Train Loss: 0.04987, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 333/1000, Train Loss: 0.04987, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 334/1000, Train Loss: 0.04987, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 335/1000, Train Loss: 0.04987, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 336/1000, Train Loss: 0.04987, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 337/1000, Train Loss: 0.04987, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 338/1000, Train Loss: 0.04987, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 339/1000, Train Loss: 0.04987, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 340/1000, Train Loss: 0.04987, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 341/1000, Train Loss: 0.04987, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 342/1000, Train Loss: 0.04987, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 343/1000, Train Loss: 0.04986, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 344/1000, Train Loss: 0.04986, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 345/1000, Train Loss: 0.04986, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 346/1000, Train Loss: 0.04986, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 347/1000, Train Loss: 0.04986, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 348/1000, Train Loss: 0.04986, lr: 3.125e-05\n",
      "Val Loss: 0.05268\n",
      "---------\n",
      "Epoch 349/1000, Train Loss: 0.04986, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 350/1000, Train Loss: 0.04986, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 351/1000, Train Loss: 0.04986, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 352/1000, Train Loss: 0.04986, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 353/1000, Train Loss: 0.04986, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 354/1000, Train Loss: 0.04986, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 355/1000, Train Loss: 0.04986, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 356/1000, Train Loss: 0.04986, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 357/1000, Train Loss: 0.04986, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 358/1000, Train Loss: 0.04986, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 359/1000, Train Loss: 0.04985, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 360/1000, Train Loss: 0.04985, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 361/1000, Train Loss: 0.04985, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 362/1000, Train Loss: 0.04985, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 363/1000, Train Loss: 0.04985, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 364/1000, Train Loss: 0.04985, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 365/1000, Train Loss: 0.04985, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 366/1000, Train Loss: 0.04985, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 367/1000, Train Loss: 0.04985, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 368/1000, Train Loss: 0.04985, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 369/1000, Train Loss: 0.04985, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 370/1000, Train Loss: 0.04985, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 371/1000, Train Loss: 0.04985, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 372/1000, Train Loss: 0.04985, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 373/1000, Train Loss: 0.04985, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 374/1000, Train Loss: 0.04985, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 375/1000, Train Loss: 0.04985, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 376/1000, Train Loss: 0.04984, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 377/1000, Train Loss: 0.04984, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 378/1000, Train Loss: 0.04984, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 379/1000, Train Loss: 0.04984, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 380/1000, Train Loss: 0.04984, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 381/1000, Train Loss: 0.04984, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 382/1000, Train Loss: 0.04984, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 383/1000, Train Loss: 0.04984, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 384/1000, Train Loss: 0.04984, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 385/1000, Train Loss: 0.04984, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 386/1000, Train Loss: 0.04984, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 387/1000, Train Loss: 0.04984, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 388/1000, Train Loss: 0.04984, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 389/1000, Train Loss: 0.04984, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 390/1000, Train Loss: 0.04984, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 391/1000, Train Loss: 0.04984, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 392/1000, Train Loss: 0.04984, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 393/1000, Train Loss: 0.04983, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 394/1000, Train Loss: 0.04983, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 395/1000, Train Loss: 0.04983, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 396/1000, Train Loss: 0.04983, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 397/1000, Train Loss: 0.04983, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 398/1000, Train Loss: 0.04983, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 399/1000, Train Loss: 0.04983, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 400/1000, Train Loss: 0.04983, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 401/1000, Train Loss: 0.04983, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 402/1000, Train Loss: 0.04983, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 403/1000, Train Loss: 0.04983, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 404/1000, Train Loss: 0.04983, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 405/1000, Train Loss: 0.04983, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 406/1000, Train Loss: 0.04983, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 407/1000, Train Loss: 0.04983, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 408/1000, Train Loss: 0.04983, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 409/1000, Train Loss: 0.04983, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 410/1000, Train Loss: 0.04983, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 411/1000, Train Loss: 0.04982, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 412/1000, Train Loss: 0.04982, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 413/1000, Train Loss: 0.04982, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 414/1000, Train Loss: 0.04982, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 415/1000, Train Loss: 0.04982, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 416/1000, Train Loss: 0.04982, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 417/1000, Train Loss: 0.04982, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 418/1000, Train Loss: 0.04982, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 419/1000, Train Loss: 0.04982, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 420/1000, Train Loss: 0.04982, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 421/1000, Train Loss: 0.04982, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 422/1000, Train Loss: 0.04982, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 423/1000, Train Loss: 0.04982, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 424/1000, Train Loss: 0.04982, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 425/1000, Train Loss: 0.04982, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 426/1000, Train Loss: 0.04982, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 427/1000, Train Loss: 0.04982, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 428/1000, Train Loss: 0.04982, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 429/1000, Train Loss: 0.04981, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 430/1000, Train Loss: 0.04981, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 431/1000, Train Loss: 0.04981, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 432/1000, Train Loss: 0.04981, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 433/1000, Train Loss: 0.04981, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 434/1000, Train Loss: 0.04981, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 435/1000, Train Loss: 0.04981, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 436/1000, Train Loss: 0.04981, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 437/1000, Train Loss: 0.04981, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 438/1000, Train Loss: 0.04981, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 439/1000, Train Loss: 0.04981, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 440/1000, Train Loss: 0.04981, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 441/1000, Train Loss: 0.04981, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 442/1000, Train Loss: 0.04981, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 443/1000, Train Loss: 0.04981, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 444/1000, Train Loss: 0.04981, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 445/1000, Train Loss: 0.04981, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 446/1000, Train Loss: 0.04981, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 447/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 448/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 449/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 450/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 451/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 452/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 453/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 454/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 455/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 456/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 457/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 458/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 459/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 460/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 461/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 462/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05267\n",
      "---------\n",
      "Epoch 463/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 464/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 465/1000, Train Loss: 0.04980, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 466/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 467/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 468/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 469/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 470/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 471/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 472/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 473/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 474/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 475/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 476/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 477/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 478/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 479/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 480/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 481/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 482/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 483/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 484/1000, Train Loss: 0.04979, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 485/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 486/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 487/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 488/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 489/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 490/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 491/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 492/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 493/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 494/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 495/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 496/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 497/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 498/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 499/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 500/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 501/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 502/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 503/1000, Train Loss: 0.04978, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 504/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 505/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 506/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 507/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 508/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 509/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 510/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 511/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 512/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 513/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 514/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 515/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 516/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 517/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 518/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 519/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 520/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 521/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 522/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 523/1000, Train Loss: 0.04977, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 524/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 525/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 526/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 527/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 528/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 529/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 530/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 531/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 532/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 533/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 534/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 535/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 536/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 537/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 538/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 539/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 540/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 541/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 542/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 543/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 544/1000, Train Loss: 0.04976, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 545/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 546/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 547/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 548/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 549/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 550/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 551/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 552/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 553/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 554/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 555/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 556/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05266\n",
      "---------\n",
      "Epoch 557/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 558/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 559/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 560/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 561/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 562/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 563/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 564/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 565/1000, Train Loss: 0.04975, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 566/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 567/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 568/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 569/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 570/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 571/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 572/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 573/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 574/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 575/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 576/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 577/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 578/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 579/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 580/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 581/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 582/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 583/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 584/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 585/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 586/1000, Train Loss: 0.04974, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 587/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 588/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 589/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 590/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 591/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 592/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 593/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 594/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 595/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 596/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 597/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 598/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 599/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 600/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 601/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 602/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 603/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 604/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 605/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 606/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 607/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 608/1000, Train Loss: 0.04973, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 609/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 610/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 611/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 612/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 613/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 614/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 615/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 616/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 617/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 618/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 619/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 620/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 621/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 622/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 623/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 624/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 625/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 626/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 627/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 628/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 629/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 630/1000, Train Loss: 0.04972, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 631/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 632/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 633/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 634/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 635/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 636/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 637/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 638/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 639/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 640/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 641/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 642/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 643/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 644/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 645/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 646/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 647/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 648/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 649/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 650/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 651/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 652/1000, Train Loss: 0.04971, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 653/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 654/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 655/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 656/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 657/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 658/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 659/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 660/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 661/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 662/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 663/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 664/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05265\n",
      "---------\n",
      "Epoch 665/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 666/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 667/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 668/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 669/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 670/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 671/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 672/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 673/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 674/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 675/1000, Train Loss: 0.04970, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 676/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 677/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 678/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 679/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 680/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 681/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 682/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 683/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 684/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 685/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 686/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 687/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 688/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 689/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 690/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 691/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 692/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 693/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 694/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 695/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 696/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 697/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 698/1000, Train Loss: 0.04969, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 699/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 700/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 701/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 702/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 703/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 704/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 705/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 706/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 707/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 708/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 709/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 710/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 711/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 712/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 713/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 714/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 715/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 716/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 717/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 718/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 719/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 720/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 721/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 722/1000, Train Loss: 0.04968, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 723/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 724/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 725/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 726/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 727/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 728/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 729/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 730/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 731/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 732/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 733/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 734/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 735/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 736/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 737/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 738/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 739/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 740/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 741/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 742/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 743/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 744/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 745/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 746/1000, Train Loss: 0.04967, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 747/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 748/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 749/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 750/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 751/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 752/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 753/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 754/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 755/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 756/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 757/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 758/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 759/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 760/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 761/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 762/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 763/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 764/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 765/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 766/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 767/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 768/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05264\n",
      "---------\n",
      "Epoch 769/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 770/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 771/1000, Train Loss: 0.04966, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 772/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 773/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 774/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 775/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 776/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 777/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 778/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 779/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 780/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 781/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 782/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 783/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 784/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 785/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 786/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 787/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 788/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 789/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 790/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 791/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 792/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 793/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 794/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 795/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 796/1000, Train Loss: 0.04965, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 797/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 798/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 799/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 800/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 801/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 802/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 803/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 804/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 805/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 806/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 807/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 808/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 809/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 810/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 811/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 812/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 813/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 814/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 815/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 816/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 817/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 818/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 819/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 820/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 821/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 822/1000, Train Loss: 0.04964, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 823/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 824/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 825/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 826/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 827/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 828/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 829/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 830/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 831/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 832/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 833/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 834/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 835/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 836/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 837/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 838/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 839/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 840/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 841/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 842/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 843/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 844/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 845/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 846/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 847/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 848/1000, Train Loss: 0.04963, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 849/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 850/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 851/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 852/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 853/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 854/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 855/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 856/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 857/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05263\n",
      "---------\n",
      "Epoch 858/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 859/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 860/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 861/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 862/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 863/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 864/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 865/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 866/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 867/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 868/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 869/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 870/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 871/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 872/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 873/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 874/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 875/1000, Train Loss: 0.04962, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 876/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 877/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 878/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 879/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 880/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 881/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 882/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 883/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 884/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 885/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 886/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 887/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 888/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 889/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 890/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 891/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 892/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 893/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 894/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 895/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 896/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 897/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 898/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 899/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 900/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 901/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 902/1000, Train Loss: 0.04961, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 903/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 904/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 905/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 906/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 907/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 908/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 909/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 910/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 911/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 912/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 913/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 914/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 915/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 916/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 917/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 918/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 919/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 920/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 921/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 922/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 923/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 924/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 925/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 926/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 927/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 928/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 929/1000, Train Loss: 0.04960, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 930/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 931/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 932/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 933/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 934/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 935/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 936/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 937/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 938/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 939/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 940/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 941/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 942/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 943/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 944/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 945/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 946/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 947/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 948/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 949/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 950/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 951/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 952/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 953/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 954/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05262\n",
      "---------\n",
      "Epoch 955/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 956/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 957/1000, Train Loss: 0.04959, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 958/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 959/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 960/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 961/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 962/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 963/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 964/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 965/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 966/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 967/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 968/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 969/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 970/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 971/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 972/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 973/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 974/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 975/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 976/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 977/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 978/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 979/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 980/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 981/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 982/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 983/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 984/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 985/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 986/1000, Train Loss: 0.04958, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 987/1000, Train Loss: 0.04957, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 988/1000, Train Loss: 0.04957, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 989/1000, Train Loss: 0.04957, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 990/1000, Train Loss: 0.04957, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 991/1000, Train Loss: 0.04957, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 992/1000, Train Loss: 0.04957, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 993/1000, Train Loss: 0.04957, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 994/1000, Train Loss: 0.04957, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 995/1000, Train Loss: 0.04957, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 996/1000, Train Loss: 0.04957, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 997/1000, Train Loss: 0.04957, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 998/1000, Train Loss: 0.04957, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 999/1000, Train Loss: 0.04957, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "Epoch 1000/1000, Train Loss: 0.04957, lr: 3.125e-05\n",
      "Val Loss: 0.05261\n",
      "---------\n",
      "5293.622930049896 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 5)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 5)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 5)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 5)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 5)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 5)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 5)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 5)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 5)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 5)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 5)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 5)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 5)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 5)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 5)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 5)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 5)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 5)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 5)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 5)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 5)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 5)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 5)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 5)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 5)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 5)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 5)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 5)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 5)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 5)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 5)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 5)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 5)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 5)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 5)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 5)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 5)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 5)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 5)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 5)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 5)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 5)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 5)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 5)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 5)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 5)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 5)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 5)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 5)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 5)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 5)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 5)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 5)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 5)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 5)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 5)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 5)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 5)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 5)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 5)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 5)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 5)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 5)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 5)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 5)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 5)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 5)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 5)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 5)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 5)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 5)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 5)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 5)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 5)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 5)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 5)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 5)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 5)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 5)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 5)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 5)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 5)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 5)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 5)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 5)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 5)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 5)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 5)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 5)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 5)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 5)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 5)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 5)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 5)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 5)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 5)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 5)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 5)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 5)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 5)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 5)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 5)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 5)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 5)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 5)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 5)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 5)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 5)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 5)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 5)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 5)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 5)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 5)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 5)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 5)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 5)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 5)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 5)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 5)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 5)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 5)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 5)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 5)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 5)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 5)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 5)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 5)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 5)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 5)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 5)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 5)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 5)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 5)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 5)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 5)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 5)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 5)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 5)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 5)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 5)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 5)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 5)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 5)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 5)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 5)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 5)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 5)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 5)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 5)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 5)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 5)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 5)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 5)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 5)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 5)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 5)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 5)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 5)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 5)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 5)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 5)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 5)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 5)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 5)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 5)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 5)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 5)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 5)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 5)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 5)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 5)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 5)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 5)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 5)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 5)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 5)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 5)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 5)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 5)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 5)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 5)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 5)\n",
      "182\n",
      "y_hat: (1458, 32, 48, 6, 5), y_hat_i: (2, 32, 48, 6, 5), y_i: (2, 32, 48, 6, 5), batch.x: torch.Size([64, 48, 6, 6]), y: (1458, 32, 48, 6, 5)\n",
      "RMSE for t2m: 3.431775484940609; MAE for t2m: 2.619723659812174;\n",
      "RMSE for sp: 4.358160332345994; MAE for sp: 3.085600528714599;\n",
      "RMSE for tcc: 0.35361773153139603; MAE for tcc: 0.2610958274127023;\n",
      "RMSE for u10: 2.2226854102229643; MAE for u10: 1.6514513507621535;\n",
      "RMSE for v10: 2.1795392068881925; MAE for v10: 1.6216035582677508;\n",
      "RMSE for tp: 0.3141323844741894; MAE for tp: 0.08379416899657478;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 5)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 5)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 5)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 5)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 5)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 5)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 5)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 5)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 5)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 5)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 5)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 5)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 5)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 5)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 5)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 5)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 5)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 5)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 5)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 5)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 5)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 5)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 5)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 5)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 5)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 5)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 5)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 5)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 5)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 5)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 5)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 5)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 5)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 5)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 5)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 5)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 5)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 5)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 5)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 5)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 5)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 5)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 5)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 5)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 5)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 5)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 5)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 5)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 5)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 5)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 5)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 5)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 5)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 5)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 5)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 5)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 5)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 5)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 5)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 5)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 5)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 5)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 5)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 5)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 5)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 5)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 5)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 5)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 5)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 5)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 5)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 5)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 5)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 5)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 5)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 5)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 5)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 5)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 5)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 5)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 5)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 5)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 5)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 5)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 5)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 5)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 5)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 5)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 5)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 5)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 5)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 5)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 5)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 5)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 5)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 5)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 5)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 5)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 5)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 5)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 5)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 5)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 5)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 5)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 5)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 5)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 5)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 5)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 5)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 5)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 5)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 5)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 5)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 5)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 5)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 5)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 5)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 5)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 5)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 5)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 5)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 5)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 5)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 5)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 5)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 5)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 5)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 5)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 5)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 5)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 5)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 5)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 5)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 5)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 5)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 5)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 5)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 5)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 5)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 5)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 5)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 5)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 5)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 5)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 5)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 5)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 5)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 5)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 5)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 5)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 5)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 5)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 5)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 5)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 5)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 5)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 5)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 5)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 5)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 5)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 5)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 5)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 5)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 5)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 5)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 5)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 5)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 5)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 5)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 5)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 5)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 5)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 5)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 5)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 5)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 5)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 5)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 5)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 5)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 5)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 5)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 5), y_hat_i: (8, 32, 48, 6, 5), y_i: (8, 32, 48, 6, 5), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 5)\n",
      "182\n",
      "y_hat: (1458, 32, 48, 6, 5), y_hat_i: (2, 32, 48, 6, 5), y_i: (2, 32, 48, 6, 5), batch.x: torch.Size([64, 48, 6, 6]), y: (1458, 32, 48, 6, 5)\n",
      "RMSE for t2m: 3.431775484940609; MAE for t2m: 2.619723659812174;\n",
      "RMSE for sp: 4.358160332345994; MAE for sp: 3.085600528714599;\n",
      "RMSE for tcc: 0.35318289751497217; MAE for tcc: 0.2602826100077596;\n",
      "RMSE for u10: 2.2226854102229643; MAE for u10: 1.6514513507621535;\n",
      "RMSE for v10: 2.1795392068881925; MAE for v10: 1.6216035582677508;\n",
      "RMSE for tp: 0.3141323844741894; MAE for tp: 0.08379416899657478;\n",
      "Epoch 1/1000, Train Loss: 0.08032, lr: 0.001----------------------------| 45.5% Complete\n",
      "Val Loss: 0.06837\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.06768, lr: 0.001\n",
      "Val Loss: 0.06623\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.06629, lr: 0.001\n",
      "Val Loss: 0.06556\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.06448, lr: 0.001\n",
      "Val Loss: 0.06327\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.06260, lr: 0.001\n",
      "Val Loss: 0.06245\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.06170, lr: 0.001\n",
      "Val Loss: 0.06227\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.06127, lr: 0.001\n",
      "Val Loss: 0.06218\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.06100, lr: 0.001\n",
      "Val Loss: 0.06208\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.06079, lr: 0.001\n",
      "Val Loss: 0.06201\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.06059, lr: 0.001\n",
      "Val Loss: 0.06190\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.06040, lr: 0.001\n",
      "Val Loss: 0.06181\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.06023, lr: 0.001\n",
      "Val Loss: 0.06171\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.06004, lr: 0.001\n",
      "Val Loss: 0.06160\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.05980, lr: 0.001\n",
      "Val Loss: 0.06134\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.05941, lr: 0.001\n",
      "Val Loss: 0.06055\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.05871, lr: 0.001\n",
      "Val Loss: 0.05929\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.05769, lr: 0.001\n",
      "Val Loss: 0.05855\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.05720, lr: 0.001\n",
      "Val Loss: 0.05824\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.05688, lr: 0.001\n",
      "Val Loss: 0.05807\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.05665, lr: 0.001\n",
      "Val Loss: 0.05804\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.05649, lr: 0.001\n",
      "Val Loss: 0.05799\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.05634, lr: 0.001\n",
      "Val Loss: 0.05791\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.05617, lr: 0.001\n",
      "Val Loss: 0.05767\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.05591, lr: 0.001\n",
      "Val Loss: 0.05747\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.05563, lr: 0.001\n",
      "Val Loss: 0.05756\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.05545, lr: 0.001\n",
      "Val Loss: 0.05739\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.05527, lr: 0.001\n",
      "Val Loss: 0.05728\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.05514, lr: 0.001\n",
      "Val Loss: 0.05718\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.05504, lr: 0.001\n",
      "Val Loss: 0.05710\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.05496, lr: 0.001\n",
      "Val Loss: 0.05706\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.05490, lr: 0.001\n",
      "Val Loss: 0.05702\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.05484, lr: 0.001\n",
      "Val Loss: 0.05698\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.05479, lr: 0.001\n",
      "Val Loss: 0.05695\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.05474, lr: 0.001\n",
      "Val Loss: 0.05692\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.05470, lr: 0.001\n",
      "Val Loss: 0.05688\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.05466, lr: 0.001\n",
      "Val Loss: 0.05682\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.05462, lr: 0.001\n",
      "Val Loss: 0.05676\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.05457, lr: 0.001\n",
      "Val Loss: 0.05668\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.05453, lr: 0.001\n",
      "Val Loss: 0.05662\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.05449, lr: 0.001\n",
      "Val Loss: 0.05657\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.05445, lr: 0.001\n",
      "Val Loss: 0.05654\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.05441, lr: 0.001\n",
      "Val Loss: 0.05652\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.05438, lr: 0.001\n",
      "Val Loss: 0.05650\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.05435, lr: 0.001\n",
      "Val Loss: 0.05649\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.05432, lr: 0.001\n",
      "Val Loss: 0.05648\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.05429, lr: 0.001\n",
      "Val Loss: 0.05646\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.05427, lr: 0.001\n",
      "Val Loss: 0.05646\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.05424, lr: 0.001\n",
      "Val Loss: 0.05644\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.05422, lr: 0.001\n",
      "Val Loss: 0.05642\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.05419, lr: 0.001\n",
      "Val Loss: 0.05641\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.05417, lr: 0.001\n",
      "Val Loss: 0.05640\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.05414, lr: 0.001\n",
      "Val Loss: 0.05639\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.05412, lr: 0.001\n",
      "Val Loss: 0.05639\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.05410, lr: 0.001\n",
      "Val Loss: 0.05638\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.05408, lr: 0.001\n",
      "Val Loss: 0.05636\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.05406, lr: 0.001\n",
      "Val Loss: 0.05635\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.05404, lr: 0.001\n",
      "Val Loss: 0.05634\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.05402, lr: 0.001\n",
      "Val Loss: 0.05634\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.05400, lr: 0.001\n",
      "Val Loss: 0.05633\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.05399, lr: 0.001\n",
      "Val Loss: 0.05633\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.05397, lr: 0.001\n",
      "Val Loss: 0.05633\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.05396, lr: 0.001\n",
      "Val Loss: 0.05634\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.05394, lr: 0.001\n",
      "Val Loss: 0.05634\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.05393, lr: 0.001\n",
      "Val Loss: 0.05634\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.05391, lr: 0.001\n",
      "Val Loss: 0.05634\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.05389, lr: 0.001\n",
      "Val Loss: 0.05635\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.05388, lr: 0.001\n",
      "Val Loss: 0.05635\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 68/1000, Train Loss: 0.05310, lr: 0.0005\n",
      "Val Loss: 0.05525\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.05303, lr: 0.0005\n",
      "Val Loss: 0.05525\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.05301, lr: 0.0005\n",
      "Val Loss: 0.05525\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.05299, lr: 0.0005\n",
      "Val Loss: 0.05525\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.05298, lr: 0.0005\n",
      "Val Loss: 0.05526\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.05297, lr: 0.0005\n",
      "Val Loss: 0.05526\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.05296, lr: 0.0005\n",
      "Val Loss: 0.05526\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.05295, lr: 0.0005\n",
      "Val Loss: 0.05526\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.05294, lr: 0.0005\n",
      "Val Loss: 0.05526\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 77/1000, Train Loss: 0.05257, lr: 0.00025\n",
      "Val Loss: 0.05490\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.05252, lr: 0.00025\n",
      "Val Loss: 0.05490\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.05251, lr: 0.00025\n",
      "Val Loss: 0.05490\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.05251, lr: 0.00025\n",
      "Val Loss: 0.05490\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.05250, lr: 0.00025\n",
      "Val Loss: 0.05490\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.05249, lr: 0.00025\n",
      "Val Loss: 0.05490\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.05248, lr: 0.00025\n",
      "Val Loss: 0.05490\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.05248, lr: 0.00025\n",
      "Val Loss: 0.05490\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.05247, lr: 0.00025\n",
      "Val Loss: 0.05490\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.05246, lr: 0.00025\n",
      "Val Loss: 0.05490\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.05245, lr: 0.00025\n",
      "Val Loss: 0.05490\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 88/1000, Train Loss: 0.05228, lr: 0.000125\n",
      "Val Loss: 0.05486\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.05226, lr: 0.000125\n",
      "Val Loss: 0.05486\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.05225, lr: 0.000125\n",
      "Val Loss: 0.05486\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.05225, lr: 0.000125\n",
      "Val Loss: 0.05486\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.05224, lr: 0.000125\n",
      "Val Loss: 0.05486\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.05224, lr: 0.000125\n",
      "Val Loss: 0.05486\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.05223, lr: 0.000125\n",
      "Val Loss: 0.05486\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.05223, lr: 0.000125\n",
      "Val Loss: 0.05486\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.05223, lr: 0.000125\n",
      "Val Loss: 0.05486\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 97/1000, Train Loss: 0.05213, lr: 6.25e-05\n",
      "Val Loss: 0.05477\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.05212, lr: 6.25e-05\n",
      "Val Loss: 0.05477\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.05211, lr: 6.25e-05\n",
      "Val Loss: 0.05477\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.05211, lr: 6.25e-05\n",
      "Val Loss: 0.05477\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.05211, lr: 6.25e-05\n",
      "Val Loss: 0.05477\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.05211, lr: 6.25e-05\n",
      "Val Loss: 0.05477\n",
      "---------\n",
      "Epoch 103/1000, Train Loss: 0.05210, lr: 6.25e-05\n",
      "Val Loss: 0.05477\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.05210, lr: 6.25e-05\n",
      "Val Loss: 0.05477\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 105/1000, Train Loss: 0.05205, lr: 3.125e-05\n",
      "Val Loss: 0.05475\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.05204, lr: 3.125e-05\n",
      "Val Loss: 0.05475\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.05204, lr: 3.125e-05\n",
      "Val Loss: 0.05475\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.05204, lr: 3.125e-05\n",
      "Val Loss: 0.05475\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.05203, lr: 3.125e-05\n",
      "Val Loss: 0.05475\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.05203, lr: 3.125e-05\n",
      "Val Loss: 0.05475\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.05203, lr: 3.125e-05\n",
      "Val Loss: 0.05475\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.05203, lr: 3.125e-05\n",
      "Val Loss: 0.05475\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 113/1000, Train Loss: 0.05200, lr: 1.5625e-05\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 114/1000, Train Loss: 0.05200, lr: 1.5625e-05\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.05199, lr: 1.5625e-05\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.05199, lr: 1.5625e-05\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.05199, lr: 1.5625e-05\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.05199, lr: 1.5625e-05\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.05199, lr: 1.5625e-05\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 120/1000, Train Loss: 0.05197, lr: 7.8125e-06\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 121/1000, Train Loss: 0.05197, lr: 7.8125e-06\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.05197, lr: 7.8125e-06\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.05197, lr: 7.8125e-06\n",
      "Val Loss: 0.05477\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.05197, lr: 7.8125e-06\n",
      "Val Loss: 0.05477\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.05197, lr: 7.8125e-06\n",
      "Val Loss: 0.05477\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.05197, lr: 7.8125e-06\n",
      "Val Loss: 0.05477\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 127/1000, Train Loss: 0.05196, lr: 3.90625e-06\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 128/1000, Train Loss: 0.05196, lr: 3.90625e-06\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.05196, lr: 3.90625e-06\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.05196, lr: 3.90625e-06\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.05196, lr: 3.90625e-06\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.05196, lr: 3.90625e-06\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.05196, lr: 3.90625e-06\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 134/1000, Train Loss: 0.05196, lr: 1.953125e-06\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 135/1000, Train Loss: 0.05196, lr: 1.953125e-06\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 136/1000, Train Loss: 0.05196, lr: 1.953125e-06\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.05195, lr: 1.953125e-06\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.05195, lr: 1.953125e-06\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.05195, lr: 1.953125e-06\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.05195, lr: 1.953125e-06\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 141/1000, Train Loss: 0.05195, lr: 9.765625e-07\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 142/1000, Train Loss: 0.05195, lr: 9.765625e-07\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.05195, lr: 9.765625e-07\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.05195, lr: 9.765625e-07\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Epoch 145/1000, Train Loss: 0.05195, lr: 9.765625e-07\n",
      "Val Loss: 0.05476\n",
      "---------\n",
      "Early stopping ....\n",
      "675.6037316322327 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 6)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 6)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 6)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 6)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 6)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 6)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 6)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 6)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 6)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 6)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 6)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 6)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 6)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 6)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 6)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 6)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 6)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 6)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 6)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 6)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 6)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 6)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 6)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 6)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 6)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 6)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 6)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 6)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 6)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 6)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 6)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 6)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 6)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 6)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 6)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 6)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 6)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 6)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 6)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 6)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 6)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 6)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 6)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 6)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 6)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 6)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 6)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 6)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 6)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 6)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 6)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 6)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 6)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 6)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 6)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 6)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 6)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 6)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 6)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 6)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 6)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 6)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 6)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 6)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 6)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 6)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 6)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 6)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 6)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 6)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 6)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 6)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 6)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 6)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 6)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 6)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 6)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 6)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 6)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 6)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 6)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 6)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 6)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 6)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 6)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 6)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 6)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 6)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 6)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 6)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 6)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 6)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 6)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 6)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 6)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 6)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 6)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 6)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 6)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 6)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 6)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 6)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 6)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 6)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 6)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 6)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 6)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 6)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 6)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 6)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 6)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 6)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 6)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 6)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 6)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 6)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 6)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 6)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 6)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 6)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 6)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 6)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 6)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 6)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 6)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 6)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 6)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 6)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 6)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 6)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 6)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 6)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 6)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 6)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 6)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 6)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 6)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 6)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 6)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 6)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 6)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 6)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 6)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 6)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 6)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 6)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 6)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 6)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 6)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 6)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 6)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 6)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 6)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 6)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 6)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 6)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 6)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 6)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 6)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 6)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 6)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 6)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 6)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 6)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 6)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 6)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 6)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 6)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 6)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 6)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 6)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 6)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 6)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 6)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 6)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 6)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 6)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 6)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 6)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 6)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 6)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 6)\n",
      "182\n",
      "y_hat: (1459, 32, 48, 6, 6), y_hat_i: (3, 32, 48, 6, 6), y_i: (3, 32, 48, 6, 6), batch.x: torch.Size([96, 48, 6, 6]), y: (1459, 32, 48, 6, 6)\n",
      "RMSE for t2m: 3.5567324928345574; MAE for t2m: 2.726264461128995;\n",
      "RMSE for sp: 4.989948993391374; MAE for sp: 3.631032653772958;\n",
      "RMSE for tcc: 0.3577679370748421; MAE for tcc: 0.2686902475828881;\n",
      "RMSE for u10: 2.2896832286756816; MAE for u10: 1.704492187419052;\n",
      "RMSE for v10: 2.2247045127969147; MAE for v10: 1.670794100848004;\n",
      "RMSE for tp: 0.3149917797659723; MAE for tp: 0.08384648815491702;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 6)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 6)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 6)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 6)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 6)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 6)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 6)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 6)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 6)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 6)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 6)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 6)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 6)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 6)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 6)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 6)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 6)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 6)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 6)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 6)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 6)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 6)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 6)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 6)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 6)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 6)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 6)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 6)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 6)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 6)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 6)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 6)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 6)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 6)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 6)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 6)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 6)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 6)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 6)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 6)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 6)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 6)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 6)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 6)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 6)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 6)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 6)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 6)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 6)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 6)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 6)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 6)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 6)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 6)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 6)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 6)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 6)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 6)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 6)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 6)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 6)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 6)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 6)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 6)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 6)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 6)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 6)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 6)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 6)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 6)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 6)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 6)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 6)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 6)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 6)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 6)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 6)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 6)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 6)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 6)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 6)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 6)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 6)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 6)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 6)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 6)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 6)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 6)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 6)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 6)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 6)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 6)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 6)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 6)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 6)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 6)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 6)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 6)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 6)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 6)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 6)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 6)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 6)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 6)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 6)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 6)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 6)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 6)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 6)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 6)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 6)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 6)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 6)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 6)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 6)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 6)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 6)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 6)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 6)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 6)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 6)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 6)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 6)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 6)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 6)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 6)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 6)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 6)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 6)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 6)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 6)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 6)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 6)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 6)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 6)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 6)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 6)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 6)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 6)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 6)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 6)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 6)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 6)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 6)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 6)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 6)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 6)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 6)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 6)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 6)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 6)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 6)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 6)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 6)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 6)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 6)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 6)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 6)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 6)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 6)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 6)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 6)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 6)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 6)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 6)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 6)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 6)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 6)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 6)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 6)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 6)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 6)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 6)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 6)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 6)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 6)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 6)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 6)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 6)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 6)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 6)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 6), y_hat_i: (8, 32, 48, 6, 6), y_i: (8, 32, 48, 6, 6), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 6)\n",
      "182\n",
      "y_hat: (1459, 32, 48, 6, 6), y_hat_i: (3, 32, 48, 6, 6), y_i: (3, 32, 48, 6, 6), batch.x: torch.Size([96, 48, 6, 6]), y: (1459, 32, 48, 6, 6)\n",
      "RMSE for t2m: 3.5567324928345574; MAE for t2m: 2.726264461128995;\n",
      "RMSE for sp: 4.989948993391374; MAE for sp: 3.631032653772958;\n",
      "RMSE for tcc: 0.3572494383139815; MAE for tcc: 0.2676635445544606;\n",
      "RMSE for u10: 2.2896832286756816; MAE for u10: 1.704492187419052;\n",
      "RMSE for v10: 2.2247045127969147; MAE for v10: 1.670794100848004;\n",
      "RMSE for tp: 0.3149917797659723; MAE for tp: 0.08384648815491702;\n",
      "Epoch 1/1000, Train Loss: 0.08117, lr: 0.001-----------------------| 54.5% Complete\n",
      "Val Loss: 0.07007\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.06781, lr: 0.001\n",
      "Val Loss: 0.06631\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.06661, lr: 0.001\n",
      "Val Loss: 0.06626\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.06619, lr: 0.001\n",
      "Val Loss: 0.06611\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.06588, lr: 0.001\n",
      "Val Loss: 0.06577\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.06536, lr: 0.001\n",
      "Val Loss: 0.06502\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.06403, lr: 0.001\n",
      "Val Loss: 0.06396\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.06318, lr: 0.001\n",
      "Val Loss: 0.06375\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.06249, lr: 0.001\n",
      "Val Loss: 0.06252\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.06095, lr: 0.001\n",
      "Val Loss: 0.06093\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.05931, lr: 0.001\n",
      "Val Loss: 0.06044\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.05863, lr: 0.001\n",
      "Val Loss: 0.06039\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.05829, lr: 0.001\n",
      "Val Loss: 0.06008\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.05807, lr: 0.001\n",
      "Val Loss: 0.05976\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.05790, lr: 0.001\n",
      "Val Loss: 0.05948\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.05773, lr: 0.001\n",
      "Val Loss: 0.05934\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.05758, lr: 0.001\n",
      "Val Loss: 0.05932\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.05741, lr: 0.001\n",
      "Val Loss: 0.05937\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.05727, lr: 0.001\n",
      "Val Loss: 0.05914\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.05714, lr: 0.001\n",
      "Val Loss: 0.05921\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.05700, lr: 0.001\n",
      "Val Loss: 0.05915\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.05687, lr: 0.001\n",
      "Val Loss: 0.05905\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.05672, lr: 0.001\n",
      "Val Loss: 0.05901\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.05662, lr: 0.001\n",
      "Val Loss: 0.05898\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.05649, lr: 0.001\n",
      "Val Loss: 0.05890\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.05637, lr: 0.001\n",
      "Val Loss: 0.05895\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.05628, lr: 0.001\n",
      "Val Loss: 0.05915\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.05616, lr: 0.001\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.05600, lr: 0.001\n",
      "Val Loss: 0.05938\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.05589, lr: 0.001\n",
      "Val Loss: 0.05943\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.05578, lr: 0.001\n",
      "Val Loss: 0.05929\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.05557, lr: 0.001\n",
      "Val Loss: 0.05939\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 33/1000, Train Loss: 0.05479, lr: 0.0005\n",
      "Val Loss: 0.05771\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.05460, lr: 0.0005\n",
      "Val Loss: 0.05768\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.05447, lr: 0.0005\n",
      "Val Loss: 0.05764\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.05437, lr: 0.0005\n",
      "Val Loss: 0.05761\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.05429, lr: 0.0005\n",
      "Val Loss: 0.05760\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.05421, lr: 0.0005\n",
      "Val Loss: 0.05759\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.05415, lr: 0.0005\n",
      "Val Loss: 0.05759\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.05408, lr: 0.0005\n",
      "Val Loss: 0.05757\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.05401, lr: 0.0005\n",
      "Val Loss: 0.05757\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.05395, lr: 0.0005\n",
      "Val Loss: 0.05757\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.05389, lr: 0.0005\n",
      "Val Loss: 0.05756\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.05382, lr: 0.0005\n",
      "Val Loss: 0.05751\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.05377, lr: 0.0005\n",
      "Val Loss: 0.05754\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.05371, lr: 0.0005\n",
      "Val Loss: 0.05754\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.05365, lr: 0.0005\n",
      "Val Loss: 0.05755\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.05361, lr: 0.0005\n",
      "Val Loss: 0.05755\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.05356, lr: 0.0005\n",
      "Val Loss: 0.05755\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.05351, lr: 0.0005\n",
      "Val Loss: 0.05758\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.05348, lr: 0.0005\n",
      "Val Loss: 0.05760\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 52/1000, Train Loss: 0.05312, lr: 0.00025\n",
      "Val Loss: 0.05707\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.05303, lr: 0.00025\n",
      "Val Loss: 0.05707\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.05298, lr: 0.00025\n",
      "Val Loss: 0.05708\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.05293, lr: 0.00025\n",
      "Val Loss: 0.05709\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.05289, lr: 0.00025\n",
      "Val Loss: 0.05710\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.05286, lr: 0.00025\n",
      "Val Loss: 0.05711\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.05283, lr: 0.00025\n",
      "Val Loss: 0.05712\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.05280, lr: 0.00025\n",
      "Val Loss: 0.05712\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 60/1000, Train Loss: 0.05258, lr: 0.000125\n",
      "Val Loss: 0.05706\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.05251, lr: 0.000125\n",
      "Val Loss: 0.05707\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.05249, lr: 0.000125\n",
      "Val Loss: 0.05707\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.05246, lr: 0.000125\n",
      "Val Loss: 0.05708\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.05244, lr: 0.000125\n",
      "Val Loss: 0.05709\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.05242, lr: 0.000125\n",
      "Val Loss: 0.05709\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.05240, lr: 0.000125\n",
      "Val Loss: 0.05710\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.05239, lr: 0.000125\n",
      "Val Loss: 0.05710\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 68/1000, Train Loss: 0.05228, lr: 6.25e-05\n",
      "Val Loss: 0.05712\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.05224, lr: 6.25e-05\n",
      "Val Loss: 0.05713\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.05223, lr: 6.25e-05\n",
      "Val Loss: 0.05713\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.05222, lr: 6.25e-05\n",
      "Val Loss: 0.05713\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.05221, lr: 6.25e-05\n",
      "Val Loss: 0.05714\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.05220, lr: 6.25e-05\n",
      "Val Loss: 0.05714\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.05219, lr: 6.25e-05\n",
      "Val Loss: 0.05714\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 75/1000, Train Loss: 0.05211, lr: 3.125e-05\n",
      "Val Loss: 0.05715\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.05209, lr: 3.125e-05\n",
      "Val Loss: 0.05715\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.05208, lr: 3.125e-05\n",
      "Val Loss: 0.05716\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.05208, lr: 3.125e-05\n",
      "Val Loss: 0.05716\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.05207, lr: 3.125e-05\n",
      "Val Loss: 0.05716\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.05207, lr: 3.125e-05\n",
      "Val Loss: 0.05716\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.05206, lr: 3.125e-05\n",
      "Val Loss: 0.05717\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 82/1000, Train Loss: 0.05202, lr: 1.5625e-05\n",
      "Val Loss: 0.05717\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.05201, lr: 1.5625e-05\n",
      "Val Loss: 0.05718\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.05200, lr: 1.5625e-05\n",
      "Val Loss: 0.05718\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.05200, lr: 1.5625e-05\n",
      "Val Loss: 0.05718\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.05200, lr: 1.5625e-05\n",
      "Val Loss: 0.05718\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.05199, lr: 1.5625e-05\n",
      "Val Loss: 0.05718\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.05199, lr: 1.5625e-05\n",
      "Val Loss: 0.05718\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 89/1000, Train Loss: 0.05197, lr: 7.8125e-06\n",
      "Val Loss: 0.05719\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.05196, lr: 7.8125e-06\n",
      "Val Loss: 0.05719\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.05196, lr: 7.8125e-06\n",
      "Val Loss: 0.05719\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.05196, lr: 7.8125e-06\n",
      "Val Loss: 0.05719\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.05196, lr: 7.8125e-06\n",
      "Val Loss: 0.05719\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.05196, lr: 7.8125e-06\n",
      "Val Loss: 0.05719\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.05195, lr: 7.8125e-06\n",
      "Val Loss: 0.05719\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 96/1000, Train Loss: 0.05194, lr: 3.90625e-06\n",
      "Val Loss: 0.05719\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.05194, lr: 3.90625e-06\n",
      "Val Loss: 0.05719\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.05194, lr: 3.90625e-06\n",
      "Val Loss: 0.05719\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.05194, lr: 3.90625e-06\n",
      "Val Loss: 0.05719\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.05194, lr: 3.90625e-06\n",
      "Val Loss: 0.05719\n",
      "---------\n",
      "Early stopping ....\n",
      "461.0586636066437 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 7)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 7)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 7)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 7)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 7)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 7)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 7)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 7)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 7)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 7)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 7)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 7)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 7)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 7)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 7)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 7)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 7)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 7)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 7)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 7)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 7)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 7)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 7)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 7)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 7)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 7)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 7)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 7)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 7)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 7)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 7)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 7)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 7)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 7)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 7)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 7)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 7)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 7)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 7)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 7)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 7)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 7)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 7)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 7)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 7)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 7)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 7)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 7)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 7)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 7)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 7)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 7)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 7)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 7)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 7)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 7)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 7)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 7)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 7)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 7)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 7)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 7)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 7)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 7)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 7)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 7)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 7)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 7)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 7)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 7)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 7)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 7)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 7)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 7)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 7)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 7)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 7)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 7)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 7)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 7)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 7)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 7)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 7)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 7)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 7)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 7)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 7)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 7)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 7)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 7)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 7)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 7)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 7)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 7)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 7)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 7)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 7)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 7)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 7)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 7)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 7)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 7)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 7)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 7)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 7)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 7)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 7)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 7)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 7)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 7)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 7)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 7)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 7)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 7)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 7)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 7)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 7)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 7)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 7)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 7)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 7)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 7)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 7)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 7)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 7)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 7)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 7)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 7)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 7)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 7)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 7)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 7)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 7)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 7)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 7)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 7)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 7)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 7)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 7)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 7)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 7)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 7)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 7)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 7)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 7)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 7)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 7)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 7)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 7)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 7)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 7)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 7)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 7)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 7)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 7)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 7)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 7)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 7)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 7)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 7)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 7)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 7)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 7)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 7)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 7)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 7)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 7)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 7)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 7)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 7)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 7)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 7)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 7)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 7)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 7)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 7)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 7)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 7)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 7)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 7)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 7)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 7)\n",
      "182\n",
      "y_hat: (1458, 32, 48, 6, 7), y_hat_i: (2, 32, 48, 6, 7), y_i: (2, 32, 48, 6, 7), batch.x: torch.Size([64, 48, 6, 6]), y: (1458, 32, 48, 6, 7)\n",
      "RMSE for t2m: 3.7027272695701883; MAE for t2m: 2.8565865071450927;\n",
      "RMSE for sp: 5.463997410702822; MAE for sp: 4.005521167077773;\n",
      "RMSE for tcc: 0.37497136155399435; MAE for tcc: 0.28131516896644315;\n",
      "RMSE for u10: 2.423377094223134; MAE for u10: 1.8210364945716018;\n",
      "RMSE for v10: 2.321655794473283; MAE for v10: 1.756424103084834;\n",
      "RMSE for tp: 0.31626574240215044; MAE for tp: 0.08465338700803049;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 7)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 7)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 7)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 7)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 7)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 7)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 7)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 7)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 7)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 7)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 7)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 7)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 7)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 7)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 7)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 7)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 7)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 7)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 7)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 7)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 7)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 7)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 7)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 7)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 7)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 7)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 7)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 7)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 7)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 7)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 7)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 7)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 7)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 7)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 7)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 7)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 7)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 7)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 7)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 7)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 7)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 7)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 7)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 7)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 7)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 7)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 7)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 7)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 7)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 7)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 7)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 7)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 7)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 7)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 7)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 7)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 7)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 7)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 7)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 7)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 7)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 7)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 7)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 7)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 7)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 7)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 7)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 7)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 7)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 7)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 7)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 7)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 7)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 7)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 7)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 7)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 7)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 7)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 7)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 7)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 7)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 7)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 7)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 7)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 7)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 7)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 7)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 7)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 7)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 7)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 7)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 7)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 7)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 7)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 7)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 7)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 7)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 7)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 7)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 7)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 7)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 7)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 7)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 7)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 7)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 7)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 7)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 7)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 7)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 7)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 7)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 7)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 7)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 7)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 7)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 7)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 7)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 7)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 7)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 7)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 7)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 7)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 7)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 7)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 7)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 7)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 7)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 7)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 7)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 7)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 7)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 7)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 7)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 7)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 7)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 7)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 7)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 7)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 7)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 7)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 7)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 7)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 7)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 7)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 7)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 7)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 7)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 7)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 7)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 7)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 7)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 7)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 7)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 7)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 7)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 7)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 7)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 7)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 7)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 7)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 7)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 7)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 7)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 7)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 7)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 7)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 7)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 7)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 7)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 7)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 7)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 7)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 7)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 7)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 7)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 7)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 7)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 7)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 7)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 7)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 7)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 7), y_hat_i: (8, 32, 48, 6, 7), y_i: (8, 32, 48, 6, 7), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 7)\n",
      "182\n",
      "y_hat: (1458, 32, 48, 6, 7), y_hat_i: (2, 32, 48, 6, 7), y_i: (2, 32, 48, 6, 7), batch.x: torch.Size([64, 48, 6, 6]), y: (1458, 32, 48, 6, 7)\n",
      "RMSE for t2m: 3.7027272695701883; MAE for t2m: 2.8565865071450927;\n",
      "RMSE for sp: 5.463997410702822; MAE for sp: 4.005521167077773;\n",
      "RMSE for tcc: 0.37451633456287287; MAE for tcc: 0.2806509508540143;\n",
      "RMSE for u10: 2.423377094223134; MAE for u10: 1.8210364945716018;\n",
      "RMSE for v10: 2.321655794473283; MAE for v10: 1.756424103084834;\n",
      "RMSE for tp: 0.31626574240215044; MAE for tp: 0.08465338700803049;\n",
      "Epoch 1/1000, Train Loss: 0.08359, lr: 0.001-------------------| 63.6% Complete\n",
      "Val Loss: 0.07520\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.07032, lr: 0.001\n",
      "Val Loss: 0.06717\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.06728, lr: 0.001\n",
      "Val Loss: 0.06686\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.06670, lr: 0.001\n",
      "Val Loss: 0.06667\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.06639, lr: 0.001\n",
      "Val Loss: 0.06630\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.06593, lr: 0.001\n",
      "Val Loss: 0.06556\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.06463, lr: 0.001\n",
      "Val Loss: 0.06432\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.06348, lr: 0.001\n",
      "Val Loss: 0.06408\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.06285, lr: 0.001\n",
      "Val Loss: 0.06397\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.06236, lr: 0.001\n",
      "Val Loss: 0.06355\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.06163, lr: 0.001\n",
      "Val Loss: 0.06265\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.06077, lr: 0.001\n",
      "Val Loss: 0.06202\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.06018, lr: 0.001\n",
      "Val Loss: 0.06175\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.05981, lr: 0.001\n",
      "Val Loss: 0.06146\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.05970, lr: 0.001\n",
      "Val Loss: 0.06156\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.05936, lr: 0.001\n",
      "Val Loss: 0.06120\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.05916, lr: 0.001\n",
      "Val Loss: 0.06100\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.05902, lr: 0.001\n",
      "Val Loss: 0.06092\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.05891, lr: 0.001\n",
      "Val Loss: 0.06086\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.05881, lr: 0.001\n",
      "Val Loss: 0.06079\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.05872, lr: 0.001\n",
      "Val Loss: 0.06075\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.05865, lr: 0.001\n",
      "Val Loss: 0.06070\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.05857, lr: 0.001\n",
      "Val Loss: 0.06067\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.05851, lr: 0.001\n",
      "Val Loss: 0.06063\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.05844, lr: 0.001\n",
      "Val Loss: 0.06058\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.05838, lr: 0.001\n",
      "Val Loss: 0.06053\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.05832, lr: 0.001\n",
      "Val Loss: 0.06049\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.05827, lr: 0.001\n",
      "Val Loss: 0.06044\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.05821, lr: 0.001\n",
      "Val Loss: 0.06041\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.05816, lr: 0.001\n",
      "Val Loss: 0.06038\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.05812, lr: 0.001\n",
      "Val Loss: 0.06036\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.05807, lr: 0.001\n",
      "Val Loss: 0.06035\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.05803, lr: 0.001\n",
      "Val Loss: 0.06034\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.05799, lr: 0.001\n",
      "Val Loss: 0.06034\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.05795, lr: 0.001\n",
      "Val Loss: 0.06033\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.05792, lr: 0.001\n",
      "Val Loss: 0.06033\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.05788, lr: 0.001\n",
      "Val Loss: 0.06033\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.05785, lr: 0.001\n",
      "Val Loss: 0.06033\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.05782, lr: 0.001\n",
      "Val Loss: 0.06032\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.05779, lr: 0.001\n",
      "Val Loss: 0.06030\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.05775, lr: 0.001\n",
      "Val Loss: 0.06029\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.05772, lr: 0.001\n",
      "Val Loss: 0.06028\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.05769, lr: 0.001\n",
      "Val Loss: 0.06027\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.05767, lr: 0.001\n",
      "Val Loss: 0.06026\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.05764, lr: 0.001\n",
      "Val Loss: 0.06025\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.05761, lr: 0.001\n",
      "Val Loss: 0.06025\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.05759, lr: 0.001\n",
      "Val Loss: 0.06024\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.05756, lr: 0.001\n",
      "Val Loss: 0.06023\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.05753, lr: 0.001\n",
      "Val Loss: 0.06024\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.05751, lr: 0.001\n",
      "Val Loss: 0.06024\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.05748, lr: 0.001\n",
      "Val Loss: 0.06024\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.05746, lr: 0.001\n",
      "Val Loss: 0.06025\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.05743, lr: 0.001\n",
      "Val Loss: 0.06026\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.05741, lr: 0.001\n",
      "Val Loss: 0.06026\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.05738, lr: 0.001\n",
      "Val Loss: 0.06027\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 56/1000, Train Loss: 0.05690, lr: 0.0005\n",
      "Val Loss: 0.05972\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.05686, lr: 0.0005\n",
      "Val Loss: 0.05971\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.05683, lr: 0.0005\n",
      "Val Loss: 0.05971\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.05680, lr: 0.0005\n",
      "Val Loss: 0.05970\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.05678, lr: 0.0005\n",
      "Val Loss: 0.05970\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.05675, lr: 0.0005\n",
      "Val Loss: 0.05969\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.05672, lr: 0.0005\n",
      "Val Loss: 0.05968\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.05669, lr: 0.0005\n",
      "Val Loss: 0.05967\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.05666, lr: 0.0005\n",
      "Val Loss: 0.05966\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.05663, lr: 0.0005\n",
      "Val Loss: 0.05964\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.05659, lr: 0.0005\n",
      "Val Loss: 0.05962\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.05655, lr: 0.0005\n",
      "Val Loss: 0.05959\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.05650, lr: 0.0005\n",
      "Val Loss: 0.05955\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.05646, lr: 0.0005\n",
      "Val Loss: 0.05951\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.05640, lr: 0.0005\n",
      "Val Loss: 0.05946\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.05635, lr: 0.0005\n",
      "Val Loss: 0.05941\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.05630, lr: 0.0005\n",
      "Val Loss: 0.05935\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.05624, lr: 0.0005\n",
      "Val Loss: 0.05930\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.05619, lr: 0.0005\n",
      "Val Loss: 0.05924\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.05615, lr: 0.0005\n",
      "Val Loss: 0.05920\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.05610, lr: 0.0005\n",
      "Val Loss: 0.05915\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.05606, lr: 0.0005\n",
      "Val Loss: 0.05911\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.05603, lr: 0.0005\n",
      "Val Loss: 0.05907\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.05599, lr: 0.0005\n",
      "Val Loss: 0.05903\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.05596, lr: 0.0005\n",
      "Val Loss: 0.05900\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.05593, lr: 0.0005\n",
      "Val Loss: 0.05898\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.05591, lr: 0.0005\n",
      "Val Loss: 0.05896\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.05588, lr: 0.0005\n",
      "Val Loss: 0.05894\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.05586, lr: 0.0005\n",
      "Val Loss: 0.05893\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.05584, lr: 0.0005\n",
      "Val Loss: 0.05892\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.05582, lr: 0.0005\n",
      "Val Loss: 0.05891\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.05580, lr: 0.0005\n",
      "Val Loss: 0.05890\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.05579, lr: 0.0005\n",
      "Val Loss: 0.05889\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.05577, lr: 0.0005\n",
      "Val Loss: 0.05889\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.05575, lr: 0.0005\n",
      "Val Loss: 0.05888\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.05574, lr: 0.0005\n",
      "Val Loss: 0.05888\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.05572, lr: 0.0005\n",
      "Val Loss: 0.05888\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.05570, lr: 0.0005\n",
      "Val Loss: 0.05888\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.05569, lr: 0.0005\n",
      "Val Loss: 0.05888\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.05567, lr: 0.0005\n",
      "Val Loss: 0.05888\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.05566, lr: 0.0005\n",
      "Val Loss: 0.05888\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.05564, lr: 0.0005\n",
      "Val Loss: 0.05889\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.05562, lr: 0.0005\n",
      "Val Loss: 0.05889\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.05560, lr: 0.0005\n",
      "Val Loss: 0.05890\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.05559, lr: 0.0005\n",
      "Val Loss: 0.05890\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.05557, lr: 0.0005\n",
      "Val Loss: 0.05891\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 102/1000, Train Loss: 0.05526, lr: 0.00025\n",
      "Val Loss: 0.05833\n",
      "---------\n",
      "Epoch 103/1000, Train Loss: 0.05521, lr: 0.00025\n",
      "Val Loss: 0.05834\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.05520, lr: 0.00025\n",
      "Val Loss: 0.05834\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.05519, lr: 0.00025\n",
      "Val Loss: 0.05835\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.05518, lr: 0.00025\n",
      "Val Loss: 0.05835\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.05517, lr: 0.00025\n",
      "Val Loss: 0.05835\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.05516, lr: 0.00025\n",
      "Val Loss: 0.05836\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.05515, lr: 0.00025\n",
      "Val Loss: 0.05836\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 110/1000, Train Loss: 0.05499, lr: 0.000125\n",
      "Val Loss: 0.05814\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.05496, lr: 0.000125\n",
      "Val Loss: 0.05814\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.05495, lr: 0.000125\n",
      "Val Loss: 0.05814\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.05495, lr: 0.000125\n",
      "Val Loss: 0.05815\n",
      "---------\n",
      "Epoch 114/1000, Train Loss: 0.05494, lr: 0.000125\n",
      "Val Loss: 0.05815\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.05494, lr: 0.000125\n",
      "Val Loss: 0.05815\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.05493, lr: 0.000125\n",
      "Val Loss: 0.05815\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.05493, lr: 0.000125\n",
      "Val Loss: 0.05815\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 118/1000, Train Loss: 0.05485, lr: 6.25e-05\n",
      "Val Loss: 0.05806\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.05483, lr: 6.25e-05\n",
      "Val Loss: 0.05806\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.05482, lr: 6.25e-05\n",
      "Val Loss: 0.05806\n",
      "---------\n",
      "Epoch 121/1000, Train Loss: 0.05482, lr: 6.25e-05\n",
      "Val Loss: 0.05806\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.05482, lr: 6.25e-05\n",
      "Val Loss: 0.05806\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.05481, lr: 6.25e-05\n",
      "Val Loss: 0.05806\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.05481, lr: 6.25e-05\n",
      "Val Loss: 0.05806\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.05481, lr: 6.25e-05\n",
      "Val Loss: 0.05806\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.05481, lr: 6.25e-05\n",
      "Val Loss: 0.05806\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 127/1000, Train Loss: 0.05476, lr: 3.125e-05\n",
      "Val Loss: 0.05806\n",
      "---------\n",
      "Epoch 128/1000, Train Loss: 0.05475, lr: 3.125e-05\n",
      "Val Loss: 0.05806\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.05475, lr: 3.125e-05\n",
      "Val Loss: 0.05806\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.05475, lr: 3.125e-05\n",
      "Val Loss: 0.05806\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.05474, lr: 3.125e-05\n",
      "Val Loss: 0.05806\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.05474, lr: 3.125e-05\n",
      "Val Loss: 0.05806\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.05474, lr: 3.125e-05\n",
      "Val Loss: 0.05806\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 134/1000, Train Loss: 0.05471, lr: 1.5625e-05\n",
      "Val Loss: 0.05809\n",
      "---------\n",
      "Epoch 135/1000, Train Loss: 0.05471, lr: 1.5625e-05\n",
      "Val Loss: 0.05809\n",
      "---------\n",
      "Epoch 136/1000, Train Loss: 0.05471, lr: 1.5625e-05\n",
      "Val Loss: 0.05809\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.05471, lr: 1.5625e-05\n",
      "Val Loss: 0.05809\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.05471, lr: 1.5625e-05\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.05470, lr: 1.5625e-05\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.05470, lr: 1.5625e-05\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 141/1000, Train Loss: 0.05469, lr: 7.8125e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Epoch 142/1000, Train Loss: 0.05469, lr: 7.8125e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.05469, lr: 7.8125e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.05469, lr: 7.8125e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Epoch 145/1000, Train Loss: 0.05468, lr: 7.8125e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Epoch 146/1000, Train Loss: 0.05468, lr: 7.8125e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Epoch 147/1000, Train Loss: 0.05468, lr: 7.8125e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 148/1000, Train Loss: 0.05468, lr: 3.90625e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Epoch 149/1000, Train Loss: 0.05467, lr: 3.90625e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Epoch 150/1000, Train Loss: 0.05467, lr: 3.90625e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Epoch 151/1000, Train Loss: 0.05467, lr: 3.90625e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Epoch 152/1000, Train Loss: 0.05467, lr: 3.90625e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Epoch 153/1000, Train Loss: 0.05467, lr: 3.90625e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Epoch 154/1000, Train Loss: 0.05467, lr: 3.90625e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 155/1000, Train Loss: 0.05467, lr: 1.953125e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Epoch 156/1000, Train Loss: 0.05467, lr: 1.953125e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Epoch 157/1000, Train Loss: 0.05467, lr: 1.953125e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Epoch 158/1000, Train Loss: 0.05467, lr: 1.953125e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Epoch 159/1000, Train Loss: 0.05467, lr: 1.953125e-06\n",
      "Val Loss: 0.05810\n",
      "---------\n",
      "Early stopping ....\n",
      "743.4379546642303 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 8)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 8)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 8)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 8)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 8)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 8)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 8)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 8)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 8)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 8)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 8)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 8)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 8)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 8)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 8)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 8)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 8)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 8)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 8)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 8)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 8)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 8)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 8)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 8)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 8)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 8)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 8)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 8)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 8)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 8)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 8)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 8)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 8)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 8)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 8)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 8)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 8)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 8)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 8)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 8)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 8)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 8)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 8)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 8)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 8)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 8)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 8)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 8)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 8)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 8)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 8)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 8)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 8)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 8)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 8)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 8)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 8)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 8)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 8)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 8)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 8)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 8)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 8)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 8)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 8)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 8)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 8)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 8)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 8)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 8)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 8)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 8)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 8)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 8)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 8)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 8)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 8)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 8)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 8)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 8)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 8)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 8)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 8)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 8)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 8)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 8)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 8)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 8)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 8)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 8)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 8)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 8)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 8)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 8)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 8)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 8)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 8)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 8)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 8)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 8)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 8)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 8)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 8)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 8)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 8)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 8)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 8)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 8)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 8)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 8)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 8)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 8)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 8)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 8)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 8)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 8)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 8)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 8)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 8)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 8)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 8)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 8)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 8)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 8)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 8)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 8)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 8)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 8)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 8)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 8)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 8)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 8)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 8)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 8)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 8)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 8)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 8)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 8)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 8)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 8)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 8)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 8)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 8)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 8)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 8)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 8)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 8)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 8)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 8)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 8)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 8)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 8)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 8)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 8)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 8)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 8)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 8)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 8)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 8)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 8)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 8)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 8)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 8)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 8)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 8)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 8)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 8)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 8)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 8)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 8)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 8)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 8)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 8)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 8)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 8)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 8)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 8)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 8)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 8)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 8)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 8)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 8)\n",
      "182\n",
      "y_hat: (1457, 32, 48, 6, 8), y_hat_i: (1, 32, 48, 6, 8), y_i: (1, 32, 48, 6, 8), batch.x: torch.Size([32, 48, 6, 6]), y: (1457, 32, 48, 6, 8)\n",
      "RMSE for t2m: 3.8366799657844055; MAE for t2m: 2.9498154387910875;\n",
      "RMSE for sp: 5.9006130997569; MAE for sp: 4.306807086331052;\n",
      "RMSE for tcc: 0.3649206265773929; MAE for tcc: 0.2766098842112777;\n",
      "RMSE for u10: 2.438896104779407; MAE for u10: 1.8284590221497123;\n",
      "RMSE for v10: 2.369812401438914; MAE for v10: 1.8079842436906435;\n",
      "RMSE for tp: 0.317918295093666; MAE for tp: 0.0847754479558057;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 8)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 8)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 8)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 8)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 8)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 8)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 8)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 8)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 8)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 8)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 8)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 8)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 8)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 8)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 8)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 8)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 8)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 8)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 8)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 8)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 8)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 8)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 8)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 8)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 8)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 8)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 8)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 8)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 8)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 8)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 8)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 8)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 8)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 8)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 8)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 8)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 8)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 8)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 8)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 8)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 8)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 8)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 8)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 8)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 8)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 8)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 8)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 8)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 8)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 8)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 8)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 8)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 8)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 8)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 8)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 8)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 8)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 8)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 8)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 8)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 8)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 8)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 8)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 8)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 8)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 8)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 8)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 8)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 8)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 8)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 8)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 8)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 8)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 8)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 8)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 8)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 8)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 8)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 8)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 8)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 8)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 8)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 8)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 8)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 8)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 8)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 8)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 8)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 8)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 8)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 8)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 8)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 8)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 8)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 8)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 8)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 8)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 8)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 8)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 8)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 8)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 8)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 8)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 8)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 8)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 8)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 8)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 8)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 8)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 8)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 8)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 8)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 8)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 8)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 8)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 8)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 8)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 8)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 8)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 8)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 8)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 8)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 8)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 8)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 8)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 8)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 8)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 8)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 8)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 8)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 8)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 8)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 8)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 8)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 8)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 8)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 8)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 8)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 8)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 8)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 8)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 8)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 8)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 8)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 8)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 8)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 8)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 8)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 8)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 8)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 8)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 8)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 8)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 8)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 8)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 8)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 8)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 8)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 8)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 8)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 8)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 8)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 8)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 8)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 8)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 8)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 8)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 8)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 8)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 8)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 8)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 8)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 8)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 8)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 8)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 8)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 8)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 8)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 8)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 8)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 8)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 8), y_hat_i: (8, 32, 48, 6, 8), y_i: (8, 32, 48, 6, 8), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 8)\n",
      "182\n",
      "y_hat: (1457, 32, 48, 6, 8), y_hat_i: (1, 32, 48, 6, 8), y_i: (1, 32, 48, 6, 8), batch.x: torch.Size([32, 48, 6, 6]), y: (1457, 32, 48, 6, 8)\n",
      "RMSE for t2m: 3.8366799657844055; MAE for t2m: 2.9498154387910875;\n",
      "RMSE for sp: 5.9006130997569; MAE for sp: 4.306807086331052;\n",
      "RMSE for tcc: 0.36466033620668786; MAE for tcc: 0.27609285666563377;\n",
      "RMSE for u10: 2.438896104779407; MAE for u10: 1.8284590221497123;\n",
      "RMSE for v10: 2.369812401438914; MAE for v10: 1.8079842436906435;\n",
      "RMSE for tp: 0.317918295093666; MAE for tp: 0.0847754479558057;\n",
      "Epoch 1/1000, Train Loss: 0.07934, lr: 0.001--------------| 72.7% Complete\n",
      "Val Loss: 0.07002\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.06787, lr: 0.001\n",
      "Val Loss: 0.06686\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.06671, lr: 0.001\n",
      "Val Loss: 0.06642\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.06637, lr: 0.001\n",
      "Val Loss: 0.06615\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.06617, lr: 0.001\n",
      "Val Loss: 0.06600\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.06599, lr: 0.001\n",
      "Val Loss: 0.06592\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.06584, lr: 0.001\n",
      "Val Loss: 0.06585\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.06563, lr: 0.001\n",
      "Val Loss: 0.06569\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.06511, lr: 0.001\n",
      "Val Loss: 0.06515\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.06419, lr: 0.001\n",
      "Val Loss: 0.06489\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.06325, lr: 0.001\n",
      "Val Loss: 0.06439\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.06265, lr: 0.001\n",
      "Val Loss: 0.06390\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.06232, lr: 0.001\n",
      "Val Loss: 0.06367\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.06207, lr: 0.001\n",
      "Val Loss: 0.06360\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.06191, lr: 0.001\n",
      "Val Loss: 0.06356\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.06179, lr: 0.001\n",
      "Val Loss: 0.06354\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.06166, lr: 0.001\n",
      "Val Loss: 0.06351\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.06157, lr: 0.001\n",
      "Val Loss: 0.06351\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.06149, lr: 0.001\n",
      "Val Loss: 0.06353\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.06141, lr: 0.001\n",
      "Val Loss: 0.06355\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.06134, lr: 0.001\n",
      "Val Loss: 0.06353\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.06128, lr: 0.001\n",
      "Val Loss: 0.06354\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.06122, lr: 0.001\n",
      "Val Loss: 0.06354\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.06117, lr: 0.001\n",
      "Val Loss: 0.06352\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 25/1000, Train Loss: 0.06071, lr: 0.0005\n",
      "Val Loss: 0.06299\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.06061, lr: 0.0005\n",
      "Val Loss: 0.06298\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.06056, lr: 0.0005\n",
      "Val Loss: 0.06298\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.06051, lr: 0.0005\n",
      "Val Loss: 0.06298\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.06046, lr: 0.0005\n",
      "Val Loss: 0.06297\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.06041, lr: 0.0005\n",
      "Val Loss: 0.06296\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.06036, lr: 0.0005\n",
      "Val Loss: 0.06294\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.06030, lr: 0.0005\n",
      "Val Loss: 0.06292\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.06023, lr: 0.0005\n",
      "Val Loss: 0.06289\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.06015, lr: 0.0005\n",
      "Val Loss: 0.06284\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.06006, lr: 0.0005\n",
      "Val Loss: 0.06278\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.05994, lr: 0.0005\n",
      "Val Loss: 0.06268\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.05980, lr: 0.0005\n",
      "Val Loss: 0.06253\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.05961, lr: 0.0005\n",
      "Val Loss: 0.06233\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.05937, lr: 0.0005\n",
      "Val Loss: 0.06206\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.05907, lr: 0.0005\n",
      "Val Loss: 0.06175\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.05874, lr: 0.0005\n",
      "Val Loss: 0.06142\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.05842, lr: 0.0005\n",
      "Val Loss: 0.06115\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.05817, lr: 0.0005\n",
      "Val Loss: 0.06092\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.05796, lr: 0.0005\n",
      "Val Loss: 0.06072\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.05776, lr: 0.0005\n",
      "Val Loss: 0.06059\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.05758, lr: 0.0005\n",
      "Val Loss: 0.06045\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.05741, lr: 0.0005\n",
      "Val Loss: 0.06032\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.05724, lr: 0.0005\n",
      "Val Loss: 0.06019\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.05707, lr: 0.0005\n",
      "Val Loss: 0.06008\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.05692, lr: 0.0005\n",
      "Val Loss: 0.05997\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.05677, lr: 0.0005\n",
      "Val Loss: 0.05985\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.05664, lr: 0.0005\n",
      "Val Loss: 0.05976\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.05651, lr: 0.0005\n",
      "Val Loss: 0.05968\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.05639, lr: 0.0005\n",
      "Val Loss: 0.05962\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.05628, lr: 0.0005\n",
      "Val Loss: 0.05956\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.05617, lr: 0.0005\n",
      "Val Loss: 0.05955\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.05609, lr: 0.0005\n",
      "Val Loss: 0.05954\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.05601, lr: 0.0005\n",
      "Val Loss: 0.05952\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.05594, lr: 0.0005\n",
      "Val Loss: 0.05951\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.05588, lr: 0.0005\n",
      "Val Loss: 0.05950\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.05582, lr: 0.0005\n",
      "Val Loss: 0.05949\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.05576, lr: 0.0005\n",
      "Val Loss: 0.05951\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.05571, lr: 0.0005\n",
      "Val Loss: 0.05949\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.05566, lr: 0.0005\n",
      "Val Loss: 0.05947\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.05562, lr: 0.0005\n",
      "Val Loss: 0.05946\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.05557, lr: 0.0005\n",
      "Val Loss: 0.05948\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.05554, lr: 0.0005\n",
      "Val Loss: 0.05946\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.05549, lr: 0.0005\n",
      "Val Loss: 0.05946\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.05545, lr: 0.0005\n",
      "Val Loss: 0.05946\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.05541, lr: 0.0005\n",
      "Val Loss: 0.05947\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.05538, lr: 0.0005\n",
      "Val Loss: 0.05949\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.05535, lr: 0.0005\n",
      "Val Loss: 0.05947\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.05531, lr: 0.0005\n",
      "Val Loss: 0.05948\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.05528, lr: 0.0005\n",
      "Val Loss: 0.05946\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.05525, lr: 0.0005\n",
      "Val Loss: 0.05948\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.05522, lr: 0.0005\n",
      "Val Loss: 0.05950\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 77/1000, Train Loss: 0.05487, lr: 0.00025\n",
      "Val Loss: 0.05925\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.05479, lr: 0.00025\n",
      "Val Loss: 0.05926\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.05475, lr: 0.00025\n",
      "Val Loss: 0.05926\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.05472, lr: 0.00025\n",
      "Val Loss: 0.05926\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.05470, lr: 0.00025\n",
      "Val Loss: 0.05927\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.05467, lr: 0.00025\n",
      "Val Loss: 0.05927\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.05465, lr: 0.00025\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.05463, lr: 0.00025\n",
      "Val Loss: 0.05929\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 85/1000, Train Loss: 0.05444, lr: 0.000125\n",
      "Val Loss: 0.05924\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.05439, lr: 0.000125\n",
      "Val Loss: 0.05925\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.05437, lr: 0.000125\n",
      "Val Loss: 0.05926\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.05435, lr: 0.000125\n",
      "Val Loss: 0.05926\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.05434, lr: 0.000125\n",
      "Val Loss: 0.05927\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.05432, lr: 0.000125\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.05431, lr: 0.000125\n",
      "Val Loss: 0.05929\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.05429, lr: 0.000125\n",
      "Val Loss: 0.05929\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 93/1000, Train Loss: 0.05418, lr: 6.25e-05\n",
      "Val Loss: 0.05926\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.05416, lr: 6.25e-05\n",
      "Val Loss: 0.05927\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.05415, lr: 6.25e-05\n",
      "Val Loss: 0.05927\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.05414, lr: 6.25e-05\n",
      "Val Loss: 0.05927\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.05413, lr: 6.25e-05\n",
      "Val Loss: 0.05927\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.05412, lr: 6.25e-05\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.05411, lr: 6.25e-05\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 100/1000, Train Loss: 0.05405, lr: 3.125e-05\n",
      "Val Loss: 0.05924\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.05403, lr: 3.125e-05\n",
      "Val Loss: 0.05925\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.05403, lr: 3.125e-05\n",
      "Val Loss: 0.05925\n",
      "---------\n",
      "Epoch 103/1000, Train Loss: 0.05402, lr: 3.125e-05\n",
      "Val Loss: 0.05925\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.05402, lr: 3.125e-05\n",
      "Val Loss: 0.05925\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.05401, lr: 3.125e-05\n",
      "Val Loss: 0.05925\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.05401, lr: 3.125e-05\n",
      "Val Loss: 0.05925\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.05400, lr: 3.125e-05\n",
      "Val Loss: 0.05926\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 108/1000, Train Loss: 0.05397, lr: 1.5625e-05\n",
      "Val Loss: 0.05927\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.05396, lr: 1.5625e-05\n",
      "Val Loss: 0.05927\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.05396, lr: 1.5625e-05\n",
      "Val Loss: 0.05927\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.05396, lr: 1.5625e-05\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.05395, lr: 1.5625e-05\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.05395, lr: 1.5625e-05\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 114/1000, Train Loss: 0.05395, lr: 1.5625e-05\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 115/1000, Train Loss: 0.05393, lr: 7.8125e-06\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.05392, lr: 7.8125e-06\n",
      "Val Loss: 0.05929\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.05392, lr: 7.8125e-06\n",
      "Val Loss: 0.05929\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.05392, lr: 7.8125e-06\n",
      "Val Loss: 0.05929\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.05392, lr: 7.8125e-06\n",
      "Val Loss: 0.05929\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.05392, lr: 7.8125e-06\n",
      "Val Loss: 0.05929\n",
      "---------\n",
      "Epoch 121/1000, Train Loss: 0.05392, lr: 7.8125e-06\n",
      "Val Loss: 0.05929\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 122/1000, Train Loss: 0.05391, lr: 3.90625e-06\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.05390, lr: 3.90625e-06\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.05390, lr: 3.90625e-06\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.05390, lr: 3.90625e-06\n",
      "Val Loss: 0.05929\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.05390, lr: 3.90625e-06\n",
      "Val Loss: 0.05929\n",
      "---------\n",
      "Epoch 127/1000, Train Loss: 0.05390, lr: 3.90625e-06\n",
      "Val Loss: 0.05929\n",
      "---------\n",
      "Epoch 128/1000, Train Loss: 0.05390, lr: 3.90625e-06\n",
      "Val Loss: 0.05929\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 129/1000, Train Loss: 0.05389, lr: 1.953125e-06\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.05389, lr: 1.953125e-06\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.05389, lr: 1.953125e-06\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.05389, lr: 1.953125e-06\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.05389, lr: 1.953125e-06\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 134/1000, Train Loss: 0.05389, lr: 1.953125e-06\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 135/1000, Train Loss: 0.05389, lr: 1.953125e-06\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 136/1000, Train Loss: 0.05389, lr: 9.765625e-07\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.05389, lr: 9.765625e-07\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.05389, lr: 9.765625e-07\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.05389, lr: 9.765625e-07\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.05389, lr: 9.765625e-07\n",
      "Val Loss: 0.05928\n",
      "---------\n",
      "Early stopping ....\n",
      "654.4529333114624 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 9)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 9)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 9)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 9)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 9)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 9)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 9)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 9)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 9)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 9)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 9)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 9)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 9)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 9)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 9)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 9)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 9)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 9)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 9)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 9)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 9)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 9)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 9)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 9)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 9)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 9)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 9)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 9)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 9)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 9)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 9)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 9)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 9)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 9)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 9)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 9)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 9)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 9)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 9)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 9)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 9)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 9)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 9)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 9)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 9)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 9)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 9)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 9)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 9)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 9)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 9)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 9)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 9)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 9)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 9)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 9)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 9)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 9)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 9)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 9)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 9)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 9)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 9)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 9)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 9)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 9)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 9)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 9)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 9)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 9)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 9)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 9)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 9)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 9)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 9)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 9)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 9)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 9)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 9)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 9)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 9)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 9)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 9)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 9)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 9)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 9)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 9)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 9)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 9)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 9)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 9)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 9)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 9)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 9)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 9)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 9)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 9)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 9)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 9)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 9)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 9)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 9)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 9)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 9)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 9)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 9)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 9)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 9)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 9)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 9)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 9)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 9)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 9)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 9)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 9)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 9)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 9)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 9)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 9)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 9)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 9)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 9)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 9)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 9)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 9)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 9)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 9)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 9)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 9)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 9)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 9)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 9)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 9)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 9)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 9)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 9)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 9)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 9)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 9)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 9)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 9)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 9)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 9)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 9)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 9)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 9)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 9)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 9)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 9)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 9)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 9)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 9)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 9)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 9)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 9)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 9)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 9)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 9)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 9)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 9)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 9)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 9)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 9)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 9)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 9)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 9)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 9)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 9)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 9)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 9)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 9)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 9)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 9)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 9)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 9)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 9)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 9)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 9)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 9)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 9)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 9)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 9)\n",
      "182\n",
      "y_hat: (1458, 32, 48, 6, 9), y_hat_i: (2, 32, 48, 6, 9), y_i: (2, 32, 48, 6, 9), batch.x: torch.Size([64, 48, 6, 6]), y: (1458, 32, 48, 6, 9)\n",
      "RMSE for t2m: 3.884526206188421; MAE for t2m: 2.991333284127596;\n",
      "RMSE for sp: 6.398088750037177; MAE for sp: 4.725470708297313;\n",
      "RMSE for tcc: 0.3714265241321002; MAE for tcc: 0.2799677320322548;\n",
      "RMSE for u10: 2.5182257014134364; MAE for u10: 1.8952987758070536;\n",
      "RMSE for v10: 2.4516169835250894; MAE for v10: 1.8680742544729187;\n",
      "RMSE for tp: 0.3184655246296165; MAE for tp: 0.08504895451012048;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 9)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 9)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 9)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 9)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 9)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 9)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 9)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 9)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 9)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 9)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 9)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 9)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 9)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 9)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 9)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 9)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 9)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 9)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 9)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 9)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 9)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 9)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 9)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 9)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 9)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 9)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 9)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 9)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 9)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 9)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 9)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 9)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 9)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 9)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 9)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 9)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 9)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 9)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 9)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 9)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 9)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 9)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 9)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 9)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 9)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 9)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 9)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 9)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 9)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 9)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 9)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 9)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 9)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 9)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 9)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 9)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 9)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 9)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 9)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 9)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 9)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 9)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 9)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 9)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 9)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 9)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 9)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 9)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 9)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 9)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 9)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 9)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 9)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 9)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 9)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 9)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 9)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 9)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 9)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 9)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 9)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 9)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 9)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 9)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 9)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 9)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 9)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 9)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 9)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 9)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 9)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 9)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 9)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 9)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 9)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 9)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 9)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 9)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 9)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 9)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 9)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 9)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 9)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 9)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 9)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 9)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 9)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 9)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 9)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 9)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 9)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 9)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 9)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 9)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 9)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 9)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 9)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 9)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 9)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 9)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 9)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 9)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 9)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 9)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 9)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 9)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 9)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 9)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 9)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 9)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 9)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 9)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 9)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 9)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 9)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 9)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 9)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 9)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 9)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 9)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 9)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 9)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 9)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 9)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 9)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 9)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 9)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 9)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 9)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 9)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 9)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 9)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 9)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 9)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 9)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 9)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 9)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 9)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 9)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 9)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 9)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 9)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 9)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 9)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 9)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 9)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 9)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 9)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 9)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 9)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 9)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 9)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 9)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 9)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 9)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 9)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 9)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 9)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 9)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 9)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 9)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 9), y_hat_i: (8, 32, 48, 6, 9), y_i: (8, 32, 48, 6, 9), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 9)\n",
      "182\n",
      "y_hat: (1458, 32, 48, 6, 9), y_hat_i: (2, 32, 48, 6, 9), y_i: (2, 32, 48, 6, 9), batch.x: torch.Size([64, 48, 6, 6]), y: (1458, 32, 48, 6, 9)\n",
      "RMSE for t2m: 3.884526206188421; MAE for t2m: 2.991333284127596;\n",
      "RMSE for sp: 6.398088750037177; MAE for sp: 4.725470708297313;\n",
      "RMSE for tcc: 0.3706111479102365; MAE for tcc: 0.2789282236422423;\n",
      "RMSE for u10: 2.5182257014134364; MAE for u10: 1.8952987758070536;\n",
      "RMSE for v10: 2.4516169835250894; MAE for v10: 1.8680742544729187;\n",
      "RMSE for tp: 0.3184655246296165; MAE for tp: 0.08504895451012048;\n",
      "Epoch 1/1000, Train Loss: 0.07975, lr: 0.001----------| 81.8% Complete\n",
      "Val Loss: 0.06995\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.06794, lr: 0.001\n",
      "Val Loss: 0.06687\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.06685, lr: 0.001\n",
      "Val Loss: 0.06645\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.06605, lr: 0.001\n",
      "Val Loss: 0.06588\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.06468, lr: 0.001\n",
      "Val Loss: 0.06512\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.06379, lr: 0.001\n",
      "Val Loss: 0.06469\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.06335, lr: 0.001\n",
      "Val Loss: 0.06450\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.06309, lr: 0.001\n",
      "Val Loss: 0.06438\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.06293, lr: 0.001\n",
      "Val Loss: 0.06430\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.06280, lr: 0.001\n",
      "Val Loss: 0.06422\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.06269, lr: 0.001\n",
      "Val Loss: 0.06415\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.06259, lr: 0.001\n",
      "Val Loss: 0.06409\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.06250, lr: 0.001\n",
      "Val Loss: 0.06402\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.06242, lr: 0.001\n",
      "Val Loss: 0.06397\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.06234, lr: 0.001\n",
      "Val Loss: 0.06392\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.06226, lr: 0.001\n",
      "Val Loss: 0.06388\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.06219, lr: 0.001\n",
      "Val Loss: 0.06385\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.06213, lr: 0.001\n",
      "Val Loss: 0.06381\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.06205, lr: 0.001\n",
      "Val Loss: 0.06376\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.06198, lr: 0.001\n",
      "Val Loss: 0.06368\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.06187, lr: 0.001\n",
      "Val Loss: 0.06356\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.06171, lr: 0.001\n",
      "Val Loss: 0.06336\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.06142, lr: 0.001\n",
      "Val Loss: 0.06305\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.06105, lr: 0.001\n",
      "Val Loss: 0.06269\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.06066, lr: 0.001\n",
      "Val Loss: 0.06235\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.06034, lr: 0.001\n",
      "Val Loss: 0.06219\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.06013, lr: 0.001\n",
      "Val Loss: 0.06211\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.05998, lr: 0.001\n",
      "Val Loss: 0.06205\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.05986, lr: 0.001\n",
      "Val Loss: 0.06198\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.05976, lr: 0.001\n",
      "Val Loss: 0.06191\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.05967, lr: 0.001\n",
      "Val Loss: 0.06187\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.05959, lr: 0.001\n",
      "Val Loss: 0.06183\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.05952, lr: 0.001\n",
      "Val Loss: 0.06179\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.05944, lr: 0.001\n",
      "Val Loss: 0.06176\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.05936, lr: 0.001\n",
      "Val Loss: 0.06173\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.05928, lr: 0.001\n",
      "Val Loss: 0.06169\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.05919, lr: 0.001\n",
      "Val Loss: 0.06163\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.05910, lr: 0.001\n",
      "Val Loss: 0.06158\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.05901, lr: 0.001\n",
      "Val Loss: 0.06152\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.05893, lr: 0.001\n",
      "Val Loss: 0.06146\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.05884, lr: 0.001\n",
      "Val Loss: 0.06138\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.05876, lr: 0.001\n",
      "Val Loss: 0.06131\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.05869, lr: 0.001\n",
      "Val Loss: 0.06125\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.05861, lr: 0.001\n",
      "Val Loss: 0.06119\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.05854, lr: 0.001\n",
      "Val Loss: 0.06113\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.05847, lr: 0.001\n",
      "Val Loss: 0.06108\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.05840, lr: 0.001\n",
      "Val Loss: 0.06104\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.05834, lr: 0.001\n",
      "Val Loss: 0.06101\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.05827, lr: 0.001\n",
      "Val Loss: 0.06099\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.05821, lr: 0.001\n",
      "Val Loss: 0.06097\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.05815, lr: 0.001\n",
      "Val Loss: 0.06096\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.05810, lr: 0.001\n",
      "Val Loss: 0.06095\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.05805, lr: 0.001\n",
      "Val Loss: 0.06094\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.05800, lr: 0.001\n",
      "Val Loss: 0.06094\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.05796, lr: 0.001\n",
      "Val Loss: 0.06093\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.05792, lr: 0.001\n",
      "Val Loss: 0.06093\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.05788, lr: 0.001\n",
      "Val Loss: 0.06093\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.05785, lr: 0.001\n",
      "Val Loss: 0.06093\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.05782, lr: 0.001\n",
      "Val Loss: 0.06093\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.05779, lr: 0.001\n",
      "Val Loss: 0.06093\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.05777, lr: 0.001\n",
      "Val Loss: 0.06094\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.05774, lr: 0.001\n",
      "Val Loss: 0.06094\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.05772, lr: 0.001\n",
      "Val Loss: 0.06095\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.05769, lr: 0.001\n",
      "Val Loss: 0.06095\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 65/1000, Train Loss: 0.05712, lr: 0.0005\n",
      "Val Loss: 0.06035\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.05705, lr: 0.0005\n",
      "Val Loss: 0.06034\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.05703, lr: 0.0005\n",
      "Val Loss: 0.06034\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.05701, lr: 0.0005\n",
      "Val Loss: 0.06033\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.05699, lr: 0.0005\n",
      "Val Loss: 0.06034\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.05697, lr: 0.0005\n",
      "Val Loss: 0.06033\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.05696, lr: 0.0005\n",
      "Val Loss: 0.06034\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.05694, lr: 0.0005\n",
      "Val Loss: 0.06034\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.05693, lr: 0.0005\n",
      "Val Loss: 0.06034\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.05691, lr: 0.0005\n",
      "Val Loss: 0.06034\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.05689, lr: 0.0005\n",
      "Val Loss: 0.06034\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 76/1000, Train Loss: 0.05661, lr: 0.00025\n",
      "Val Loss: 0.06026\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.05655, lr: 0.00025\n",
      "Val Loss: 0.06026\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.05654, lr: 0.00025\n",
      "Val Loss: 0.06027\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.05652, lr: 0.00025\n",
      "Val Loss: 0.06027\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.05651, lr: 0.00025\n",
      "Val Loss: 0.06028\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.05649, lr: 0.00025\n",
      "Val Loss: 0.06029\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.05647, lr: 0.00025\n",
      "Val Loss: 0.06030\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.05645, lr: 0.00025\n",
      "Val Loss: 0.06031\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.05643, lr: 0.00025\n",
      "Val Loss: 0.06032\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 85/1000, Train Loss: 0.05626, lr: 0.000125\n",
      "Val Loss: 0.06021\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.05622, lr: 0.000125\n",
      "Val Loss: 0.06021\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.05620, lr: 0.000125\n",
      "Val Loss: 0.06021\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.05618, lr: 0.000125\n",
      "Val Loss: 0.06022\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.05615, lr: 0.000125\n",
      "Val Loss: 0.06023\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.05613, lr: 0.000125\n",
      "Val Loss: 0.06025\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.05611, lr: 0.000125\n",
      "Val Loss: 0.06026\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.05608, lr: 0.000125\n",
      "Val Loss: 0.06027\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 93/1000, Train Loss: 0.05598, lr: 6.25e-05\n",
      "Val Loss: 0.06023\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.05595, lr: 6.25e-05\n",
      "Val Loss: 0.06024\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.05594, lr: 6.25e-05\n",
      "Val Loss: 0.06025\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.05592, lr: 6.25e-05\n",
      "Val Loss: 0.06026\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.05591, lr: 6.25e-05\n",
      "Val Loss: 0.06026\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.05590, lr: 6.25e-05\n",
      "Val Loss: 0.06027\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.05588, lr: 6.25e-05\n",
      "Val Loss: 0.06028\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 100/1000, Train Loss: 0.05583, lr: 3.125e-05\n",
      "Val Loss: 0.06028\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.05581, lr: 3.125e-05\n",
      "Val Loss: 0.06028\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.05580, lr: 3.125e-05\n",
      "Val Loss: 0.06029\n",
      "---------\n",
      "Epoch 103/1000, Train Loss: 0.05579, lr: 3.125e-05\n",
      "Val Loss: 0.06030\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.05579, lr: 3.125e-05\n",
      "Val Loss: 0.06030\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.05578, lr: 3.125e-05\n",
      "Val Loss: 0.06031\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.05577, lr: 3.125e-05\n",
      "Val Loss: 0.06031\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 107/1000, Train Loss: 0.05574, lr: 1.5625e-05\n",
      "Val Loss: 0.06033\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.05573, lr: 1.5625e-05\n",
      "Val Loss: 0.06034\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.05573, lr: 1.5625e-05\n",
      "Val Loss: 0.06034\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.05572, lr: 1.5625e-05\n",
      "Val Loss: 0.06034\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.05572, lr: 1.5625e-05\n",
      "Val Loss: 0.06034\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.05572, lr: 1.5625e-05\n",
      "Val Loss: 0.06035\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.05571, lr: 1.5625e-05\n",
      "Val Loss: 0.06035\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 114/1000, Train Loss: 0.05569, lr: 7.8125e-06\n",
      "Val Loss: 0.06035\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.05569, lr: 7.8125e-06\n",
      "Val Loss: 0.06035\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.05569, lr: 7.8125e-06\n",
      "Val Loss: 0.06035\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.05569, lr: 7.8125e-06\n",
      "Val Loss: 0.06035\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.05569, lr: 7.8125e-06\n",
      "Val Loss: 0.06035\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.05568, lr: 7.8125e-06\n",
      "Val Loss: 0.06036\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.05568, lr: 7.8125e-06\n",
      "Val Loss: 0.06036\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 121/1000, Train Loss: 0.05567, lr: 3.90625e-06\n",
      "Val Loss: 0.06035\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.05567, lr: 3.90625e-06\n",
      "Val Loss: 0.06035\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.05567, lr: 3.90625e-06\n",
      "Val Loss: 0.06035\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.05567, lr: 3.90625e-06\n",
      "Val Loss: 0.06035\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.05567, lr: 3.90625e-06\n",
      "Val Loss: 0.06035\n",
      "---------\n",
      "Early stopping ....\n",
      "1200.9275195598602 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 10)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 10)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 10)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 10)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 10)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 10)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 10)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 10)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 10)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 10)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 10)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 10)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 10)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 10)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 10)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 10)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 10)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 10)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 10)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 10)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 10)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 10)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 10)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 10)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 10)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 10)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 10)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 10)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 10)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 10)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 10)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 10)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 10)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 10)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 10)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 10)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 10)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 10)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 10)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 10)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 10)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 10)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 10)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 10)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 10)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 10)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 10)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 10)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 10)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 10)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 10)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 10)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 10)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 10)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 10)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 10)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 10)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 10)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 10)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 10)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 10)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 10)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 10)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 10)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 10)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 10)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 10)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 10)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 10)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 10)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 10)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 10)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 10)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 10)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 10)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 10)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 10)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 10)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 10)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 10)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 10)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 10)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 10)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 10)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 10)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 10)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 10)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 10)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 10)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 10)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 10)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 10)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 10)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 10)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 10)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 10)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 10)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 10)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 10)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 10)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 10)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 10)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 10)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 10)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 10)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 10)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 10)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 10)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 10)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 10)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 10)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 10)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 10)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 10)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 10)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 10)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 10)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 10)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 10)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 10)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 10)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 10)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 10)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 10)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 10)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 10)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 10)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 10)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 10)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 10)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 10)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 10)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 10)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 10)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 10)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 10)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 10)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 10)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 10)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 10)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 10)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 10)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 10)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 10)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 10)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 10)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 10)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 10)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 10)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 10)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 10)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 10)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 10)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 10)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 10)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 10)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 10)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 10)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 10)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 10)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 10)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 10)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 10)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 10)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 10)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 10)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 10)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 10)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 10)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 10)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 10)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 10)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 10)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 10)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 10)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 10)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 10)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 10)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 10)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 10)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 10)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 10)\n",
      "182\n",
      "y_hat: (1457, 32, 48, 6, 10), y_hat_i: (1, 32, 48, 6, 10), y_i: (1, 32, 48, 6, 10), batch.x: torch.Size([32, 48, 6, 6]), y: (1457, 32, 48, 6, 10)\n",
      "RMSE for t2m: 3.9722218357550587; MAE for t2m: 3.0539170723155262;\n",
      "RMSE for sp: 6.459822869588417; MAE for sp: 4.767141513751561;\n",
      "RMSE for tcc: 0.36737757139789845; MAE for tcc: 0.2816137474426247;\n",
      "RMSE for u10: 2.5743139759817475; MAE for u10: 1.9462451155661393;\n",
      "RMSE for v10: 2.471403389204583; MAE for v10: 1.8912936979082948;\n",
      "RMSE for tp: 0.31965173727632995; MAE for tp: 0.08529921797074734;\n",
      "0\n",
      "y_hat: (8, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 10)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 10)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 10)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 10)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 10)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 10)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 10)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 10)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 10)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 10)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 10)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 10)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 10)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 10)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 10)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 10)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 10)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 10)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 10)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 10)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 10)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 10)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 10)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 10)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 10)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 10)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 10)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 10)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 10)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 10)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 10)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 10)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 10)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 10)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 10)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 10)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 10)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 10)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 10)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 10)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 10)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 10)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 10)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 10)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 10)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 10)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 10)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 10)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 10)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 10)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 10)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 10)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 10)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 10)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 10)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 10)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 10)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 10)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 10)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 10)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 10)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 10)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 10)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 10)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 10)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 10)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 10)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 10)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 10)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 10)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 10)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 10)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 10)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 10)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 10)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 10)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 10)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 10)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 10)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 10)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 10)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 10)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 10)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 10)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 10)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 10)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 10)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 10)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 10)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 10)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 10)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 10)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 10)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 10)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 10)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 10)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 10)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 10)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 10)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 10)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 10)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 10)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 10)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 10)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 10)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 10)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 10)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 10)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 10)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 10)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 10)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 10)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 10)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 10)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 10)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 10)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 10)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 10)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 10)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 10)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 10)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 10)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 10)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 10)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 10)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 10)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 10)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 10)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 10)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 10)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 10)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 10)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 10)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 10)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 10)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 10)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 10)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 10)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 10)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 10)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 10)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 10)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 10)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 10)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 10)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 10)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 10)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 10)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 10)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 10)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 10)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 10)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 10)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 10)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 10)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 10)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 10)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 10)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 10)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 10)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 10)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 10)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 10)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 10)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 10)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 10)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 10)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 10)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 10)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 10)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 10)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 10)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 10)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 10)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 10)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 10)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 10)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 10)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 10)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 10)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 10)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 10), y_hat_i: (8, 32, 48, 6, 10), y_i: (8, 32, 48, 6, 10), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 10)\n",
      "182\n",
      "y_hat: (1457, 32, 48, 6, 10), y_hat_i: (1, 32, 48, 6, 10), y_i: (1, 32, 48, 6, 10), batch.x: torch.Size([32, 48, 6, 6]), y: (1457, 32, 48, 6, 10)\n",
      "RMSE for t2m: 3.9722218357550587; MAE for t2m: 3.0539170723155262;\n",
      "RMSE for sp: 6.459822869588417; MAE for sp: 4.767141513751561;\n",
      "RMSE for tcc: 0.36674819731448816; MAE for tcc: 0.2807508678902075;\n",
      "RMSE for u10: 2.5743139759817475; MAE for u10: 1.9462451155661393;\n",
      "RMSE for v10: 2.471403389204583; MAE for v10: 1.8912936979082948;\n",
      "RMSE for tp: 0.31965173727632995; MAE for tp: 0.08529921797074734;\n",
      "Forcasting Horizon Progress: |-----| 90.9% Complete\r"
     ]
    }
   ],
   "source": [
    "hpo.determine_best_fh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo.write_plots_to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 0.07712, lr: 0.001\n",
      "Val Loss: 0.06571\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.05896, lr: 0.001\n",
      "Val Loss: 0.05601\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.05443, lr: 0.001\n",
      "Val Loss: 0.05305\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.04987, lr: 0.001\n",
      "Val Loss: 0.04827\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.04667, lr: 0.001\n",
      "Val Loss: 0.04559\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.04352, lr: 0.001\n",
      "Val Loss: 0.04243\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.04141, lr: 0.001\n",
      "Val Loss: 0.04137\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.04055, lr: 0.001\n",
      "Val Loss: 0.04206\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.04001, lr: 0.001\n",
      "Val Loss: 0.04115\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.03942, lr: 0.001\n",
      "Val Loss: 0.04139\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.03913, lr: 0.001\n",
      "Val Loss: 0.04131\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.03881, lr: 0.001\n",
      "Val Loss: 0.04177\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.03853, lr: 0.001\n",
      "Val Loss: 0.04145\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.03837, lr: 0.001\n",
      "Val Loss: 0.04122\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.03817, lr: 0.001\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.03790, lr: 0.001\n",
      "Val Loss: 0.04068\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.03770, lr: 0.001\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.03754, lr: 0.001\n",
      "Val Loss: 0.04031\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.03741, lr: 0.001\n",
      "Val Loss: 0.04054\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.03725, lr: 0.001\n",
      "Val Loss: 0.04015\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.03714, lr: 0.001\n",
      "Val Loss: 0.04013\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.03700, lr: 0.001\n",
      "Val Loss: 0.04042\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.03688, lr: 0.001\n",
      "Val Loss: 0.04038\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.03680, lr: 0.001\n",
      "Val Loss: 0.04079\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.03671, lr: 0.001\n",
      "Val Loss: 0.04090\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.03659, lr: 0.001\n",
      "Val Loss: 0.03989\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.03649, lr: 0.001\n",
      "Val Loss: 0.04031\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.03644, lr: 0.001\n",
      "Val Loss: 0.04005\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.03633, lr: 0.001\n",
      "Val Loss: 0.04081\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.03621, lr: 0.001\n",
      "Val Loss: 0.04001\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.03612, lr: 0.001\n",
      "Val Loss: 0.03977\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.03602, lr: 0.001\n",
      "Val Loss: 0.03898\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.03597, lr: 0.001\n",
      "Val Loss: 0.03913\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.03595, lr: 0.001\n",
      "Val Loss: 0.03883\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.03584, lr: 0.001\n",
      "Val Loss: 0.03876\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.03580, lr: 0.001\n",
      "Val Loss: 0.03849\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.03571, lr: 0.001\n",
      "Val Loss: 0.03799\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.03556, lr: 0.001\n",
      "Val Loss: 0.03799\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.03553, lr: 0.001\n",
      "Val Loss: 0.03793\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.03539, lr: 0.001\n",
      "Val Loss: 0.03756\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.03521, lr: 0.001\n",
      "Val Loss: 0.03752\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.03507, lr: 0.001\n",
      "Val Loss: 0.03740\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.03507, lr: 0.001\n",
      "Val Loss: 0.03756\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.03491, lr: 0.001\n",
      "Val Loss: 0.03778\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.03496, lr: 0.001\n",
      "Val Loss: 0.03836\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.03491, lr: 0.001\n",
      "Val Loss: 0.03783\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.03496, lr: 0.001\n",
      "Val Loss: 0.03768\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.03507, lr: 0.001\n",
      "Val Loss: 0.03818\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.03530, lr: 0.001\n",
      "Val Loss: 0.03740\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.03526, lr: 0.001\n",
      "Val Loss: 0.03718\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.03503, lr: 0.001\n",
      "Val Loss: 0.03721\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.03491, lr: 0.001\n",
      "Val Loss: 0.03745\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.03465, lr: 0.001\n",
      "Val Loss: 0.03713\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.03459, lr: 0.001\n",
      "Val Loss: 0.03771\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.03468, lr: 0.001\n",
      "Val Loss: 0.03777\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.03457, lr: 0.001\n",
      "Val Loss: 0.03766\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.03457, lr: 0.001\n",
      "Val Loss: 0.03700\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.03463, lr: 0.001\n",
      "Val Loss: 0.03739\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.03432, lr: 0.001\n",
      "Val Loss: 0.03703\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.03429, lr: 0.001\n",
      "Val Loss: 0.03736\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.03491, lr: 0.001\n",
      "Val Loss: 0.03848\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.03451, lr: 0.001\n",
      "Val Loss: 0.03753\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.03461, lr: 0.001\n",
      "Val Loss: 0.03804\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.03429, lr: 0.001\n",
      "Val Loss: 0.03796\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 65/1000, Train Loss: 0.03378, lr: 0.0005\n",
      "Val Loss: 0.03664\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.03348, lr: 0.0005\n",
      "Val Loss: 0.03654\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.03330, lr: 0.0005\n",
      "Val Loss: 0.03666\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.03324, lr: 0.0005\n",
      "Val Loss: 0.03665\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.03313, lr: 0.0005\n",
      "Val Loss: 0.03656\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.03305, lr: 0.0005\n",
      "Val Loss: 0.03662\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.03296, lr: 0.0005\n",
      "Val Loss: 0.03664\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.03283, lr: 0.0005\n",
      "Val Loss: 0.03692\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.03274, lr: 0.0005\n",
      "Val Loss: 0.03665\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 74/1000, Train Loss: 0.03272, lr: 0.00025\n",
      "Val Loss: 0.03561\n",
      "---------\n",
      "Epoch 75/1000, Train Loss: 0.03258, lr: 0.00025\n",
      "Val Loss: 0.03562\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.03245, lr: 0.00025\n",
      "Val Loss: 0.03564\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.03234, lr: 0.00025\n",
      "Val Loss: 0.03565\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.03225, lr: 0.00025\n",
      "Val Loss: 0.03564\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.03222, lr: 0.00025\n",
      "Val Loss: 0.03570\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.03214, lr: 0.00025\n",
      "Val Loss: 0.03565\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.03205, lr: 0.00025\n",
      "Val Loss: 0.03565\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 82/1000, Train Loss: 0.03213, lr: 0.000125\n",
      "Val Loss: 0.03583\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.03203, lr: 0.000125\n",
      "Val Loss: 0.03584\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.03194, lr: 0.000125\n",
      "Val Loss: 0.03587\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.03188, lr: 0.000125\n",
      "Val Loss: 0.03590\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.03181, lr: 0.000125\n",
      "Val Loss: 0.03593\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.03176, lr: 0.000125\n",
      "Val Loss: 0.03597\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.03171, lr: 0.000125\n",
      "Val Loss: 0.03597\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 89/1000, Train Loss: 0.03175, lr: 6.25e-05\n",
      "Val Loss: 0.03588\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.03170, lr: 6.25e-05\n",
      "Val Loss: 0.03589\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.03165, lr: 6.25e-05\n",
      "Val Loss: 0.03587\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.03161, lr: 6.25e-05\n",
      "Val Loss: 0.03588\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.03158, lr: 6.25e-05\n",
      "Val Loss: 0.03589\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.03154, lr: 6.25e-05\n",
      "Val Loss: 0.03590\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.03151, lr: 6.25e-05\n",
      "Val Loss: 0.03590\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 96/1000, Train Loss: 0.03162, lr: 3.125e-05\n",
      "Val Loss: 0.03574\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.03155, lr: 3.125e-05\n",
      "Val Loss: 0.03575\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.03152, lr: 3.125e-05\n",
      "Val Loss: 0.03575\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.03149, lr: 3.125e-05\n",
      "Val Loss: 0.03575\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.03147, lr: 3.125e-05\n",
      "Val Loss: 0.03576\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.03145, lr: 3.125e-05\n",
      "Val Loss: 0.03576\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.03143, lr: 3.125e-05\n",
      "Val Loss: 0.03576\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 103/1000, Train Loss: 0.03144, lr: 1.5625e-05\n",
      "Val Loss: 0.03560\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.03139, lr: 1.5625e-05\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.03137, lr: 1.5625e-05\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.03136, lr: 1.5625e-05\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.03135, lr: 1.5625e-05\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.03134, lr: 1.5625e-05\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.03132, lr: 1.5625e-05\n",
      "Val Loss: 0.03560\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.03131, lr: 1.5625e-05\n",
      "Val Loss: 0.03560\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.03130, lr: 1.5625e-05\n",
      "Val Loss: 0.03560\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.03129, lr: 1.5625e-05\n",
      "Val Loss: 0.03560\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 113/1000, Train Loss: 0.03130, lr: 7.8125e-06\n",
      "Val Loss: 0.03555\n",
      "---------\n",
      "Epoch 114/1000, Train Loss: 0.03128, lr: 7.8125e-06\n",
      "Val Loss: 0.03555\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.03127, lr: 7.8125e-06\n",
      "Val Loss: 0.03555\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.03126, lr: 7.8125e-06\n",
      "Val Loss: 0.03555\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.03126, lr: 7.8125e-06\n",
      "Val Loss: 0.03555\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.03125, lr: 7.8125e-06\n",
      "Val Loss: 0.03555\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.03125, lr: 7.8125e-06\n",
      "Val Loss: 0.03555\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.03124, lr: 7.8125e-06\n",
      "Val Loss: 0.03555\n",
      "---------\n",
      "Epoch 121/1000, Train Loss: 0.03124, lr: 7.8125e-06\n",
      "Val Loss: 0.03555\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.03123, lr: 7.8125e-06\n",
      "Val Loss: 0.03555\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.03123, lr: 7.8125e-06\n",
      "Val Loss: 0.03555\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.03122, lr: 7.8125e-06\n",
      "Val Loss: 0.03555\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 125/1000, Train Loss: 0.03122, lr: 3.90625e-06\n",
      "Val Loss: 0.03552\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.03120, lr: 3.90625e-06\n",
      "Val Loss: 0.03552\n",
      "---------\n",
      "Epoch 127/1000, Train Loss: 0.03120, lr: 3.90625e-06\n",
      "Val Loss: 0.03552\n",
      "---------\n",
      "Epoch 128/1000, Train Loss: 0.03119, lr: 3.90625e-06\n",
      "Val Loss: 0.03552\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.03119, lr: 3.90625e-06\n",
      "Val Loss: 0.03552\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.03119, lr: 3.90625e-06\n",
      "Val Loss: 0.03552\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.03118, lr: 3.90625e-06\n",
      "Val Loss: 0.03552\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.03118, lr: 3.90625e-06\n",
      "Val Loss: 0.03552\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.03118, lr: 3.90625e-06\n",
      "Val Loss: 0.03552\n",
      "---------\n",
      "Epoch 134/1000, Train Loss: 0.03117, lr: 3.90625e-06\n",
      "Val Loss: 0.03552\n",
      "---------\n",
      "Epoch 135/1000, Train Loss: 0.03117, lr: 3.90625e-06\n",
      "Val Loss: 0.03552\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 136/1000, Train Loss: 0.03117, lr: 1.953125e-06\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.03116, lr: 1.953125e-06\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.03116, lr: 1.953125e-06\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.03116, lr: 1.953125e-06\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.03115, lr: 1.953125e-06\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 141/1000, Train Loss: 0.03115, lr: 1.953125e-06\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 142/1000, Train Loss: 0.03115, lr: 1.953125e-06\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.03115, lr: 1.953125e-06\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.03115, lr: 1.953125e-06\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 145/1000, Train Loss: 0.03114, lr: 9.765625e-07\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 146/1000, Train Loss: 0.03114, lr: 9.765625e-07\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 147/1000, Train Loss: 0.03114, lr: 9.765625e-07\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 148/1000, Train Loss: 0.03114, lr: 9.765625e-07\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 149/1000, Train Loss: 0.03114, lr: 9.765625e-07\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 150/1000, Train Loss: 0.03114, lr: 9.765625e-07\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 151/1000, Train Loss: 0.03114, lr: 9.765625e-07\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 152/1000, Train Loss: 0.03113, lr: 9.765625e-07\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 153/1000, Train Loss: 0.03113, lr: 9.765625e-07\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 154/1000, Train Loss: 0.03113, lr: 9.765625e-07\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 155/1000, Train Loss: 0.03113, lr: 4.8828125e-07\n",
      "Val Loss: 0.03551\n",
      "---------\n",
      "Epoch 156/1000, Train Loss: 0.03113, lr: 4.8828125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 157/1000, Train Loss: 0.03113, lr: 4.8828125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 158/1000, Train Loss: 0.03113, lr: 4.8828125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 159/1000, Train Loss: 0.03113, lr: 4.8828125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 160/1000, Train Loss: 0.03113, lr: 4.8828125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 161/1000, Train Loss: 0.03113, lr: 4.8828125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 162/1000, Train Loss: 0.03113, lr: 4.8828125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 163/1000, Train Loss: 0.03113, lr: 4.8828125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 164/1000, Train Loss: 0.03113, lr: 4.8828125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 165/1000, Train Loss: 0.03112, lr: 2.44140625e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 166/1000, Train Loss: 0.03112, lr: 2.44140625e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 167/1000, Train Loss: 0.03112, lr: 2.44140625e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 168/1000, Train Loss: 0.03112, lr: 2.44140625e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 169/1000, Train Loss: 0.03112, lr: 2.44140625e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 170/1000, Train Loss: 0.03112, lr: 2.44140625e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 171/1000, Train Loss: 0.03112, lr: 2.44140625e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 172/1000, Train Loss: 0.03112, lr: 2.44140625e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 173/1000, Train Loss: 0.03112, lr: 2.44140625e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 174/1000, Train Loss: 0.03112, lr: 2.44140625e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 175/1000, Train Loss: 0.03112, lr: 2.44140625e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 176/1000, Train Loss: 0.03112, lr: 2.44140625e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 177/1000, Train Loss: 0.03112, lr: 2.44140625e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 178/1000, Train Loss: 0.03112, lr: 2.44140625e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 179/1000, Train Loss: 0.03112, lr: 2.44140625e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 180/1000, Train Loss: 0.03112, lr: 1.220703125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 181/1000, Train Loss: 0.03112, lr: 1.220703125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 182/1000, Train Loss: 0.03112, lr: 1.220703125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 183/1000, Train Loss: 0.03112, lr: 1.220703125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 184/1000, Train Loss: 0.03112, lr: 1.220703125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 185/1000, Train Loss: 0.03112, lr: 1.220703125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 186/1000, Train Loss: 0.03112, lr: 1.220703125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 187/1000, Train Loss: 0.03112, lr: 1.220703125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 188/1000, Train Loss: 0.03112, lr: 1.220703125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 189/1000, Train Loss: 0.03112, lr: 1.220703125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 190/1000, Train Loss: 0.03112, lr: 1.220703125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 191/1000, Train Loss: 0.03112, lr: 1.220703125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 192/1000, Train Loss: 0.03112, lr: 1.220703125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 193/1000, Train Loss: 0.03112, lr: 1.220703125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 194/1000, Train Loss: 0.03112, lr: 1.220703125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 195/1000, Train Loss: 0.03112, lr: 1.220703125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 196/1000, Train Loss: 0.03112, lr: 1.220703125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 197/1000, Train Loss: 0.03112, lr: 1.220703125e-07\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 198/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 199/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 200/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 201/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 202/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 203/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 204/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 205/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 206/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 207/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 208/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 209/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 210/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 211/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 212/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 213/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 214/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 215/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 216/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 217/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 218/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 219/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 220/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 221/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 222/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 223/1000, Train Loss: 0.03112, lr: 6.103515625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 224/1000, Train Loss: 0.03112, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 225/1000, Train Loss: 0.03112, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 226/1000, Train Loss: 0.03112, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 227/1000, Train Loss: 0.03112, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 228/1000, Train Loss: 0.03112, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 229/1000, Train Loss: 0.03112, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 230/1000, Train Loss: 0.03112, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 231/1000, Train Loss: 0.03112, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 232/1000, Train Loss: 0.03111, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 233/1000, Train Loss: 0.03111, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 234/1000, Train Loss: 0.03111, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 235/1000, Train Loss: 0.03111, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 236/1000, Train Loss: 0.03111, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 237/1000, Train Loss: 0.03111, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 238/1000, Train Loss: 0.03111, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 239/1000, Train Loss: 0.03111, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 240/1000, Train Loss: 0.03111, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 241/1000, Train Loss: 0.03111, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 242/1000, Train Loss: 0.03111, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 243/1000, Train Loss: 0.03111, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 244/1000, Train Loss: 0.03111, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 245/1000, Train Loss: 0.03111, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 246/1000, Train Loss: 0.03111, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 247/1000, Train Loss: 0.03111, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 248/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 249/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 250/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 251/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 252/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 253/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 254/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 255/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 256/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 257/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 258/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 259/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 260/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 261/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 262/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 263/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 264/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 265/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 266/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 267/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 268/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 269/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 270/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 271/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 272/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 273/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 274/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 275/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 276/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 277/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 278/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 279/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 280/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 281/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 282/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 283/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 284/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 285/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 286/1000, Train Loss: 0.03111, lr: 1.52587890625e-08\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 287/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 288/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 289/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 290/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 291/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 292/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 293/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 294/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 295/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 296/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 297/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 298/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 299/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 300/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 301/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 302/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 303/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 304/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 305/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 306/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 307/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 308/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 309/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 310/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 311/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 312/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 313/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 314/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 315/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 316/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 317/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 318/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 319/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 320/1000, Train Loss: 0.03111, lr: 7.62939453125e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 321/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 322/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 323/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 324/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 325/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 326/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 327/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 328/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 329/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 330/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 331/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 332/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 333/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 334/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 335/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 336/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 337/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 338/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 339/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 340/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 341/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 342/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 343/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 344/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 345/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 346/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 347/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 348/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 349/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 350/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 351/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 352/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 353/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 354/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 355/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 356/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 357/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 358/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 359/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 360/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 361/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 362/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 363/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 364/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 365/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 366/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 367/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 368/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 369/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 370/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 371/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 372/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 373/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 374/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 375/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 376/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 377/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 378/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 379/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 380/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 381/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 382/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 383/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 384/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 385/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 386/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 387/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 388/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 389/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 390/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 391/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 392/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 393/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 394/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 395/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 396/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 397/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 398/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 399/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 400/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 401/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 402/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 403/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 404/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 405/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 406/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 407/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 408/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 409/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 410/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 411/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 412/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 413/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 414/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 415/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 416/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 417/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 418/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 419/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 420/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 421/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 422/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 423/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 424/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 425/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 426/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 427/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 428/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 429/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 430/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 431/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 432/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 433/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 434/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 435/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 436/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 437/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 438/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 439/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 440/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 441/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 442/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 443/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 444/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 445/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 446/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 447/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 448/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 449/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 450/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 451/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 452/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 453/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 454/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 455/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 456/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 457/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 458/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 459/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 460/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 461/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 462/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 463/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 464/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 465/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 466/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 467/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 468/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 469/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 470/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 471/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 472/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 473/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 474/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 475/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 476/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 477/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 478/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 479/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 480/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 481/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 482/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 483/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 484/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 485/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 486/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 487/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 488/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 489/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 490/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 491/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 492/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 493/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 494/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 495/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 496/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 497/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 498/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 499/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 500/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 501/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 502/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 503/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 504/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 505/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 506/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 507/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 508/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 509/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 510/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 511/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 512/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 513/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 514/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 515/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 516/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 517/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 518/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 519/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 520/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 521/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 522/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 523/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 524/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 525/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 526/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 527/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 528/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 529/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 530/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 531/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 532/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 533/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 534/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 535/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 536/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 537/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 538/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 539/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 540/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 541/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 542/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 543/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 544/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 545/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 546/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 547/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 548/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 549/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 550/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 551/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 552/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 553/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 554/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 555/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 556/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 557/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 558/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 559/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 560/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 561/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 562/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 563/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 564/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 565/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 566/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 567/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 568/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 569/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 570/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 571/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 572/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 573/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 574/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 575/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 576/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 577/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 578/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 579/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 580/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 581/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 582/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 583/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 584/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 585/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 586/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 587/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 588/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 589/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 590/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 591/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 592/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 593/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 594/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 595/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 596/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 597/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 598/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 599/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 600/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 601/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 602/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 603/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 604/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 605/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 606/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 607/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 608/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 609/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 610/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 611/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 612/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 613/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 614/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 615/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 616/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 617/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 618/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 619/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 620/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 621/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 622/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 623/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 624/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 625/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 626/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 627/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 628/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 629/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 630/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 631/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 632/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 633/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 634/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 635/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 636/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 637/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 638/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 639/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 640/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 641/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 642/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 643/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 644/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 645/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 646/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 647/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 648/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 649/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 650/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 651/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 652/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 653/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 654/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 655/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 656/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 657/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 658/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 659/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 660/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 661/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 662/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 663/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 664/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 665/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 666/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 667/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 668/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 669/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 670/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 671/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 672/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 673/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 674/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 675/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 676/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 677/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 678/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 679/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 680/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 681/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 682/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 683/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 684/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 685/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 686/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 687/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 688/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 689/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 690/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 691/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 692/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 693/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 694/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 695/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 696/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 697/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 698/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 699/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 700/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 701/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 702/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 703/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 704/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 705/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 706/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 707/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 708/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 709/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 710/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 711/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 712/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 713/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 714/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 715/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 716/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 717/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 718/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 719/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 720/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 721/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 722/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 723/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 724/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 725/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 726/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 727/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 728/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 729/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 730/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 731/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 732/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 733/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 734/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 735/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 736/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 737/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 738/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 739/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 740/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 741/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 742/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 743/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 744/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 745/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 746/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 747/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 748/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 749/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 750/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 751/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 752/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 753/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 754/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 755/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 756/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 757/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 758/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 759/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 760/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 761/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 762/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 763/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 764/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 765/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 766/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 767/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 768/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 769/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 770/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 771/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 772/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 773/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 774/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 775/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 776/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 777/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 778/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 779/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 780/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 781/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 782/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 783/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 784/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 785/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 786/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 787/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 788/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 789/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 790/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 791/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 792/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 793/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 794/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 795/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 796/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 797/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 798/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 799/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 800/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 801/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 802/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 803/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 804/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 805/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 806/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 807/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 808/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 809/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 810/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 811/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 812/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 813/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 814/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 815/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 816/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 817/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 818/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 819/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 820/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 821/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 822/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 823/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 824/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 825/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 826/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 827/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 828/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 829/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 830/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 831/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 832/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 833/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 834/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 835/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 836/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 837/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 838/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 839/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 840/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 841/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 842/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 843/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 844/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 845/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 846/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 847/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 848/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 849/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 850/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 851/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 852/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 853/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 854/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 855/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 856/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 857/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 858/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 859/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 860/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 861/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 862/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 863/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 864/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 865/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 866/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 867/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 868/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 869/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 870/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 871/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 872/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 873/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 874/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 875/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 876/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 877/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 878/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 879/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 880/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 881/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 882/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 883/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 884/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 885/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 886/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 887/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 888/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 889/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 890/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 891/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 892/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 893/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 894/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 895/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 896/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 897/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 898/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 899/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 900/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 901/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 902/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 903/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 904/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 905/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 906/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 907/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 908/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 909/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 910/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 911/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 912/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 913/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 914/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 915/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 916/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 917/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 918/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 919/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 920/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 921/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 922/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 923/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 924/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 925/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 926/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 927/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 928/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 929/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 930/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 931/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 932/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 933/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 934/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 935/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 936/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 937/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 938/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 939/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 940/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 941/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 942/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 943/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 944/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 945/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 946/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 947/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 948/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 949/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 950/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 951/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 952/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 953/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 954/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 955/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 956/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 957/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 958/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 959/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 960/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 961/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 962/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 963/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 964/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 965/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 966/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 967/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 968/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 969/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 970/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 971/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 972/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 973/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 974/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 975/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 976/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 977/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 978/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 979/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 980/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 981/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 982/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 983/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 984/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 985/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 986/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 987/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 988/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 989/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 990/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 991/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 992/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 993/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 994/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 995/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 996/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 997/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 998/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 999/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 1000/1000, Train Loss: 0.03111, lr: 3.814697265625e-09\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "4713.575623035431 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABkoAAAJdCAYAAAB9KSs4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABoFElEQVR4nO3deXzcdZ0/8NckaZLeB4W2QKHc932UIopI14Iscigiohzi7uoCoqy4wirgtVUUf6iwIh6LurIouqJyaa2CHJUbuU+BVqAtBdqUliZpZn5/TJI2tAVS0k4683w+HuPMfOczM+9vUr4+6Iv3510olUqlAAAAAAAA1KC6ShcAAAAAAABQKYISAAAAAACgZglKAAAAAACAmiUoAQAAAAAAapagBAAAAAAAqFmCEgAAAAAAoGYJSgAAAAAAgJolKAEAAAAAAGqWoAQAAAAAAKhZghIAAICVKBQKOffccytdBgAAsIYJSgAAgDft0ksvTaFQyB133FHpUiruwQcfzLnnnpunnnqq0qUAAABvgKAEAACgDz344IP5/Oc/LygBAIB1hKAEAAAAAACoWYISAABgrbn77rtz8MEHZ9iwYRkyZEgOPPDA/OUvf+mxpr29PZ///Oez1VZbpbm5Oeutt17222+/TJs2rXvN7Nmzc+KJJ2bjjTdOU1NTxo0bl8MOO+x1uzhOOOGEDBkyJH/7298yZcqUDB48OBtuuGG+8IUvpFQqven6L7300hx11FFJkgMOOCCFQiGFQiHXX3/9G/8hAQAAa1VDpQsAAABqwwMPPJC3vvWtGTZsWD796U9nwIAB+e53v5u3v/3tueGGGzJx4sQkybnnnpupU6fmIx/5SPbee++0tLTkjjvuyF133ZV/+Id/SJK85z3vyQMPPJBTTz01EyZMyNy5czNt2rTMnDkzEyZMeM06Ojo6ctBBB2WfffbJeeedl+uuuy7nnHNOli5dmi984Qtvqv63ve1t+fjHP55vfetbOeuss7LddtslSfc9AADQ/xRKb+Q/mwIAAHgNl156aU488cTcfvvt2XPPPVe65ogjjsg111yThx56KJtvvnmS5Lnnnss222yT3XbbLTfccEOSZNddd83GG2+cq666aqWfM3/+/IwcOTJf+9rX8qlPfapXdZ5wwgn50Y9+lFNPPTXf+ta3kiSlUimHHnpopk2blmeeeSajR49OkhQKhZxzzjk599xze1X/L37xixx11FH505/+lLe//e29qg8AAFj7bL0FAACscR0dHfn973+fww8/vDtkSJJx48blAx/4QG666aa0tLQkSUaMGJEHHnggjz322Eo/a+DAgWlsbMz111+fl156abXqOeWUU7ofFwqFnHLKKWlra8sf/vCHN10/AACwbhGUAAAAa9zzzz+fxYsXZ5tttlnhte222y7FYjGzZs1KknzhC1/I/Pnzs/XWW2ennXbKGWeckXvvvbd7fVNTU7761a/m2muvzZgxY/K2t70t5513XmbPnv2Gaqmrq+sRdiTJ1ltvnSSrnHHSm/oBAIB1i6AEAADoV972trfliSeeyA9/+MPsuOOO+f73v5/dd9893//+97vXfOITn8ijjz6aqVOnprm5OZ/73Oey3Xbb5e67765g5QAAwLpIUAIAAKxx66+/fgYNGpRHHnlkhdcefvjh1NXVZfz48d3HRo0alRNPPDH/+7//m1mzZmXnnXfunhXSZYsttsi//du/5fe//33uv//+tLW15fzzz3/dWorFYv72t7/1OPboo48mySoHwfem/kKh8Lo1AAAA/YegBAAAWOPq6+vzzne+M7/+9a97bG81Z86cXHbZZdlvv/0ybNiwJMkLL7zQ471DhgzJlltumdbW1iTJ4sWLs2TJkh5rtthiiwwdOrR7zeu58MILux+XSqVceOGFGTBgQA488MA3Xf/gwYOTlIfOAwAA/V9DpQsAAACqxw9/+MNcd911Kxw/7bTT8qUvfSnTpk3Lfvvtl3/9139NQ0NDvvvd76a1tTXnnXde99rtt98+b3/727PHHntk1KhRueOOO/KLX/yiewD7o48+mgMPPDDve9/7sv3226ehoSG/+tWvMmfOnLz//e9/3Rqbm5tz3XXX5fjjj8/EiRNz7bXX5uqrr85ZZ52V9ddff5Xve6P177rrrqmvr89Xv/rVLFiwIE1NTXnHO96RDTbYoDc/SgAAYC0RlAAAAH3mO9/5zkqPn3DCCdlhhx1y44035swzz8zUqVNTLBYzceLE/M///E8mTpzYvfbjH/94fvOb3+T3v/99Wltbs+mmm+ZLX/pSzjjjjCTJ+PHjc8wxx2T69On5yU9+koaGhmy77bb5+c9/nve85z2vW2N9fX2uu+66fOxjH8sZZ5yRoUOH5pxzzsnZZ5/9mu97o/WPHTs2F198caZOnZqTTjopHR0d+dOf/iQoAQCAfqpQKpVKlS4CAABgbTjhhBPyi1/8Ii+//HKlSwEAAPoJM0oAAAAAAICaJSgBAAAAAABqlqAEAAAAAACoWWaUAAAAAAAANUtHCQAAAAAAULMEJQAAAAAAQM1qqHQBfaFYLObZZ5/N0KFDUygUKl0OAAAAAABQQaVSKQsXLsyGG26YurrX7hmpiqDk2Wefzfjx4ytdBgAAAAAA0I/MmjUrG2+88WuuqYqgZOjQoUnKJzxs2LAKVwMAAAAAAFRSS0tLxo8f350fvJaqCEq6ttsaNmyYoAQAAAAAAEiSNzSuwzB3AAAAAACgZglKAAAAAACAmiUoAQAAAAAAalZVzCgBAAAAAIDV1dHRkfb29kqXQS8NGDAg9fX1b/pzBCUAAAAAANSkUqmU2bNnZ/78+ZUuhdU0YsSIjB079g0NbV8VQQkAAAAAADWpKyTZYIMNMmjQoDf1l+2sXaVSKYsXL87cuXOTJOPGjVvtzxKUAAAAAABQczo6OrpDkvXWW6/S5bAaBg4cmCSZO3duNthgg9XehsswdwAAAAAAak7XTJJBgwZVuBLejK7f35uZMSMoAQAAAACgZtlua93WF78/QQkAAAAAAFCzBCUAAAAAAFCjJkyYkAsuuKDin1FJhrkDAAAAAMA64u1vf3t23XXXPgsmbr/99gwePLhPPmtdJSgBAAAAAIAqUiqV0tHRkYaG148A1l9//bVQUf9m6y0AAAAAAFgHnHDCCbnhhhvyzW9+M4VCIYVCIU899VSuv/76FAqFXHvttdljjz3S1NSUm266KU888UQOO+ywjBkzJkOGDMlee+2VP/zhDz0+89XbZhUKhXz/+9/PEUcckUGDBmWrrbbKb37zm17VOXPmzBx22GEZMmRIhg0blve9732ZM2dO9+t//etfc8ABB2To0KEZNmxY9thjj9xxxx1JkqeffjqHHnpoRo4cmcGDB2eHHXbINddcs/o/tDdARwkAAAAAAKTcifFKe8da/96BA+pTKBRed903v/nNPProo9lxxx3zhS98IUm5I+Spp55KknzmM5/J17/+9Wy++eYZOXJkZs2alXe961358pe/nKampvz4xz/OoYcemkceeSSbbLLJKr/n85//fM4777x87Wtfy7e//e0ce+yxefrppzNq1KjXrbFYLHaHJDfccEOWLl2ak08+OUcffXSuv/76JMmxxx6b3XbbLd/5zndSX1+fe+65JwMGDEiSnHzyyWlra8uf//znDB48OA8++GCGDBnyut/7ZghKAAAAAAAgySvtHdn+7N+t9e998AtTMqjx9f+6fvjw4WlsbMygQYMyduzYFV7/whe+kH/4h3/ofj5q1Kjssssu3c+/+MUv5le/+lV+85vf5JRTTlnl95xwwgk55phjkiT/+Z//mW9961u57bbbctBBB71ujdOnT899992XJ598MuPHj0+S/PjHP84OO+yQ22+/PXvttVdmzpyZM844I9tuu22SZKuttup+/8yZM/Oe97wnO+20U5Jk8803f93vfLNsvQUAAAAAAFVgzz337PH85Zdfzqc+9alst912GTFiRIYMGZKHHnooM2fOfM3P2XnnnbsfDx48OMOGDcvcuXPfUA0PPfRQxo8f3x2SJMn222+fESNG5KGHHkqSnH766fnIRz6SyZMn5ytf+UqeeOKJ7rUf//jH86UvfSlvectbcs455+Tee+99Q9/7ZugoAQAAAACAlLfAevALUyryvX1h8ODBPZ5/6lOfyrRp0/L1r389W265ZQYOHJj3vve9aWtre83P6doGq0uhUEixWOyTGpPk3HPPzQc+8IFcffXVufbaa3POOefk8ssvzxFHHJGPfOQjmTJlSq6++ur8/ve/z9SpU3P++efn1FNP7bPvfzVBCQAAAAAApBwIvJEtsCqpsbExHR1vbI7KzTffnBNOOCFHHHFEknKHSdc8kzVlu+22y6xZszJr1qzurpIHH3ww8+fPz/bbb9+9buutt87WW2+dT37ykznmmGPy3//93911jh8/Ph/96Efz0Y9+NGeeeWa+973vrdGgxNZbAAAAAACwjpgwYUJuvfXWPPXUU5k3b95rdnpstdVW+b//+7/cc889+etf/5oPfOADfdoZsjKTJ0/OTjvtlGOPPTZ33XVXbrvtthx33HHZf//9s+eee+aVV17JKaeckuuvvz5PP/10br755tx+++3ZbrvtkiSf+MQn8rvf/S5PPvlk7rrrrvzpT3/qfm1NEZRUsSXtHbnpsXm55fF5lS4FAAAAAIA+8KlPfSr19fXZfvvts/7667/mvJFvfOMbGTlyZPbdd98ceuihmTJlSnbfffc1Wl+hUMivf/3rjBw5Mm9729syefLkbL755vnZz36WJKmvr88LL7yQ4447LltvvXXe97735eCDD87nP//5JElHR0dOPvnkbLfddjnooIOy9dZb57/+67/WbM2lUqm0Rr9hLWhpacnw4cOzYMGCDBs2rNLl9BvPzH8lb/nKH9PUUJdHvnRwpcsBAAAAAOg3lixZkieffDKbbbZZmpubK10Oq2lVv8fe5AY6SqpYXaF8v+5HYQAAAAAAsGYISqpYXaGclBQlJQAAAAAAsFKCkirW2VAiKAEAAAAAgFUQlFSxQmdHiZgEAAAAAABWTlBSxZafUVLSVQIAAAAAACsQlFSxrhkliYHuAAAAAACwMoKSKrZ8UGJOCQAAAAAArEhQUsUKy/12i3ISAAAAAABYgaCkiukoAQAAAACA1yYoqWJ1y3ISM0oAAAAAAEiSTJgwIRdccMEqXz/hhBNy+OGHr7V6Kk1QUsUK0VECAAAAAACvRVBSxQrLd5RUrgwAAAAAAOi3BCVVzIwSAAAAAIDqcckll2TDDTdMsVjscfywww7Lhz/84STJE088kcMOOyxjxozJkCFDstdee+UPf/jDm/re1tbWfPzjH88GG2yQ5ubm7Lfffrn99tu7X3/ppZdy7LHHZv3118/AgQOz1VZb5b//+7+TJG1tbTnllFMybty4NDc3Z9NNN83UqVPfVD19raHSBbDm9JhRUlz1OgAAAAAAUh723L547X/vgEE9twhahaOOOiqnnnpq/vSnP+XAAw9Mkrz44ou57rrrcs011yRJXn755bzrXe/Kl7/85TQ1NeXHP/5xDj300DzyyCPZZJNNVqu8T3/60/nlL3+ZH/3oR9l0001z3nnnZcqUKXn88cczatSofO5zn8uDDz6Ya6+9NqNHj87jjz+eV155JUnyrW99K7/5zW/y85//PJtssklmzZqVWbNmrVYda4qgpIrpKAEAAAAA6IX2xcl/brj2v/esZ5PGwa+7bOTIkTn44INz2WWXdQclv/jFLzJ69OgccMABSZJddtklu+yyS/d7vvjFL+ZXv/pVfvOb3+SUU07pdWmLFi3Kd77znVx66aU5+OCDkyTf+973Mm3atPzgBz/IGWeckZkzZ2a33XbLnnvumaQ8LL7LzJkzs9VWW2W//fZLoVDIpptu2usa1jRbb1Wx5QNIQQkAAAAAwLrv2GOPzS9/+cu0trYmSX7605/m/e9/f+rqyn/d//LLL+dTn/pUtttuu4wYMSJDhgzJQw89lJkzZ67W9z3xxBNpb2/PW97ylu5jAwYMyN57752HHnooSfKxj30sl19+eXbdddd8+tOfzi233NK99oQTTsg999yTbbbZJh//+Mfz+9//fnVPfY3RUVLFCoVCCoVyt1hRTgIAAAAA8NoGDCp3d1Tie9+gQw89NKVSKVdffXX22muv3Hjjjfl//+//db/+qU99KtOmTcvXv/71bLnllhk4cGDe+973pq2tbU1UniQ5+OCD8/TTT+eaa67JtGnTcuCBB+bkk0/O17/+9ey+++558sknc+211+YPf/hD3ve+92Xy5Mn5xS9+scbq6S1BSZUrJCklKekoAQAAAAB4bYXCG9oCq5Kam5tz5JFH5qc//Wkef/zxbLPNNtl99927X7/55ptzwgkn5IgjjkhS7jB56qmnVvv7tthiizQ2Nubmm2/u3jarvb09t99+ez7xiU90r1t//fVz/PHH5/jjj89b3/rWnHHGGfn617+eJBk2bFiOPvroHH300Xnve9+bgw46KC+++GJGjRq12nX1JUFJlasrFFIslSImAQAAAACoDscee2z+8R//MQ888EA++MEP9nhtq622yv/93//l0EMPTaFQyOc+97kUi8XV/q7BgwfnYx/7WM4444yMGjUqm2yySc4777wsXrw4J510UpLk7LPPzh577JEddtghra2tueqqq7LddtslSb7xjW9k3Lhx2W233VJXV5crrrgiY8eOzYgRI1a7pr4mKKly5YHuJTNKAAAAAACqxDve8Y6MGjUqjzzySD7wgQ/0eO0b3/hGPvzhD2fffffN6NGj8+///u9paWl5U9/3la98JcViMR/60IeycOHC7Lnnnvnd736XkSNHJkkaGxtz5pln5qmnnsrAgQPz1re+NZdffnmSZOjQoTnvvPPy2GOPpb6+PnvttVeuueaa7pkq/UGhVAV7MrW0tGT48OFZsGBBhg0bVuly+pVtPnttWpcWc/Nn3pGNRgysdDkAAAAAAP3CkiVL8uSTT2azzTZLc3NzpcthNa3q99ib3KD/RDasEeWOkqRomjsAAAAAAKxAUFLl6so5Sdb9viEAAAAAAOh7gpIq191RIikBAAAAAIAVCEqqXGdOIigBAAAAAICVEJRUuUJ3R0mFCwEAAAAA6IdK/iPzdVpf/P4EJVWua0ZJ4h92AAAAAIAuAwYMSJIsXry4wpXwZnT9/rp+n6ujoa+KoX+q01ECAAAAALCC+vr6jBgxInPnzk2SDBo0qHuHHvq/UqmUxYsXZ+7cuRkxYkTq6+tX+7MEJVWuYJg7AAAAAMBKjR07Nkm6wxLWPSNGjOj+Pa4uQUmV69p6q1isbB0AAAAAAP1NoVDIuHHjssEGG6S9vb3S5dBLAwYMeFOdJF0EJVWuTkcJAAAAAMBrqq+v75O/cGfdZJh7levqKJGTAAAAAADAigQlVc6MEgAAAAAAWDVBSZUrdHWUVLYMAAAAAADolwQlVc6MEgAAAAAAWDVBSZVbNqNEUAIAAAAAAK8mKKlyyzpKKlwIAAAAAAD0Q4KSKtc1o6QoKQEAAAAAgBUISqqcjhIAAAAAAFg1QUmV6wpKzCgBAAAAAIAVCUqqXPfWW3ISAAAAAABYgaCkyhW6OkoiKQEAAAAAgFcTlFS5Oh0lAAAAAACwSoKSKrdsmLukBAAAAAAAXk1QUuW6OkoMcwcAAAAAgBUJSqpc14ySYrHChQAAAAAAQD8kKKlyy2aU6CgBAAAAAIBXE5RUue6OEjkJAAAAAACsQFBS5bo6ShJJCQAAAAAAvJqgpMrpKAEAAAAAgFUTlFQ5M0oAAAAAAGDVBCVVrk5HCQAAAAAArJKgpMp1BSUlHSUAAAAAALACQUmVK9h6CwAAAAAAVklQUuW6t94qVrgQAAAAAADohwQlVU5HCQAAAAAArJqgpMp1zyipcB0AAAAAANAfCUqqXF1nR4lh7gAAAAAAsCJBSZUrdM0okZMAAAAAAMAKBCVVrs6MEgAAAAAAWCVBSZWr01ECAAAAAACrJCipct3D3HWUAAAAAADACgQl1a5r6y0tJQAAAAAAsAJBSZWz9RYAAAAAAKyaoKTKdQ1zl5MAAAAAAMCKBCVVzowSAAAAAABYNUFJlSt0zSgRlAAAAAAAwAoEJVXOjBIAAAAAAFg1QUmVq9NRAgAAAAAAqyQoqXLLZpRUuBAAAAAAAOiHBCVVrntGib23AAAAAABgBasVlFx00UWZMGFCmpubM3HixNx2222vuf6KK67Itttum+bm5uy000655pprerxeKBRWevva1762OuWxnEJXR0mF6wAAAAAAgP6o10HJz372s5x++uk555xzctddd2WXXXbJlClTMnfu3JWuv+WWW3LMMcfkpJNOyt13353DDz88hx9+eO6///7uNc8991yP2w9/+MMUCoW85z3vWf0zI4kZJQAAAAAA8FoKpVLv/gZ94sSJ2WuvvXLhhRcmSYrFYsaPH59TTz01n/nMZ1ZYf/TRR2fRokW56qqruo/ts88+2XXXXXPxxRev9DsOP/zwLFy4MNOnT39DNbW0tGT48OFZsGBBhg0b1pvTqXpn//r+/HjG0/n4gVvl9H/YutLlAAAAAADAGteb3KBXHSVtbW258847M3ny5GUfUFeXyZMnZ8aMGSt9z4wZM3qsT5IpU6ascv2cOXNy9dVX56STTupNaazCsmHuOkoAAAAAAODVGnqzeN68eeno6MiYMWN6HB8zZkwefvjhlb5n9uzZK10/e/bsla7/0Y9+lKFDh+bII49cZR2tra1pbW3tft7S0vJGT6HmFGy9BQAAAAAAq7Raw9zXpB/+8Ic59thj09zcvMo1U6dOzfDhw7tv48ePX4sVrlu6OkqKchIAAAAAAFhBr4KS0aNHp76+PnPmzOlxfM6cORk7duxK3zN27Ng3vP7GG2/MI488ko985COvWceZZ56ZBQsWdN9mzZrVm9OoKZ0NJTpKAAAAAABgJXoVlDQ2NmaPPfboMWS9WCxm+vTpmTRp0krfM2nSpBWGsk+bNm2l63/wgx9kjz32yC677PKadTQ1NWXYsGE9bqxcXV3XjJIKFwIAAAAAAP1Qr2aUJMnpp5+e448/PnvuuWf23nvvXHDBBVm0aFFOPPHEJMlxxx2XjTbaKFOnTk2SnHbaadl///1z/vnn55BDDsnll1+eO+64I5dcckmPz21packVV1yR888/vw9Oiy5dM0oMcwcAAAAAgBX1Oig5+uij8/zzz+fss8/O7Nmzs+uuu+a6667rHtg+c+bM1NUta1TZd999c9lll+Wzn/1szjrrrGy11Va58sors+OOO/b43MsvvzylUinHHHPMmzwllmdGCQAAAAAArFqhVAWtBi0tLRk+fHgWLFhgG65X+drvHs5Ff3oiJ75lQs45dIdKlwMAAAAAAGtcb3KDXs0oYd3T1VGy7sdhAAAAAADQ9wQlVa7QvfWWpAQAAAAAAF5NUFLl6jqHuQtKAAAAAABgRYKSKleIYe4AAAAAALAqgpIq19VRoqEEAAAAAABWJCipcnV1XcPcJSUAAAAAAPBqgpIqVzCjBAAAAAAAVklQUuXqCmaUAAAAAADAqghKqlydjhIAAAAAAFglQUmV6+ookZMAAAAAAMCKBCU1QkcJAAAAAACsSFBS5cwoAQAAAACAVROUVLmuGSUlHSUAAAAAALACQUmVq6szowQAAAAAAFZFUFLlCt1bb0lKAAAAAADg1QQlVa5r6y1BCQAAAAAArEhQUuUMcwcAAAAAgFUTlFQ5w9wBAAAAAGDVBCVVrhAdJQAAAAAAsCqCkipX0FECAAAAAACrJCipcmaUAAAAAADAqglKqlxd52+4qKMEAAAAAABWICipcl0dJXISAAAAAABYkaCkyhW6t96SlAAAAAAAwKsJSqpcXecwd0EJAAAAAACsSFBS5QoxzB0AAAAAAFZFUFLlujpKSjpKAAAAAABgBYKSKlcwzB0AAAAAAFZJUFLlzCgBAAAAAIBVE5RUubqCGSUAAAAAALAqgpIqV9f5GzajBAAAAAAAViQoqXIFHSUAAAAAALBKgpIqt2zrLUkJAAAAAAC8mqCkynXOctdRAgAAAAAAKyEoqXJdHSVmlAAAAAAAwIoEJVWurrOlRE4CAAAAAAArEpRUuYIZJQAAAAAAsEqCkirX1VEiKAEAAAAAgBUJSqrZwtnZ4dr35GeNX7D1FgAAAAAArERDpQtgDSouzZDn786uhQE6SgAAAAAAYCV0lFSzQn2SpD4dKcpJAAAAAABgBYKSalZXbhhqKBRTLBYrXAwAAAAAAPQ/gpJqVlff/bBQEpQAAAAAAMCrCUqqWY+gpKOChQAAAAAAQP8kKKlmBR0lAAAAAADwWgQl1axzRkmiowQAAAAAAFZGUFLNbL0FAAAAAACvSVBSzZbbeqtOUAIAAAAAACsQlFSzurqUUkiSFEpLK1wMAAAAAAD0P4KSatc5p6QuhrkDAAAAAMCrCUqqXKlz+y0zSgAAAAAAYEWCkmrXOdC9rqSjBAAAAAAAXk1QUu26O0rMKAEAAAAAgFcTlFS5Ul1XUKKjBAAAAAAAXk1QUu26t94yowQAAAAAAF5NUFLtCg1JzCgBAAAAAICVEZRUu66tt2JGCQAAAAAAvJqgpNp1bb2VUoULAQAAAACA/kdQUuVKha4ZJTpKAAAAAADg1QQl1a5u2YySUklXCQAAAAAALE9QUuUKnVtv1ReKkZMAAAAAAEBPgpJq19lRUp9iipISAAAAAADoQVBS7QrlX3FDOlKUkwAAAAAAQA+CkmrXNaNERwkAAAAAAKxAUFLtOmeUNKSjwoUAAAAAAED/Iyipdt0dJSUdJQAAAAAA8CqCkmq3XEeJGSUAAAAAANCToKTKFcwoAQAAAACAVRKUVLvlOkpKxQrXAgAAAAAA/YygpMp1dZTUF3SUAAAAAADAqwlKql1d+Vdcb+stAAAAAABYgaCkynV3lKRomDsAAAAAALyKoKTaFcozSurTkVIkJQAAAAAAsDxBSbVbrqPEzlsAAAAAANCToKTa1XV1lJhRAgAAAAAAryYoqXZmlAAAAAAAwCoJSqpdofwrrk9HipISAAAAAADoQVBS7cwoAQAAAACAVRKUVLvOGSUNBTNKAAAAAADg1QQl1a5QDkrqUoyYBAAAAAAAehKUVLvOrbca0qGjBAAAAAAAXkVQUu3qyr/iuhRTEpQAAAAAAEAPgpJq191RUkxRTgIAAAAAAD0ISqrdcjNKbL0FAAAAAAA9CUqq3fIzSooVrgUAAAAAAPoZQUm1qyt3lNTrKAEAAAAAgBUISqrdckGJnAQAAAAAAHoSlFS7zhkl9YViSpGUAAAAAADA8gQl1a5zRkl9OlKUkwAAAAAAQA+CkmpnRgkAAAAAAKySoKTaLddRUhKUAAAAAABAD4KSalco/4rrU7L1FgAAAAAAvIqgpNotP6NEUgIAAAAAAD0ISqpdjxklFa4FAAAAAAD6GUFJtevsKGlIR0qRlAAAAAAAwPIEJdWuUO4oqUsxZrkDAAAAAEBPqxWUXHTRRZkwYUKam5szceLE3Hbbba+5/oorrsi2226b5ubm7LTTTrnmmmtWWPPQQw/l3e9+d4YPH57Bgwdnr732ysyZM1enPJbXufVWQ6GYoqQEAAAAAAB66HVQ8rOf/Synn356zjnnnNx1113ZZZddMmXKlMydO3el62+55ZYcc8wxOemkk3L33Xfn8MMPz+GHH57777+/e80TTzyR/fbbL9tuu22uv/763Hvvvfnc5z6X5ubm1T8zyuqWdZSYUQIAAAAAAD0VSqXetRlMnDgxe+21Vy688MIkSbFYzPjx43PqqafmM5/5zArrjz766CxatChXXXVV97F99tknu+66ay6++OIkyfvf//4MGDAgP/nJT1brJFpaWjJ8+PAsWLAgw4YNW63PqFoPX51c/oHcVdwyC469Ngdss0GlKwIAAAAAgDWqN7lBrzpK2tracuedd2by5MnLPqCuLpMnT86MGTNW+p4ZM2b0WJ8kU6ZM6V5fLBZz9dVXZ+utt86UKVOywQYbZOLEibnyyitXWUdra2taWlp63FiFHjNKtJQAAAAAAMDyehWUzJs3Lx0dHRkzZkyP42PGjMns2bNX+p7Zs2e/5vq5c+fm5Zdfzle+8pUcdNBB+f3vf58jjjgiRx55ZG644YaVfubUqVMzfPjw7tv48eN7cxq1pa4hSdKQYorFCtcCAAAAAAD9zGoNc+9Lxc6/vT/ssMPyyU9+Mrvuums+85nP5B//8R+7t+Z6tTPPPDMLFizovs2aNWttlrxuqSv/iutjmDsAAAAAALxaQ28Wjx49OvX19ZkzZ06P43PmzMnYsWNX+p6xY8e+5vrRo0enoaEh22+/fY812223XW666aaVfmZTU1Oampp6U3rt6uwoqU9HxCQAAAAAANBTrzpKGhsbs8cee2T69Ondx4rFYqZPn55Jkyat9D2TJk3qsT5Jpk2b1r2+sbExe+21Vx555JEeax599NFsuummvSmPlemcUVJvRgkAAAAAAKygVx0lSXL66afn+OOPz5577pm99947F1xwQRYtWpQTTzwxSXLcccdlo402ytSpU5Mkp512Wvbff/+cf/75OeSQQ3L55ZfnjjvuyCWXXNL9mWeccUaOPvrovO1tb8sBBxyQ6667Lr/97W9z/fXX981Z1rLujpJiinISAAAAAADooddBydFHH53nn38+Z599dmbPnp1dd9011113XffA9pkzZ6aublmjyr777pvLLrssn/3sZ3PWWWdlq622ypVXXpkdd9yxe80RRxyRiy++OFOnTs3HP/7xbLPNNvnlL3+Z/fbbrw9OscbVLesoMaMEAAAAAAB6KpSqYD+mlpaWDB8+PAsWLMiwYcMqXU7/8uzdySVvz7OlUbnjyJvz7l02rHRFAAAAAACwRvUmN+jVjBLWQWaUAAAAAADAKglKql2PGSWCEgAAAAAAWJ6gpNotP6OkWOFaAAAAAACgnxGUVLvujpKO6CcBAAAAAICeBCXVrlD+FdenmKUdWkoAAAAAAGB5gpJq19lR0pBi2gQlAAAAAADQg6Ck2nXOKKlLMW1LBSUAAAAAALA8QUm16+woGVDoSKugBAAAAAAAehCUVLtCfffD1valFSwEAAAAAAD6H0FJtatbFpS0t7dVsBAAAAAAAOh/BCXVbrmgZOlSHSUAAAAAALA8QUm165xRkiTt7e0VLAQAAAAAAPofQUm1KyzfUSIoAQAAAACA5QlKqt1yHSVLdZQAAAAAAEAPgpJqV7fsV6yjBAAAAAAAehKU1IBiodxV0mGYOwAAAAAA9CAoqQGlQvnXvFRQAgAAAAAAPQhKakCpq6OkQ1ACAAAAAADLE5TUgrqujhIzSgAAAAAAYHmCkhrQ1VFStPUWAAAAAAD0ICipBXX1SZKODh0lAAAAAACwPEFJLegKSnSUAAAAAABAD4KSWlDo6igRlAAAAAAAwPIEJbWgrnNGSUdHhQsBAAAAAID+RVBSAwqdW28VzSgBAAAAAIAeBCW1oH5ZR0mpVKpwMQAAAAAA0H8ISmpAV0dJQ6EjbR3FClcDAAAAAAD9h6CkBhQ6Z5TUpZjWpYISAAAAAADoIiipAd0dJSmmTVACAAAAAADdBCU1QEcJAAAAAACsnKCkFnR3lHToKAEAAAAAgOUISmpBj46SjgoXAwAAAAAA/YegpBYUzCgBAAAAAICVEZTUgs6tt+rNKAEAAAAAgB4EJbWgOygxowQAAAAAAJYnKKkFnTNKGgqCEgAAAAAAWJ6gpBYMGJgkaU6bYe4AAAAAALAcQUktGDA4STI4S8woAQAAAACA5QhKakFjOSgZWGgVlAAAAAAAwHIEJbWgcVCSckeJGSUAAAAAALCMoKQWNA5JkgyKjhIAAAAAAFieoKQWDCh3lAwstOooAQAAAACA5QhKakHj8sPcOypcDAAAAAAA9B+CklrQNcw9OkoAAAAAAGB5gpJa0Ln11uDCEjNKAAAAAABgOYKSWqCjBAAAAAAAVkpQUguWm1EiKAEAAAAAgGUEJbWgq6Ok0GqYOwAAAAAALEdQUgs6Z5QMSmvaOnSUAAAAAABAF0FJLWgckiQZXGhNW9vSChcDAAAAAAD9h6CkFjQOWvZ46SuVqwMAAAAAAPoZQUktaBjY/bDQtriChQAAAAAAQP8iKKkFdXVZ2lDuKil0CEoAAAAAAKCLoKRGFDu7SurbF1W4EgAAAAAA6D8EJTWi2DA4SVLfYUYJAAAAAAB0EZTUiNKA8tZbDe223gIAAAAAgC6CkhpRaix3lDQUdZQAAAAAAEAXQUmt6OwoGdCxpMKFAAAAAABA/yEoqRWdHSWNRVtvAQAAAABAF0FJjajrDkpeSalUqnA1AAAAAADQPwhKakShqRyUDCy1ZmlRUAIAAAAAAImgpGbUNQ1JkgwqtKZtabHC1QAAAAAAQP8gKKkR9Z0dJYOyJK2CEgAAAAAASCIoqRk6SgAAAAAAYEWCklrRuHxHSUeFiwEAAAAAgP5BUFIruoMSHSUAAAAAANBFUFIrBgxKkgwstJpRAgAAAAAAnQQltaIzKDHMHQAAAAAAlhGU1IqGxiRJY5baegsAAAAAADoJSmpFfVOSpDHthrkDAAAAAEAnQUmt6OooKegoAQAAAACALoKSWtGjo0RQAgAAAAAAiaCkdjSUg5ImM0oAAAAAAKCboKRWNOgoAQAAAACAVxOU1IrurbeWpq19aYWLAQAAAACA/kFQUis6h7nXFUppb2+rcDEAAAAAANA/CEpqRWdHSZJ0tLdWsBAAAAAAAOg/BCW1omFZULK0bUkFCwEAAAAAgP5DUFIr6upTTH2SpLhURwkAAAAAACSCkprSUTcgSVJq11ECAAAAAACJoKSmdNSVB7p3CEoAAAAAACCJoKSmdAUltt4CAAAAAIAyQUkNKXVvvSUoAQAAAACARFBSU4r1TUmS0tK2ClcCAAAAAAD9g6CkhhTry1tvlWy9BQAAAAAASQQlNaXU2VFS6BCUAAAAAABAIiipLTpKAAAAAACgB0FJLenuKDGjBAAAAAAAEkFJbWkod5TUCUoAAAAAACCJoKS2dHaURFACAAAAAABJBCU1pTCgHJTUF80oAQAAAACARFBSU+oaykFJXVFHCQAAAAAAJIKSmlLoDErqbb0FAAAAAABJBCU1pa5z6626UnuFKwEAAAAAgP5BUFJD6gY0J0kaSu3pKJYqXA0AAAAAAFTeagUlF110USZMmJDm5uZMnDgxt91222uuv+KKK7Ltttumubk5O+20U6655poer59wwgkpFAo9bgcddNDqlMZrqO8MShrTnralxQpXAwAAAAAAldfroORnP/tZTj/99Jxzzjm56667sssuu2TKlCmZO3fuStffcsstOeaYY3LSSSfl7rvvzuGHH57DDz88999/f491Bx10UJ577rnu2//+7/+u3hmxSvWNghIAAAAAAFher4OSb3zjG/mnf/qnnHjiidl+++1z8cUXZ9CgQfnhD3+40vXf/OY3c9BBB+WMM87Idtttly9+8YvZfffdc+GFF/ZY19TUlLFjx3bfRo4cuXpnxCrVdQ5zbyosTevSjgpXAwAAAAAAlderoKStrS133nlnJk+evOwD6uoyefLkzJgxY6XvmTFjRo/1STJlypQV1l9//fXZYIMNss022+RjH/tYXnjhhd6UxhtQ6AxKGtOeVh0lAAAAAACQht4snjdvXjo6OjJmzJgex8eMGZOHH354pe+ZPXv2StfPnj27+/lBBx2UI488MptttlmeeOKJnHXWWTn44IMzY8aM1NfXr/CZra2taW1t7X7e0tLSm9OoXQ2NSZLGLE1bh6AEAAAAAAB6FZSsKe9///u7H++0007Zeeeds8UWW+T666/PgQceuML6qVOn5vOf//zaLLE61C/XUdIuKAEAAAAAgF5tvTV69OjU19dnzpw5PY7PmTMnY8eOXel7xo4d26v1SbL55ptn9OjRefzxx1f6+plnnpkFCxZ032bNmtWb06hdXTNK0p4lZpQAAAAAAEDvgpLGxsbssccemT59evexYrGY6dOnZ9KkSSt9z6RJk3qsT5Jp06atcn2S/P3vf88LL7yQcePGrfT1pqamDBs2rMeNN6C+c+utwtK80iYoAQAAAACAXgUlSXL66afne9/7Xn70ox/loYceysc+9rEsWrQoJ554YpLkuOOOy5lnntm9/rTTTst1112X888/Pw8//HDOPffc3HHHHTnllFOSJC+//HLOOOOM/OUvf8lTTz2V6dOn57DDDsuWW26ZKVOm9NFpkqS7o6Qx7YISAAAAAADIaswoOfroo/P888/n7LPPzuzZs7Prrrvmuuuu6x7YPnPmzNTVLctf9t1331x22WX57Gc/m7POOitbbbVVrrzyyuy4445Jkvr6+tx777350Y9+lPnz52fDDTfMO9/5znzxi19MU1NTH50mSZZ1lGRpFrcLSgAAAAAAoFAqlUqVLuLNamlpyfDhw7NgwQLbcL2Wp29J/vvg/K04NrcfOi1H77VJpSsCAAAAAIA+15vcoNdbb7EO69p6y4wSAAAAAABIIiipLfXloKQp7bbeAgAAAACACEpqi2HuAAAAAADQg6Ckliw/zF1QAgAAAAAAgpKasnxHia23AAAAAABAUFJTOjtK6gultLa2VrgYAAAAAACoPEFJLensKEmS9tZXKlgIAAAAAAD0D4KSWlK/LChZ2qajBAAAAAAABCW1pL4hpc5f+dI2HSUAAAAAACAoqTHFzjklS9uWVLgSAAAAAACoPEFJjSl1BiUd7bbeAgAAAAAAQUmNKXXOKSm26ygBAAAAAABBSa0ZMLB8325GCQAAAAAACEpqTePgJEl9x+IKFwIAAAAAAJUnKKkxhcYhSZKmjsXpKJYqXA0AAAAAAFSWoKTGFJqHJkkGZ0kWty2tcDUAAAAAAFBZgpIaU9dU7igZXFiSV9o6KlwNAAAAAABUlqCkxhSayh0lQ7Ikr7QLSgAAAAAAqG2CklrTOaNkUGFJFusoAQAAAACgxglKak3j4CTJkLwiKAEAAAAAoOYJSmpN14ySLMkSW28BAAAAAFDjBCW1prE8o8TWWwAAAAAAICipPd1bby3J4ralFS4GAAAAAAAqS1BSa7q23iq8YustAAAAAABqnqCk1jR2zShptfUWAAAAAAA1T1BSa7qDklcEJQAAAAAA1DxBSa3p3npria23AAAAAACoeYKSWtPdUbJERwkAAAAAADVPUFJrOoOSgYW2LGltrXAxAAAAAABQWYKSWtO59VaSFFtfrmAhAAAAAABQeYKSWtPQlI5CQxJBCQAAAAAACEpqUEfDoPIDQQkAAAAAADVOUFKDOgaUt98qtAlKAAAAAACobYKSGlQcMDhJUmhfVOFKAAAAAACgsgQlNajUGZTUC0oAAAAAAKhxgpIaVGosb73VICgBAAAAAKDGCUpqUKFpaJKkoWNxhSsBAAAAAIDKEpTUoLrm8tZbAwQlAAAAAADUOEFJDarr7ChpLi3O0o5ihasBAAAAAIDKEZTUoIaBw5Ikg7Mkr7R3VLgaAAAAAACoHEFJDapvLg9zH5wleaVNUAIAAAAAQO0SlNSgQmN5RsmgQmsWC0oAAAAAAKhhgpJaNGBQkmRQWm29BQAAAABATROU1KLOjpLm6CgBAAAAAKC2CUpq0YCBScpbb5lRAgAAAABALROU1KLltt5a3La0wsUAAAAAAEDlCEpqUefWWwPNKAEAAAAAoMYJSmpRZ0fJQFtvAQAAAABQ4wQltagrKDHMHQAAAACAGicoqUWNy2aUvGJGCQAAAAAANUxQUos6O0oaCsW0tS6pcDEAAAAAAFA5gpJa1DnMPUnalyyqYCEAAAAAAFBZgpJaVD8gHYX6JElHq6AEAAAAAIDaJSipUUvrByZJSq0vV7gSAAAAAACoHEFJjeqoL88p6WhbXOFKAAAAAACgcgQlNaqjodxRknZBCQAAAAAAtUtQUqNKXUGJjhIAAAAAAGqYoKRGlQaUt94q6CgBAAAAAKCGCUpqVVdQsvSVChcCAAAAAACVIyipVY3loKR+qY4SAAAAAABql6CkRtU1Dk6S1HfoKAEAAAAAoHYJSmpUXVM5KBkgKAEAAAAAoIYJSmpUfWdQ0lhqTXtHscLVAAAAAABAZQhKalRjczkoGZjWLFyytMLVAAAAAABAZQhKalTX1luD0pqWV9orXA0AAAAAAFSGoKRWdQ5zH1hoTcsSQQkAAAAAALVJUFKrBgxKUt56q+UVW28BAAAAAFCbBCW1qjMoGRQdJQAAAAAA1C5BSa1q7AxKCmaUAAAAAABQuwQltaqzo6RZRwkAAAAAADVMUFKrOoe5DzKjBAAAAACAGiYoqVUDBibp3HpLRwkAAAAAADVKUFKrOrfeGhgzSgAAAAAAqF2CklrVufWWoAQAAAAAgFomKKlVTUOTJPWFUpYuXlDhYgAAAAAAoDIEJbWqcXDam0YlSQa+8kyFiwEAAAAAgMoQlNSw9qEbJ0mGLXmuwpUAAAAAAEBlCEpq2YhNkiQbtj2V/OjdyS0XVrYeAAAAAABYywQlNaxu5KZJkqMKf0ievCG55VsVrggAAAAAANYuQUkNaxw9IUmycWFe+cDLc5LWhZUrCAAAAAAA1jJBSQ3r6ijp4cW/LXv83L3JLz+SvPT02isKAAAAAADWIkFJLeucUdLDC08se/zHLyX3XZFM//zaqwkAAAAAANYiQUktGz5+xWNdQUn7K8mTfy4/fvA3yctz115dAAAAAACwlghKalnTkLTUDe957MXOoOSpm5Olr5QfF9uTu368dmsDAAAAAIC1QFBS415oGJskaa8fWD7w1E3J//1L8osPl58PHVe+v+17SevLFagQAAAAAADWHEFJjVswqLz91mOj3t55YFZy7+VJ64Ly8ylfTkZsmrw8O7nx65UpEgAAAAAA1hBBSY17dNtTcsHSI/M/Qz/c84WGgcnQDZOt3pkcNLV87JYLkxefXPtFAgAAAADAGiIoqXHrbbp9Llj63tz90sBlBzd/e/KpR5OP3Zw0DU22eVcy4a3lWSX3/aJitQIAAAAAQF8TlNS4zdcfkiR5ct7LKf7DF8uByJHfS5qHJYNGlRcVCslOR5UfP/zbClUKAAAAAAB9T1BS48aPHJgB9YUsaS/m2e0/kpxwVTJkgxUXbvOupFCXPPfXZP7M3n3J0rbkzkuT2ff3Sc0AAAAAANBXBCU1rqG+LpuuNzhJ8rfnF6164ZD1k/H7lB8/fPUb/4JF85JL9k9+e1ry65N7vtbybDLjv5IlLb2sGgAAAAAA+oaghGw+uhyUPPH8y6+9cLtDy/fXT01m3vrGPvzaf0/mPlh+/Nw9Sam07LU/fTn53ZnlbhMAAAAAAKgAQQndc0pes6MkSfY4Phk/MVmyIPnJ4a8flrQtTh65puexRfOWPX72r+X72ff1rmAAAAAAAOgjghKyxfqdW2/Ne52OksbByYd+lWx+QNK+OLnsqGTuQ8nTM5JrP5O0vSpoeeKP5XXDN0mGbVQ+9tJT5fuO9mTeI+XHzz/cdycDAAAAAAC9ICghW40ZmiR54NmWFIul117cODh5/0/L80qWLCjPHfn5h5Jbv5Pc/v3ymqVtyX2/SO76Ufn5docmIyeUH89/unz/4t+Sjrby43mPJsWOvj0pAAAAAAB4AwQlZIcNh2XggPrMX9yex+a+TldJUg5LjvrvZMCg5Jk7k0XPl4/f/3/l+z9+MfnlScljvy8/3+7QZMSm5ccvPZksfjGZ88Cyz1u6ZFmAAgAAAAAAa9FqBSUXXXRRJkyYkObm5kycODG33Xbba66/4oorsu2226a5uTk77bRTrrnmmlWu/ehHP5pCoZALLrhgdUpjNQyor8sem45Mktz65Atv7E3DNkz2+2TPY8/dkzx7T3LHf5ef1zUkY3ZKxu+djOwMSm69JDlvs+QXJ/Z87/OPrHb9AAAAAACwunodlPzsZz/L6aefnnPOOSd33XVXdtlll0yZMiVz585d6fpbbrklxxxzTE466aTcfffdOfzww3P44Yfn/vvvX2Htr371q/zlL3/Jhhtu2Psz4U3Ze7NRSZJb//biG3/TpFOSrQ9O9v7nZPO3l49dcXzStjBZf9vkzGeSf/lzUle/bOutRa/6c1Lo/CM496E3VT8AAAAAAKyOXgcl3/jGN/JP//RPOfHEE7P99tvn4osvzqBBg/LDH/5wpeu/+c1v5qCDDsoZZ5yR7bbbLl/84hez++6758ILL+yx7plnnsmpp56an/70pxkwYMDqnQ2rbWJXUPLkCymVXmdOSZfGQckHLk/e9bVkx/eUj3UNa590SjKgOanr/CPWtfXWq014a/leRwkAAAAAABXQq6Ckra0td955ZyZPnrzsA+rqMnny5MyYMWOl75kxY0aP9UkyZcqUHuuLxWI+9KEP5YwzzsgOO+zwunW0trampaWlx403Z5fxI9LYUJd5L7fliecX9f4Ddn5/8rZPlztMdvtQsvPRPV/v6ihJksYhyx7veGT5fvZ9yRsNaAAAAAAAoI809GbxvHnz0tHRkTFjxvQ4PmbMmDz88MMrfc/s2bNXun727Nndz7/61a+moaEhH//4x99QHVOnTs3nP//53pTO62geUJ9dNx6R2556MXc9/VK23GDI679peQ2NyTv+Y9WvD1nuz8AW70i2PigpFJItDkzqz0jmPpA8Ni3Z+p2rdwIAAAAAALAaVmuYe1+68847881vfjOXXnppCoXCG3rPmWeemQULFnTfZs2atYarrA17TCgPdL/z6Zf6/sPr6pINdy8/3udfk92OTXb9QDJ0TDLxX8rHp30u6Vja998NAAAAAACr0KugZPTo0amvr8+cOXN6HJ8zZ07Gjh270veMHTv2NdffeOONmTt3bjbZZJM0NDSkoaEhTz/9dP7t3/4tEyZMWOlnNjU1ZdiwYT1uvHl7bNIZlMxcA0FJkhxzefIvNyabTup5/K2fSgaOTJ5/OHny+jXz3QAAAAAAsBK9CkoaGxuzxx57ZPr06d3HisVipk+fnkmTJq30PZMmTeqxPkmmTZvWvf5DH/pQ7r333txzzz3dtw033DBnnHFGfve73/X2fHgTdt+0HJQ8PvflzF/c1vdfMHRMMm7nFY8PHJFsfkD58ZwH+v57AQAAAABgFXo1oyRJTj/99Bx//PHZc889s/fee+eCCy7IokWLcuKJJyZJjjvuuGy00UaZOnVqkuS0007L/vvvn/PPPz+HHHJILr/88txxxx255JJLkiTrrbde1ltvvR7fMWDAgIwdOzbbbLPNmz0/emHU4MZsPnpw/jZvUe6eOT8HbLvB2vvy9bct3z//yNr7TgAAAAAAal6vg5Kjjz46zz//fM4+++zMnj07u+66a6677rruge0zZ85MXd2yRpV99903l112WT772c/mrLPOylZbbZUrr7wyO+64Y9+dBX1m901H5m/zFuXOp19au0HJBp1BydyH1t53AgAAAABQ8wqlUqlU6SLerJaWlgwfPjwLFiwwr+RNuuKOWTnjF/dm0/UG5Y//9vbU1xXWzhc//2hy0V7JgMHJmX8vD38HAAAAAIDV0JvcwN9G08MhO4/LiEED8vQLizPtwdlr74tHbZ7UDUjaFyULZq297wUAAAAAoKYJSuhhUGNDPjhx0yTJd//8t7X3xfUNyeityo+v/XRy8zeTdb/ZCQAAAACAfk5QwgqO23fTNNbX5e6Z8/PXWfPX3hd3DXR/9Lpk2tnJUzeuve8GAAAAAKAmCUpYwQZDm3PIzuOSJD/5y9Nr74tHb93z+YyL1t53AwAAAABQkwQlrNQH9ylvv/Xbvz6b+Yvb1s6XbndoMnBksssx5eePXpfMeXDtfDcAAAAAADVJUMJK7b7JiGw/blhalxZz+e1rabj62B2TTz+ZHHFxsvVB5WPfPzC5/ftr5/sBAAAAAKg5ghJWqlAo5MS3TEiS/OCmJ7OkvWNtfXH5/l1fS8ZPTNoXJ9d8OnlxLQ6WBwAAAACgZghKWKXDd9soG40YmOcXtuaKO/++dr98xCbJh3+XbHFgUupIbjx/7X4/AAAAAAA1QVDCKg2or8s/v23zJMm3pj+WuQuXrN0CCoXk7WeWH//18uSltThYflUW/D357WnJ3IcqXQkAAAAAAH1AUMJrOnqv8dlygyF5fmFrTrns7rR3FNduAeP3Sia8NSkuTR76zRt/31M3JU/f0vf13PWT5M5Lk1u+3fefDQAAAADAWico4TU1D6jPxR/cI0OaGnLbky/moj89vvaL6Brs/rcb3tj6xS8mPzmifHtlft/WMr+zq+WFCvwcAAAAAADoc4ISXteWGwzJl4/YMUly4R8fz31/X7B2C9j87eX7p29Olra9/vpZtyYdbcnSJcnMv5SPzX0oufeKN1/L/Fnle8PlAQAAAACqgqCEN+Tdu2yYQ3Yal6XFUs7+zf0plUpr78s32D4ZNDppX5w8c8frr59167LHT99Uvv/58cn/fSR56uY3V8uCzqBk0fNJ68Jkbf4cAAAAAADoc4IS3pBCoZBz3r19mhrqcvfM+ZnxxAtr78vr6pLN3lZ+fOP55RkhrQtXvX7WbcseP3VzMn9mMu+R8vPn7ln9OoodScszy55fd2bylU2TP/1n0r6WB90DAAAAANAnBCW8YRsMbc7Re41Pkly4tmeVbHFA+f7xPyS/PS35xvblge2v1tGePHPnsufP/TV5+Oplz+c+tPo1vDynPFS+y90/SVoXJDd8NbnqE6v/uQAAAAAAVIyghF75l/23SENdIbc88UJ+98DstffFOx+d7Hd6sscJyXpbJq0t5U6OV5t9b3k2ycCRyYhNklJH8scvL3v99YKSjvak/ZWVv7bg76t+39+uf70zAAAAAACgHxKU0CsbjRiYf3rb5kmS//jVfXnh5da188UNTcnkc5JDv5kc9+skhfJw9/kze6575Nry/cZ7J1tOLj9uW26brucfWflckVIpueFryfnbJOdvW+5cebVXf1eSDN6gfP/ynPLWXAAAAAAArFMEJfTaJyZvlW3GDM28l9ty2uX3pL2juHYLGL5xMmG/8uP7rlh2/KWnklu+XX68y/uTd3wuGTpu2et1A8qhyco6Q+7/ZfKnLyWLX0iWzE9+etSKYUnX+xqalx3b6b1JoS4pFZNF897smQEAAAAAsJYJSui1pob6XPD+XTOosT43PT4v5/zmgZRW1qWxJu18dPl+xkXJ9C8kSxYkV/9bedutzd6W7HBEMmhU8pHpyeZvTyZ/Phm9Vfk9K9t+68k/l+93+2Cy3aHl4OOvl/dc0xWUbLLPsmNbvCMZMqb8eOFzfXZ6AAAAAACsHYISVst244bl/x29awqF5LJbZ+Y/rrw/xeJaDEu2f3cyeP1yB8iN5yff3LXcAVLflLzr60mhUF43fKPyVl37fSJZf9vysec7g5KlrclVpye3fz+ZdWv52DaHlOegJMmzd5e35GpfUn6+YFb5fvMDyl0ljUOSTSYlQ8eWjy9cizNbAAAAAADoE4ISVtuUHcbmq0fu3B2WnD/tkbX35c3Dk1PvTI78XjJoveSVF8vH3/3tZP1tVv6eDbYv38/sDEVuvTi54wfJtf+ePP9w+dj4vZNxu5Ufv/B4ctUnkq+MT2bdvqyjZMyO5fDl+N8mTUOSIV1BiY4SAAAAAIB1jaCEN+V9e43Pee/ZOUly0Z+eyG/++uza+/Lm4cnO70s+/Pvy4PaDvpLscvSq1289JUkheeTq5MFfJ38+v3y8uLR8P2qLZPDoZPB6yYhNysfuvDTpaEumf37Zll2jtyxvv7XR7uXnOkoAAAAAANZZghLetKP2HJ+T9tssSXLa5Xfne3/+29otYPSWyQd/mezzsddeN27nZI/jy49/flzSuiAZMHjZ68vPHtlwt57vferGpNRR3mpr5ISer3UNjNdRAgAAAACwzhGU0CfOPHjbfGDiJimVki9f81Au+tPjlS5p5Q48Jxm8QfnxyM2S465MBo4qPx+/97J1rw5KunTNL1mejhIAAAAAgHVWQ6ULoDo01Nfly4fvmI1HDsx51z2Sr/3ukTTUFfIv+29R6dJ6GjQq+dcZyZIFyajNy0PfD/1m8vBVyY7vXbZu+aBku0OTh36bNI9Itj9sxc/sTUfJ0tak2JE0DnpTpwEAAAAAQN8QlNBnCoVC/vXtW6a1vZhvTn8sU699OHMXtubTB22Tpob6Spe3zODR5VuX7d9dvi1v473KW2wN27g8IL5Ql2z7j8mAgSt+3hvtKOloT773jmTRvORjN/esAQAAAACAihCU0Oc++Q9bZ2Bjfb5y7cP5wU1P5s+PPp+vvnfn7L7JyEqX9sY1Dk4+fk9SKiV1dcn7frzqtV0dJYueTzqWJvWr+Mfqvl8kc+4vP7714uQdn+3TkgEAAAAA6D0zSlgjPrr/Frn4g7tn9JDGPDb35bznO7fkq9c9nI5iqdKlvXGFQjkkeT2D1kvqGpKUkkVzkxefTB6+Jpl9XzloScoBys0XLHvPrZeUt/8CAAAAAKCidJSwxhy047hM3Gy9fOGqB/Oru5/Jd65/Ivc/syAXHrN7hg8aUOny+k5dXTJkTNLyTDLtnOSBXyXF9vJrWx+U1Dcmj/4u6WhNmoYlQzZIXng8ueeyZJ+PVbZ2AAAAAIAap6OENWrk4Mb8v6N3zbeP2S3NA+py42Pz8u6Lbsr0h+akvaNY6fL6zq4fKN/f9/NySDJqi86A5Lrkod+UQ5KGgck7PpfsfHR57ez7K1cvAAAAAABJdJSwlhy6y4bZfP3B+ecf35mnX1ick350RzZdb1C+d9ye2XrM0EqX9+Yd8B/JoNHJn89Ldjkm+YcvJnMfTH53VnnY+6RTkjE7JHX1yV9/Vn7P/KcrWzMAAAAAACmUSqV1aGjEyrW0tGT48OFZsGBBhg0bVulyeA0vLmrLt//4WH59z7N5cVFbhjY15Lsf2iP7bjm60qX1jVKpPNvktcz8S/LDKcmITZJP3Ld26gIAAAAAqCG9yQ1svcVaNWpwY845dIf84fT9s/dmo7KwdWlOuPT2/O6B2ZUurW+8XkiSJCM2Ld8veKY85B0AAAAAgIoRlFARowY35scf3juTtxuTtqXF/MtP7swnLr87LUvaK13amjdkTFLflJQ6ygPgAQAAAACoGEEJFdM8oD7f+eDuOWm/zVJXSK6859kccdHNuf2pF1MFO8KtWl1dMmJ8+XFfzSnpaE+WtvXNZwEAAAAA1BDD3KmoAfV1+dw/bp9Ddh6Xf/2fu/LE84ty1MUzMnpIYzZff0g+OXnrTNpivUqX2fdGbJq88Hjy0tPJZqvx/ldeSp65M5l5a/LQb5LnH05SSHY4PDnw7KRxaDJoVFKoS9oXJ0sWJK/ML993tCab7pfU+8cfAAAAAMDflNIv7L7JyPzm1Lfka9c9kqvvey7zXm7LvJdfzId+cGtOPmDLHLrLuGy5wdBKl9l3RmxSvn+jHSXzHkvu+0Xy0lPJs3cl8x5dyaJS8sCvyrckqWtIUkiKK9nObIcjk/f+8I3NVAEAAAAAqGKFUhXscdSb6fX0f6+0deSJ51/OJX/+W37z12e7jx83adOcefB2GdhYX8Hq+shN/y/5w7nJzkcnR16y4uulUvlWV5e8+GTy3f2T1gU914zcLNl4r2SLA5ItJycLZiVX/1vy7N0rfl6hPhk4ImkensyfmRSXJlsflAwcmSxtTTrayrelreVtvDpak4GjkilfTtbfZk38BAAAAAAA1pje5AaCEvqtUqmUX9z59/z23ufy50efT5LsNWFkfnLSxDQPWMfDkvt/mfziw8mYHZO3np4seCZpebYcVhQKyX1XJCkkm++fzHmgvE3XmB2T7Q9Pxu2cbLRHMnj0qj+/2JEsnF1+3Dw8aRy8rHvkzkuT3572xuocuVnyT38sByor6z4plcqhS7GjfF/qKIcyTUN68cMAAAAAAOhbghKqzp8ffT4nX3ZXFi5Zmrdvs34+OHHTvH2b9dNQX1fp0lbP3+9Mvv+ON75+0HrJv9yYDN+ob77/0d8nz/01qR+QNDQl9Y3L7usby9t2/e7McvdJl7qG5bbz6gxFSsWVf/6Etyab7lteu1Kvc9mpH5BssEMyZEz5eaHzfwqF175PXmdNerH2jXzuSo6/1uf2Z/16G7b+XFv87N4MP7vV159/dv25NgAAAGqGoISqdMsT83L8D29Le0f5j+wB26yfiz+0R5oa1sHukqVtyU/fU545MmzjZNiG5RCkbkCyZH6y5T8kzcOSv99RDiN2ODwZOWHt1vjcX5PLjk4WPrd2vxcAAAAA6L3PzCzvLkMSQUmly2ENuv2pF/PLO/+eK+95Jkvai3nn9mNy8Qf3SF2d/3p1jehoT1oXdm6v1XkrFcuBTl19Z5dJfXm7ra7HL89N7r182dZfq/Qav7O2l5PZ9yVLWjoPdM5sed37Va191fHkDX7eq9YCAAAAQH8lKOlBUELVu/nxefnwpbendWkx5xy6fU58y2av+54n5y3Kf/zqvmw4YmCO3mt89powai1UStUpvcFQZWWv9Vf9+v8G+nNt8bN7M/r1z66f69c/u/5cW/r5zw4AAOBNGrReUreOjipYAwQl1ISfzHgqn/v1A2lsqMv/fWzf7LjRqtPSUqmUD3zv1sz42wtJkrpCctWpb832G/rzAgAAAABQbXqTG4iXWGd9cJ9Nc8A266dtaTHH/fC2/OQvT+eOp15c6drfPTA7M/72Qhob6rLXhJEplpIvXf1gqiAnBAAAAADgTRCUsM4qFAr51jG7ZeeNh+fFRW353JX3570Xz8hPb3068xe3ZcEr7UmSPz08N2dccW+S5F/etnm+8b5d09hQl1ueeCE/u31WJU8BAAAAAIAKs/UW67z5i9ty/u8fzSNzFua2J19M11z3ukIhm40enMfmvpwk2XuzUfnRiXtnYGN9vjHt0Xxr+mNJknMP3T4nvIEZJ0ny3IJX8pVrH85H998i243zZw0AAAAAoD+y9RY1ZcSgxnzx8B3zs3/eJ4ftumGKpaRYSpYWS3ls7stpqCvkuEmb5n9OmpiBjfVJkk8cuFU+sl85HPnyNQ/l7y8tfkPf9YMbn8yv73k2U699eI2dDwAAAAAAa09DpQuAvlIoFPL1o3bJITuNy3bjhuWV9o48+GxL9t1yvWwwtLnH2rq6Qj77j9vnodktufnxF3LhHx/PV96zc+YuXJLFrR2ZMHrwSr/jr3+fnyS55fF5mb+4LSMGNa7p0wIAAAAAYA0SlFBVBtTX5Z07jO1+vvWYoa+5/vR/2CY3P35Lrrjz72lqqMvP7/h7Wpd25Kx3bZfDd9soV979TB6dszBnvWu7DGlqyH3PLEhS7lb5r+ufyEYjBub9e49PU0P9Gj0vAAAAAADWDEEJNW2PTUfmXTuNzTX3zc6PZjzdffxLVz+UL139UPfzkYMbc9guG2VJe7H72CV//luS5IWXW3P6O7dZe0UDAAAAANBnzCih5l1w9G75+lG75K1bjc5nD9kun/vH7TOsuZwhrje4vLXWz2+flVuffCFJMn7UwB7v/9/bZ6VtaTEAAAAAAKx7CqVSqVTpIt6s3kyvhzdqSXtHBtTX5W3n/SnPzH+l+/jJB2yRscMHplQq5cI/Pp65C1tz4Qd2yz/uvGEFqwUAAAAAoEtvcgMdJbAKzQPqU19XyLH7bNLj+C4bj8iH9tk0x02akPfvXX7tsltnVqJEAAAAAADeJEEJvI4P7bNp3r3Lhll/aFO2WH9w9tlive7X3rP7RkmS2558MYvbllaqRAAAAAAAVpNh7vA6hjYPyLeO2W2lr20yalA2GjEwz8x/Jbc/9VL233r9tVwdAAAAAABvho4SeBMKhUL27ewwueXxeRWuBgAAAACA3hKUwJv0li1HJ0lueeKFClcCAAAAAEBvCUrgTZrU2VFy/7MLsmBxe4WrAQAAAACgNwQl8CaNGdacrccMSamUXHXfs5UuBwAAAACAXhCUQB84eq9NkiSX3vxUSqVShasBAAAAAOCNEpRAHzhqz40zuLE+j819OTc/blYJAAAAAMC6QlACfWBY84C8d4+NkyRfue6htC7tqHBFAAAAAAC8EYIS6CMfe/uWGTloQO5/piWf/+2DWdIuLAEAAAAA6O8EJdBHxg5vztfeu0uS5LJbZ2a/r/4xv39gdoWrAgAAAADgtQhKoA9N3n5Mvn7ULtloxMDMe7kt//yTO/ON3z+StqXFSpcGAAAAAMBKCEqgj713j41z/Rlvz/GTNk2SfOuPj+fQb9+Uu2e+VOHKAAAAAAB4NUEJrAED6uvy+cN2zLeP2S3rDW7MI3MW5sjv3JJvT38spVKp0uUBAAAAANBJUAJr0KG7bJg/nL5/jtx9o5RKyfnTHs0Xrnow7R224gIAAAAA6A8EJbCGjRzcmG+8b9ece+j2SZL/vvmpHHbhzfnen/+Wp19YVOHqAAAAAABqm6AE1pIT3rJZLvrA7hkxaEAefK4lX77mobzrmzfmLrNLAAAAAAAqRlACa9EhO4/LtE/unzOmbJOdNhqeRW0dOf4Ht+Wvs+ZXujQAAAAAgJokKIG1bP2hTTn5gC3zs3/ZJ3tPGJWFrUvzoR/cmnv/Pr/SpQEAAAAA1BxBCVTIoMaG/PDEvbLHpiPTsmRpjrp4Ri67dWZKpVKlSwMAAAAAqBmCEqigIU0NufTEvXLANuundWkxZ/3qvpz6v3fnwWdb0lEUmAAAAAAArGmFUhX85+stLS0ZPnx4FixYkGHDhlW6HOi1YrGU79/0t5x33SNZ2hmQbLreoHz58J2y31aj01EsZUl7RwY3NVS4UgAAAACA/q83uYGgBPqRu2a+lAv+8FjuevqlvNy6NEnyjzuPy71/X5A5LUvyH4dslw/ts2kKhUKFKwUAAAAA6L8EJbCOW7ikPV//3SP58V+ezqv/CT1w2w1y3nt3znpDmipTHAAAAABAPycogSpx59Mv5YI/PJrtxw3L+kObct7vHknb0mKGDxyQ4yZtmhP2nSAwAQAAAAB4FUEJVKmHnmvJJy6/J4/MWZgkaWqoy9F7jc8/vXXzjB81KB3FUgpJ6upszQUAAAAA1C5BCVSxjmIpv39gdr5zwxO59+8LkiSFQrLr+BF5Yu7LKZaSQ3cZl6P2HJ/dxo8wzwQAAAAAqDmCEqgBpVIpM554Id+54Ync+Ni8la7ZeOTA7L/1+nnb1utn3y3Wy9DmAWnvKOb7Nz6Zxoa6HLP3+AxqbFjLlQMAAAAArFmCEqgxs15cnJsen5ct1h+SjmIpV9wxK9fc/1yWtBe71zTUFbL7JiPTUSrlzqdfSpKMGDQge246KrtsPDw7bjw8W64/JBuNGGjrLgAAAABgnSYoAbKodWn+8rcX8udHn8+fH5uXJ+ct6n5tUGN91hvSmFkvvrLC+5oH1GXz0UOy4YiBWbikPVuNGZJDd94wm40enNFDmoQoAAAAAEC/JygBVjDrxcW54dHn8+BzLfnQPptmyw2G5J5Z8/PXWfNz798X5OHZLXly3qK0d6z6kjCgvpCxw5szbtjArDekMaMG97ytN7gpQ5sbsnDJ0gwfOCBbjx2Spob6tXiWAAAAAACCkkqXA+uspR3FzHrplTwx9+XMblmSQY31uf6R53P7Uy9mTsuSFFfjatE8oC6DGhsyqLE+gxsbMqipPoMa67uPDWqsT1NDfQY21mfggPo0D6jLwAH1aRpQfl4+Vp+BjXVp7nq83H1TQ50uFwAAAACgB0EJ0OeWdhQzZ2Frnpv/Sp5bsCQvLmpb6a1lSXuGNjdkTktrFrzSvlZqa2qoS1NDXRrq69JQVyjfOh/XL/e4ob7QfWxAfV35tbpXv1aXAfWvXlN+vb6uLoUkhUJSSKHzvvN5oRzWdN6t+Hrn8/KawnLHO593Ps5yr63qc7L8+7q//7W+Y1ltyz572dq+1NcfWVgDRa6JWG2N/Cz7+DMLa+LM142PXCf+HK2JP0MAAADUln02Xy8D6usqXUa/0ZvcoGEt1QSs4xrq67LRiIHZaMTAN7S+VCrlpcXtWdS6NIvbOrK4rXy/7Hn52CttHXmlvSNL2ot5pb0jre3l5+VjHXmlvbjsWFv52JL2Yto6lg2qb11aTOvS4mtUAwAAAADV7d5z3ykoWU2CEmCNKBQK3bNL1oSOYqkzSCmHJ61Li+kolrK0o5SlxWKWLv+4o5SOYintHZ1risuOL+163vlae0cpHcVi5/2y17reUyolpSTlXrxS+XlnX16p63nn66WUn5Sfl5Y7Xn5efk861yz7rFd/Tno8X/a+FY69xufk1d/Th7+LvmxM7Nu6+vCz+u6j+rSw/vvz6sNz7Mu6+umfiSpo7gUAAKAfqLNdwWoTlADrpPq6QgY3NWRwk8sYAAAAALD69OEAAAAAAAA1S1ACAAAAAADULEEJAAAAAABQswQlAAAAAABAzRKUAAAAAAAANWu1gpKLLrooEyZMSHNzcyZOnJjbbrvtNddfccUV2XbbbdPc3Jyddtop11xzTY/Xzz333Gy77bYZPHhwRo4cmcmTJ+fWW29dndIAAAAAAADesF4HJT/72c9y+umn55xzzsldd92VXXbZJVOmTMncuXNXuv6WW27JMccck5NOOil33313Dj/88Bx++OG5//77u9dsvfXWufDCC3PfffflpptuyoQJE/LOd74zzz///OqfGQAAAAAAwOsolEqlUm/eMHHixOy111658MILkyTFYjHjx4/Pqaeems985jMrrD/66KOzaNGiXHXVVd3H9tlnn+y66665+OKLV/odLS0tGT58eP7whz/kwAMPfN2autYvWLAgw4YN683pAAAAAAAAVaY3uUGvOkra2tpy5513ZvLkycs+oK4ukydPzowZM1b6nhkzZvRYnyRTpkxZ5fq2trZccsklGT58eHbZZZfelAcAAAAAANArDb1ZPG/evHR0dGTMmDE9jo8ZMyYPP/zwSt8ze/bsla6fPXt2j2NXXXVV3v/+92fx4sUZN25cpk2bltGjR6/0M1tbW9Pa2tr9vKWlpTenAQAAAAAAkGQ1h7mvCQcccEDuueee3HLLLTnooIPyvve9b5VzT6ZOnZrhw4d338aPH7+WqwUAAAAAAKpBr4KS0aNHp76+PnPmzOlxfM6cORk7duxK3zN27Ng3tH7w4MHZcssts88+++QHP/hBGhoa8oMf/GCln3nmmWdmwYIF3bdZs2b15jQAAAAAAACS9DIoaWxszB577JHp06d3HysWi5k+fXomTZq00vdMmjSpx/okmTZt2irXL/+5y2+vtbympqYMGzasxw0AAAAAAKC3ejWjJElOP/30HH/88dlzzz2z995754ILLsiiRYty4oknJkmOO+64bLTRRpk6dWqS5LTTTsv++++f888/P4ccckguv/zy3HHHHbnkkkuSJIsWLcqXv/zlvPvd7864ceMyb968XHTRRXnmmWdy1FFH9eGpAgAAAAAA9NTroOToo4/O888/n7PPPjuzZ8/Orrvumuuuu657YPvMmTNTV7esUWXffffNZZddls9+9rM566yzstVWW+XKK6/MjjvumCSpr6/Pww8/nB/96EeZN29e1ltvvey111658cYbs8MOO/TRaQIAAAAAAKyoUCqVSpUu4s1qaWnJ8OHDs2DBAttwAQAAAABAjetNbtCrGSUAAAAAAADVRFACAAAAAADULEEJAAAAAABQswQlAAAAAABAzRKUAAAAAAAANauh0gX0hVKplKQ8xR4AAAAAAKhtXXlBV37wWqoiKFm4cGGSZPz48RWuBAAAAAAA6C8WLlyY4cOHv+aaQumNxCn9XLFYzLPPPpuhQ4emUChUupx+o6WlJePHj8+sWbMybNiwSpcDVCHXGWBNc50B1jTXGWBNc50B1jTXmZUrlUpZuHBhNtxww9TVvfYUkqroKKmrq8vGG29c6TL6rWHDhvkHBFijXGeANc11BljTXGeANc11BljTXGdW9HqdJF0McwcAAAAAAGqWoAQAAAAAAKhZgpIq1tTUlHPOOSdNTU2VLgWoUq4zwJrmOgOsaa4zwJrmOgOsaa4zb15VDHMHAAAAAABYHTpKAAAAAACAmiUoAQAAAAAAapagBAAAAAAAqFmCEgAAAAAAoGYJSqrURRddlAkTJqS5uTkTJ07MbbfdVumSgHXA1KlTs9dee2Xo0KHZYIMNcvjhh+eRRx7psWbJkiU5+eSTs95662XIkCF5z3vekzlz5vRYM3PmzBxyyCEZNGhQNthgg5xxxhlZunTp2jwVYB3xla98JYVCIZ/4xCe6j7nOAH3hmWeeyQc/+MGst956GThwYHbaaafccccd3a+XSqWcffbZGTduXAYOHJjJkyfnscce6/EZL774Yo499tgMGzYsI0aMyEknnZSXX355bZ8K0A91dHTkc5/7XDbbbLMMHDgwW2yxRb74xS+mVCp1r3GdAXrjz3/+cw499NBsuOGGKRQKufLKK3u83lfXlHvvvTdvfetb09zcnPHjx+e8885b06e2ThCUVKGf/exnOf3003POOefkrrvuyi677JIpU6Zk7ty5lS4N6OduuOGGnHzyyfnLX/6SadOmpb29Pe985zuzaNGi7jWf/OQn89vf/jZXXHFFbrjhhjz77LM58sgju1/v6OjIIYcckra2ttxyyy350Y9+lEsvvTRnn312JU4J6Mduv/32fPe7383OO+/c47jrDPBmvfTSS3nLW96SAQMG5Nprr82DDz6Y888/PyNHjuxec9555+Vb3/pWLr744tx6660ZPHhwpkyZkiVLlnSvOfbYY/PAAw9k2rRpueqqq/LnP/85//zP/1yJUwL6ma9+9av5zne+kwsvvDAPPfRQvvrVr+a8887Lt7/97e41rjNAbyxatCi77LJLLrroopW+3hfXlJaWlrzzne/MpptumjvvvDNf+9rXcu655+aSSy5Z4+fX75WoOnvvvXfp5JNP7n7e0dFR2nDDDUtTp06tYFXAumju3LmlJKUbbrihVCqVSvPnzy8NGDCgdMUVV3Sveeihh0pJSjNmzCiVSqXSNddcU6qrqyvNnj27e813vvOd0rBhw0qtra1r9wSAfmvhwoWlrbbaqjRt2rTS/vvvXzrttNNKpZLrDNA3/v3f/7203377rfL1YrFYGjt2bOlrX/ta97H58+eXmpqaSv/7v/9bKpVKpQcffLCUpHT77bd3r7n22mtLhUKh9Mwzz6y54oF1wiGHHFL68Ic/3OPYkUceWTr22GNLpZLrDPDmJCn96le/6n7eV9eU//qv/yqNHDmyx783/fu//3tpm222WcNn1P/pKKkybW1tufPOOzN58uTuY3V1dZk8eXJmzJhRwcqAddGCBQuSJKNGjUqS3HnnnWlvb+9xjdl2222zySabdF9jZsyYkZ122iljxozpXjNlypS0tLTkgQceWIvVA/3ZySefnEMOOaTH9SRxnQH6xm9+85vsueeeOeqoo7LBBhtkt912y/e+973u15988snMnj27x7Vm+PDhmThxYo9rzYgRI7Lnnnt2r5k8eXLq6upy6623rr2TAfqlfffdN9OnT8+jjz6aJPnrX/+am266KQcffHAS1xmgb/XVNWXGjBl529velsbGxu41U6ZMySOPPJKXXnppLZ1N/9RQ6QLoW/PmzUtHR0ePvzhIkjFjxuThhx+uUFXAuqhYLOYTn/hE3vKWt2THHXdMksyePTuNjY0ZMWJEj7VjxozJ7Nmzu9es7BrU9RrA5Zdfnrvuuiu33377Cq+5zgB94W9/+1u+853v5PTTT89ZZ52V22+/PR//+MfT2NiY448/vvtasbJryfLXmg022KDH6w0NDRk1apRrDZDPfOYzaWlpybbbbpv6+vp0dHTky1/+co499tgkcZ0B+lRfXVNmz56dzTbbbIXP6Hpt+W1Ka42gBICVOvnkk3P//ffnpptuqnQpQBWZNWtWTjvttEybNi3Nzc2VLgeoUsViMXvuuWf+8z//M0my22675f7778/FF1+c448/vsLVAdXg5z//eX7605/msssuyw477JB77rknn/jEJ7Lhhhu6zgCsg2y9VWVGjx6d+vr6zJkzp8fxOXPmZOzYsRWqCljXnHLKKbnqqqvypz/9KRtvvHH38bFjx6atrS3z58/vsX75a8zYsWNXeg3qeg2obXfeeWfmzp2b3XffPQ0NDWloaMgNN9yQb33rW2loaMiYMWNcZ4A3bdy4cdl+++17HNtuu+0yc+bMJMuuFa/1701jx47N3Llze7y+dOnSvPjii641QM4444x85jOfyfvf//7stNNO+dCHPpRPfvKTmTp1ahLXGaBv9dU1xb9LrZqgpMo0NjZmjz32yPTp07uPFYvFTJ8+PZMmTapgZcC6oFQq5ZRTTsmvfvWr/PGPf1yhHXOPPfbIgAEDelxjHnnkkcycObP7GjNp0qTcd999Pf7Pedq0aRk2bNgKf2EB1J4DDzww9913X+65557u25577pljjz22+7HrDPBmveUtb8kjjzzS49ijjz6aTTfdNEmy2WabZezYsT2uNS0tLbn11lt7XGvmz5+fO++8s3vNH//4xxSLxUycOHEtnAXQny1evDh1dT3/Wq2+vj7FYjGJ6wzQt/rqmjJp0qT8+c9/Tnt7e/eaadOmZZtttqnpbbeSJJWeJk/fu/zyy0tNTU2lSy+9tPTggw+W/vmf/7k0YsSI0uzZsytdGtDPfexjHysNHz68dP3115eee+657tvixYu713z0ox8tbbLJJqU//vGPpTvuuKM0adKk0qRJk7pfX7p0aWnHHXcsvfOd7yzdc889peuuu660/vrrl84888xKnBKwDth///1Lp512Wvdz1xngzbrttttKDQ0NpS9/+culxx57rPTTn/60NGjQoNL//M//dK/5yle+UhoxYkTp17/+denee+8tHXbYYaXNNtus9Morr3SvOeigg0q77bZb6dZbby3ddNNNpa222qp0zDHHVOKUgH7m+OOPL2200Ualq666qvTkk0+W/u///q80evTo0qc//enuNa4zQG8sXLiwdPfdd5fuvvvuUpLSN77xjdLdd99devrpp0ulUt9cU+bPn18aM2ZM6UMf+lDp/vvvL11++eWlQYMGlb773e+u9fPtbwQlVerb3/52aZNNNik1NjaW9t5779Jf/vKXSpcErAOSrPT23//9391rXnnlldK//uu/lkaOHFkaNGhQ6Ygjjig999xzPT7nqaeeKh188MGlgQMHlkaPHl36t3/7t1J7e/taPhtgXfHqoMR1BugLv/3tb0s77rhjqampqbTtttuWLrnkkh6vF4vF0uc+97nSmDFjSk1NTaUDDzyw9Mgjj/RY88ILL5SOOeaY0pAhQ0rDhg0rnXjiiaWFCxeuzdMA+qmWlpbSaaedVtpkk01Kzc3Npc0337z0H//xH6XW1tbuNa4zQG/86U9/WunfyRx//PGlUqnvril//etfS/vtt1+pqamptNFGG5W+8pWvrK1T7NcKpVKpVJleFgAAAAAAgMoyowQAAAAAAKhZghIAAAAAAKBmCUoAAAAAAICaJSgBAAAAAABqlqAEAAAAAACoWYISAAAAAACgZglKAAAAAACAmiUoAQAAAAAAapagBAAAAAAAqFmCEgAAAAAAoGYJSgAAAAAAgJolKAEAAAAAAGrW/wfmpFftbM4rYgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "y_hat: (8, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (8, 32, 48, 6, 1)\n",
      "1\n",
      "y_hat: (16, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (16, 32, 48, 6, 1)\n",
      "2\n",
      "y_hat: (24, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (24, 32, 48, 6, 1)\n",
      "3\n",
      "y_hat: (32, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (32, 32, 48, 6, 1)\n",
      "4\n",
      "y_hat: (40, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (40, 32, 48, 6, 1)\n",
      "5\n",
      "y_hat: (48, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (48, 32, 48, 6, 1)\n",
      "6\n",
      "y_hat: (56, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (56, 32, 48, 6, 1)\n",
      "7\n",
      "y_hat: (64, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (64, 32, 48, 6, 1)\n",
      "8\n",
      "y_hat: (72, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (72, 32, 48, 6, 1)\n",
      "9\n",
      "y_hat: (80, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (80, 32, 48, 6, 1)\n",
      "10\n",
      "y_hat: (88, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (88, 32, 48, 6, 1)\n",
      "11\n",
      "y_hat: (96, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (96, 32, 48, 6, 1)\n",
      "12\n",
      "y_hat: (104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (104, 32, 48, 6, 1)\n",
      "13\n",
      "y_hat: (112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (112, 32, 48, 6, 1)\n",
      "14\n",
      "y_hat: (120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (120, 32, 48, 6, 1)\n",
      "15\n",
      "y_hat: (128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (128, 32, 48, 6, 1)\n",
      "16\n",
      "y_hat: (136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (136, 32, 48, 6, 1)\n",
      "17\n",
      "y_hat: (144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (144, 32, 48, 6, 1)\n",
      "18\n",
      "y_hat: (152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (152, 32, 48, 6, 1)\n",
      "19\n",
      "y_hat: (160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (160, 32, 48, 6, 1)\n",
      "20\n",
      "y_hat: (168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (168, 32, 48, 6, 1)\n",
      "21\n",
      "y_hat: (176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (176, 32, 48, 6, 1)\n",
      "22\n",
      "y_hat: (184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (184, 32, 48, 6, 1)\n",
      "23\n",
      "y_hat: (192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (192, 32, 48, 6, 1)\n",
      "24\n",
      "y_hat: (200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (200, 32, 48, 6, 1)\n",
      "25\n",
      "y_hat: (208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (208, 32, 48, 6, 1)\n",
      "26\n",
      "y_hat: (216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (216, 32, 48, 6, 1)\n",
      "27\n",
      "y_hat: (224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (224, 32, 48, 6, 1)\n",
      "28\n",
      "y_hat: (232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (232, 32, 48, 6, 1)\n",
      "29\n",
      "y_hat: (240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (240, 32, 48, 6, 1)\n",
      "30\n",
      "y_hat: (248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (248, 32, 48, 6, 1)\n",
      "31\n",
      "y_hat: (256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (256, 32, 48, 6, 1)\n",
      "32\n",
      "y_hat: (264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (264, 32, 48, 6, 1)\n",
      "33\n",
      "y_hat: (272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (272, 32, 48, 6, 1)\n",
      "34\n",
      "y_hat: (280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (280, 32, 48, 6, 1)\n",
      "35\n",
      "y_hat: (288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (288, 32, 48, 6, 1)\n",
      "36\n",
      "y_hat: (296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (296, 32, 48, 6, 1)\n",
      "37\n",
      "y_hat: (304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (304, 32, 48, 6, 1)\n",
      "38\n",
      "y_hat: (312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (312, 32, 48, 6, 1)\n",
      "39\n",
      "y_hat: (320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (320, 32, 48, 6, 1)\n",
      "40\n",
      "y_hat: (328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (328, 32, 48, 6, 1)\n",
      "41\n",
      "y_hat: (336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (336, 32, 48, 6, 1)\n",
      "42\n",
      "y_hat: (344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (344, 32, 48, 6, 1)\n",
      "43\n",
      "y_hat: (352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (352, 32, 48, 6, 1)\n",
      "44\n",
      "y_hat: (360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (360, 32, 48, 6, 1)\n",
      "45\n",
      "y_hat: (368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (368, 32, 48, 6, 1)\n",
      "46\n",
      "y_hat: (376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (376, 32, 48, 6, 1)\n",
      "47\n",
      "y_hat: (384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (384, 32, 48, 6, 1)\n",
      "48\n",
      "y_hat: (392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (392, 32, 48, 6, 1)\n",
      "49\n",
      "y_hat: (400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (400, 32, 48, 6, 1)\n",
      "50\n",
      "y_hat: (408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (408, 32, 48, 6, 1)\n",
      "51\n",
      "y_hat: (416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (416, 32, 48, 6, 1)\n",
      "52\n",
      "y_hat: (424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (424, 32, 48, 6, 1)\n",
      "53\n",
      "y_hat: (432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (432, 32, 48, 6, 1)\n",
      "54\n",
      "y_hat: (440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (440, 32, 48, 6, 1)\n",
      "55\n",
      "y_hat: (448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (448, 32, 48, 6, 1)\n",
      "56\n",
      "y_hat: (456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (456, 32, 48, 6, 1)\n",
      "57\n",
      "y_hat: (464, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (464, 32, 48, 6, 1)\n",
      "58\n",
      "y_hat: (472, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (472, 32, 48, 6, 1)\n",
      "59\n",
      "y_hat: (480, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (480, 32, 48, 6, 1)\n",
      "60\n",
      "y_hat: (488, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (488, 32, 48, 6, 1)\n",
      "61\n",
      "y_hat: (496, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (496, 32, 48, 6, 1)\n",
      "62\n",
      "y_hat: (504, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (504, 32, 48, 6, 1)\n",
      "63\n",
      "y_hat: (512, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (512, 32, 48, 6, 1)\n",
      "64\n",
      "y_hat: (520, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (520, 32, 48, 6, 1)\n",
      "65\n",
      "y_hat: (528, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (528, 32, 48, 6, 1)\n",
      "66\n",
      "y_hat: (536, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (536, 32, 48, 6, 1)\n",
      "67\n",
      "y_hat: (544, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (544, 32, 48, 6, 1)\n",
      "68\n",
      "y_hat: (552, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (552, 32, 48, 6, 1)\n",
      "69\n",
      "y_hat: (560, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (560, 32, 48, 6, 1)\n",
      "70\n",
      "y_hat: (568, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (568, 32, 48, 6, 1)\n",
      "71\n",
      "y_hat: (576, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (576, 32, 48, 6, 1)\n",
      "72\n",
      "y_hat: (584, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (584, 32, 48, 6, 1)\n",
      "73\n",
      "y_hat: (592, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (592, 32, 48, 6, 1)\n",
      "74\n",
      "y_hat: (600, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (600, 32, 48, 6, 1)\n",
      "75\n",
      "y_hat: (608, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (608, 32, 48, 6, 1)\n",
      "76\n",
      "y_hat: (616, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (616, 32, 48, 6, 1)\n",
      "77\n",
      "y_hat: (624, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (624, 32, 48, 6, 1)\n",
      "78\n",
      "y_hat: (632, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (632, 32, 48, 6, 1)\n",
      "79\n",
      "y_hat: (640, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (640, 32, 48, 6, 1)\n",
      "80\n",
      "y_hat: (648, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (648, 32, 48, 6, 1)\n",
      "81\n",
      "y_hat: (656, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (656, 32, 48, 6, 1)\n",
      "82\n",
      "y_hat: (664, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (664, 32, 48, 6, 1)\n",
      "83\n",
      "y_hat: (672, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (672, 32, 48, 6, 1)\n",
      "84\n",
      "y_hat: (680, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (680, 32, 48, 6, 1)\n",
      "85\n",
      "y_hat: (688, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (688, 32, 48, 6, 1)\n",
      "86\n",
      "y_hat: (696, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (696, 32, 48, 6, 1)\n",
      "87\n",
      "y_hat: (704, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (704, 32, 48, 6, 1)\n",
      "88\n",
      "y_hat: (712, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (712, 32, 48, 6, 1)\n",
      "89\n",
      "y_hat: (720, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (720, 32, 48, 6, 1)\n",
      "90\n",
      "y_hat: (728, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (728, 32, 48, 6, 1)\n",
      "91\n",
      "y_hat: (736, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (736, 32, 48, 6, 1)\n",
      "92\n",
      "y_hat: (744, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (744, 32, 48, 6, 1)\n",
      "93\n",
      "y_hat: (752, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (752, 32, 48, 6, 1)\n",
      "94\n",
      "y_hat: (760, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (760, 32, 48, 6, 1)\n",
      "95\n",
      "y_hat: (768, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (768, 32, 48, 6, 1)\n",
      "96\n",
      "y_hat: (776, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (776, 32, 48, 6, 1)\n",
      "97\n",
      "y_hat: (784, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (784, 32, 48, 6, 1)\n",
      "98\n",
      "y_hat: (792, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (792, 32, 48, 6, 1)\n",
      "99\n",
      "y_hat: (800, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (800, 32, 48, 6, 1)\n",
      "100\n",
      "y_hat: (808, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (808, 32, 48, 6, 1)\n",
      "101\n",
      "y_hat: (816, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (816, 32, 48, 6, 1)\n",
      "102\n",
      "y_hat: (824, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (824, 32, 48, 6, 1)\n",
      "103\n",
      "y_hat: (832, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (832, 32, 48, 6, 1)\n",
      "104\n",
      "y_hat: (840, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (840, 32, 48, 6, 1)\n",
      "105\n",
      "y_hat: (848, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (848, 32, 48, 6, 1)\n",
      "106\n",
      "y_hat: (856, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (856, 32, 48, 6, 1)\n",
      "107\n",
      "y_hat: (864, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (864, 32, 48, 6, 1)\n",
      "108\n",
      "y_hat: (872, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (872, 32, 48, 6, 1)\n",
      "109\n",
      "y_hat: (880, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (880, 32, 48, 6, 1)\n",
      "110\n",
      "y_hat: (888, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (888, 32, 48, 6, 1)\n",
      "111\n",
      "y_hat: (896, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (896, 32, 48, 6, 1)\n",
      "112\n",
      "y_hat: (904, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (904, 32, 48, 6, 1)\n",
      "113\n",
      "y_hat: (912, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (912, 32, 48, 6, 1)\n",
      "114\n",
      "y_hat: (920, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (920, 32, 48, 6, 1)\n",
      "115\n",
      "y_hat: (928, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (928, 32, 48, 6, 1)\n",
      "116\n",
      "y_hat: (936, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (936, 32, 48, 6, 1)\n",
      "117\n",
      "y_hat: (944, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (944, 32, 48, 6, 1)\n",
      "118\n",
      "y_hat: (952, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (952, 32, 48, 6, 1)\n",
      "119\n",
      "y_hat: (960, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (960, 32, 48, 6, 1)\n",
      "120\n",
      "y_hat: (968, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (968, 32, 48, 6, 1)\n",
      "121\n",
      "y_hat: (976, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (976, 32, 48, 6, 1)\n",
      "122\n",
      "y_hat: (984, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (984, 32, 48, 6, 1)\n",
      "123\n",
      "y_hat: (992, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (992, 32, 48, 6, 1)\n",
      "124\n",
      "y_hat: (1000, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1000, 32, 48, 6, 1)\n",
      "125\n",
      "y_hat: (1008, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1008, 32, 48, 6, 1)\n",
      "126\n",
      "y_hat: (1016, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1016, 32, 48, 6, 1)\n",
      "127\n",
      "y_hat: (1024, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1024, 32, 48, 6, 1)\n",
      "128\n",
      "y_hat: (1032, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1032, 32, 48, 6, 1)\n",
      "129\n",
      "y_hat: (1040, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1040, 32, 48, 6, 1)\n",
      "130\n",
      "y_hat: (1048, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1048, 32, 48, 6, 1)\n",
      "131\n",
      "y_hat: (1056, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1056, 32, 48, 6, 1)\n",
      "132\n",
      "y_hat: (1064, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1064, 32, 48, 6, 1)\n",
      "133\n",
      "y_hat: (1072, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1072, 32, 48, 6, 1)\n",
      "134\n",
      "y_hat: (1080, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1080, 32, 48, 6, 1)\n",
      "135\n",
      "y_hat: (1088, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1088, 32, 48, 6, 1)\n",
      "136\n",
      "y_hat: (1096, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1096, 32, 48, 6, 1)\n",
      "137\n",
      "y_hat: (1104, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1104, 32, 48, 6, 1)\n",
      "138\n",
      "y_hat: (1112, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1112, 32, 48, 6, 1)\n",
      "139\n",
      "y_hat: (1120, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1120, 32, 48, 6, 1)\n",
      "140\n",
      "y_hat: (1128, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1128, 32, 48, 6, 1)\n",
      "141\n",
      "y_hat: (1136, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1136, 32, 48, 6, 1)\n",
      "142\n",
      "y_hat: (1144, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1144, 32, 48, 6, 1)\n",
      "143\n",
      "y_hat: (1152, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1152, 32, 48, 6, 1)\n",
      "144\n",
      "y_hat: (1160, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1160, 32, 48, 6, 1)\n",
      "145\n",
      "y_hat: (1168, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1168, 32, 48, 6, 1)\n",
      "146\n",
      "y_hat: (1176, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1176, 32, 48, 6, 1)\n",
      "147\n",
      "y_hat: (1184, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1184, 32, 48, 6, 1)\n",
      "148\n",
      "y_hat: (1192, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1192, 32, 48, 6, 1)\n",
      "149\n",
      "y_hat: (1200, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1200, 32, 48, 6, 1)\n",
      "150\n",
      "y_hat: (1208, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1208, 32, 48, 6, 1)\n",
      "151\n",
      "y_hat: (1216, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1216, 32, 48, 6, 1)\n",
      "152\n",
      "y_hat: (1224, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1224, 32, 48, 6, 1)\n",
      "153\n",
      "y_hat: (1232, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1232, 32, 48, 6, 1)\n",
      "154\n",
      "y_hat: (1240, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1240, 32, 48, 6, 1)\n",
      "155\n",
      "y_hat: (1248, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1248, 32, 48, 6, 1)\n",
      "156\n",
      "y_hat: (1256, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1256, 32, 48, 6, 1)\n",
      "157\n",
      "y_hat: (1264, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1264, 32, 48, 6, 1)\n",
      "158\n",
      "y_hat: (1272, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1272, 32, 48, 6, 1)\n",
      "159\n",
      "y_hat: (1280, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1280, 32, 48, 6, 1)\n",
      "160\n",
      "y_hat: (1288, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1288, 32, 48, 6, 1)\n",
      "161\n",
      "y_hat: (1296, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1296, 32, 48, 6, 1)\n",
      "162\n",
      "y_hat: (1304, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1304, 32, 48, 6, 1)\n",
      "163\n",
      "y_hat: (1312, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1312, 32, 48, 6, 1)\n",
      "164\n",
      "y_hat: (1320, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1320, 32, 48, 6, 1)\n",
      "165\n",
      "y_hat: (1328, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1328, 32, 48, 6, 1)\n",
      "166\n",
      "y_hat: (1336, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1336, 32, 48, 6, 1)\n",
      "167\n",
      "y_hat: (1344, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1344, 32, 48, 6, 1)\n",
      "168\n",
      "y_hat: (1352, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1352, 32, 48, 6, 1)\n",
      "169\n",
      "y_hat: (1360, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1360, 32, 48, 6, 1)\n",
      "170\n",
      "y_hat: (1368, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1368, 32, 48, 6, 1)\n",
      "171\n",
      "y_hat: (1376, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1376, 32, 48, 6, 1)\n",
      "172\n",
      "y_hat: (1384, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1384, 32, 48, 6, 1)\n",
      "173\n",
      "y_hat: (1392, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1392, 32, 48, 6, 1)\n",
      "174\n",
      "y_hat: (1400, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1400, 32, 48, 6, 1)\n",
      "175\n",
      "y_hat: (1408, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1408, 32, 48, 6, 1)\n",
      "176\n",
      "y_hat: (1416, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1416, 32, 48, 6, 1)\n",
      "177\n",
      "y_hat: (1424, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1424, 32, 48, 6, 1)\n",
      "178\n",
      "y_hat: (1432, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1432, 32, 48, 6, 1)\n",
      "179\n",
      "y_hat: (1440, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1440, 32, 48, 6, 1)\n",
      "180\n",
      "y_hat: (1448, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1448, 32, 48, 6, 1)\n",
      "181\n",
      "y_hat: (1456, 32, 48, 6, 1), y_hat_i: (8, 32, 48, 6, 1), y_i: (8, 32, 48, 6, 1), batch.x: torch.Size([256, 48, 6, 6]), y: (1456, 32, 48, 6, 1)\n",
      "RMSE for t2m: 1.7056826248585792; MAE for t2m: 1.285307537275658;\n",
      "RMSE for sp: 1.7826369058960037; MAE for sp: 1.3392538606785738;\n",
      "RMSE for tcc: 0.2922628192145925; MAE for tcc: 0.2035519013894322;\n",
      "RMSE for u10: 1.3417298218678986; MAE for u10: 0.9900199250762052;\n",
      "RMSE for v10: 1.2503851418738563; MAE for v10: 0.9182763536563712;\n",
      "RMSE for tp: 0.3163988198849225; MAE for tp: 0.08475949566526317;\n",
      "Metrics collected. [1.7056826248585792, 1.7826369058960037, 0.2922628192145925, 1.3417298218678986, 1.2503851418738563, 0.3163988198849225]\n"
     ]
    }
   ],
   "source": [
    "hpo.collect_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo.write_plots_to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 0.07451, lr: 0.001\n",
      "Val Loss: 0.06592\n",
      "---------\n",
      "Epoch 2/1000, Train Loss: 0.06208, lr: 0.001\n",
      "Val Loss: 0.05903\n",
      "---------\n",
      "Epoch 3/1000, Train Loss: 0.05640, lr: 0.001\n",
      "Val Loss: 0.05540\n",
      "---------\n",
      "Epoch 4/1000, Train Loss: 0.05305, lr: 0.001\n",
      "Val Loss: 0.05045\n",
      "---------\n",
      "Epoch 5/1000, Train Loss: 0.05025, lr: 0.001\n",
      "Val Loss: 0.04891\n",
      "---------\n",
      "Epoch 6/1000, Train Loss: 0.04842, lr: 0.001\n",
      "Val Loss: 0.04791\n",
      "---------\n",
      "Epoch 7/1000, Train Loss: 0.04729, lr: 0.001\n",
      "Val Loss: 0.04728\n",
      "---------\n",
      "Epoch 8/1000, Train Loss: 0.04653, lr: 0.001\n",
      "Val Loss: 0.04663\n",
      "---------\n",
      "Epoch 9/1000, Train Loss: 0.04586, lr: 0.001\n",
      "Val Loss: 0.04577\n",
      "---------\n",
      "Epoch 10/1000, Train Loss: 0.04438, lr: 0.001\n",
      "Val Loss: 0.04244\n",
      "---------\n",
      "Epoch 11/1000, Train Loss: 0.04132, lr: 0.001\n",
      "Val Loss: 0.04116\n",
      "---------\n",
      "Epoch 12/1000, Train Loss: 0.04011, lr: 0.001\n",
      "Val Loss: 0.03993\n",
      "---------\n",
      "Epoch 13/1000, Train Loss: 0.03943, lr: 0.001\n",
      "Val Loss: 0.03940\n",
      "---------\n",
      "Epoch 14/1000, Train Loss: 0.03890, lr: 0.001\n",
      "Val Loss: 0.03938\n",
      "---------\n",
      "Epoch 15/1000, Train Loss: 0.03854, lr: 0.001\n",
      "Val Loss: 0.03936\n",
      "---------\n",
      "Epoch 16/1000, Train Loss: 0.03825, lr: 0.001\n",
      "Val Loss: 0.03978\n",
      "---------\n",
      "Epoch 17/1000, Train Loss: 0.03818, lr: 0.001\n",
      "Val Loss: 0.03947\n",
      "---------\n",
      "Epoch 18/1000, Train Loss: 0.03798, lr: 0.001\n",
      "Val Loss: 0.03914\n",
      "---------\n",
      "Epoch 19/1000, Train Loss: 0.03772, lr: 0.001\n",
      "Val Loss: 0.03883\n",
      "---------\n",
      "Epoch 20/1000, Train Loss: 0.03749, lr: 0.001\n",
      "Val Loss: 0.03860\n",
      "---------\n",
      "Epoch 21/1000, Train Loss: 0.03724, lr: 0.001\n",
      "Val Loss: 0.03844\n",
      "---------\n",
      "Epoch 22/1000, Train Loss: 0.03708, lr: 0.001\n",
      "Val Loss: 0.03829\n",
      "---------\n",
      "Epoch 23/1000, Train Loss: 0.03687, lr: 0.001\n",
      "Val Loss: 0.03800\n",
      "---------\n",
      "Epoch 24/1000, Train Loss: 0.03670, lr: 0.001\n",
      "Val Loss: 0.03801\n",
      "---------\n",
      "Epoch 25/1000, Train Loss: 0.03655, lr: 0.001\n",
      "Val Loss: 0.03779\n",
      "---------\n",
      "Epoch 26/1000, Train Loss: 0.03641, lr: 0.001\n",
      "Val Loss: 0.03761\n",
      "---------\n",
      "Epoch 27/1000, Train Loss: 0.03625, lr: 0.001\n",
      "Val Loss: 0.03751\n",
      "---------\n",
      "Epoch 28/1000, Train Loss: 0.03614, lr: 0.001\n",
      "Val Loss: 0.03746\n",
      "---------\n",
      "Epoch 29/1000, Train Loss: 0.03605, lr: 0.001\n",
      "Val Loss: 0.03735\n",
      "---------\n",
      "Epoch 30/1000, Train Loss: 0.03598, lr: 0.001\n",
      "Val Loss: 0.03718\n",
      "---------\n",
      "Epoch 31/1000, Train Loss: 0.03588, lr: 0.001\n",
      "Val Loss: 0.03710\n",
      "---------\n",
      "Epoch 32/1000, Train Loss: 0.03581, lr: 0.001\n",
      "Val Loss: 0.03710\n",
      "---------\n",
      "Epoch 33/1000, Train Loss: 0.03571, lr: 0.001\n",
      "Val Loss: 0.03703\n",
      "---------\n",
      "Epoch 34/1000, Train Loss: 0.03564, lr: 0.001\n",
      "Val Loss: 0.03703\n",
      "---------\n",
      "Epoch 35/1000, Train Loss: 0.03556, lr: 0.001\n",
      "Val Loss: 0.03704\n",
      "---------\n",
      "Epoch 36/1000, Train Loss: 0.03546, lr: 0.001\n",
      "Val Loss: 0.03701\n",
      "---------\n",
      "Epoch 37/1000, Train Loss: 0.03537, lr: 0.001\n",
      "Val Loss: 0.03701\n",
      "---------\n",
      "Epoch 38/1000, Train Loss: 0.03527, lr: 0.001\n",
      "Val Loss: 0.03687\n",
      "---------\n",
      "Epoch 39/1000, Train Loss: 0.03519, lr: 0.001\n",
      "Val Loss: 0.03671\n",
      "---------\n",
      "Epoch 40/1000, Train Loss: 0.03508, lr: 0.001\n",
      "Val Loss: 0.03655\n",
      "---------\n",
      "Epoch 41/1000, Train Loss: 0.03498, lr: 0.001\n",
      "Val Loss: 0.03643\n",
      "---------\n",
      "Epoch 42/1000, Train Loss: 0.03490, lr: 0.001\n",
      "Val Loss: 0.03640\n",
      "---------\n",
      "Epoch 43/1000, Train Loss: 0.03485, lr: 0.001\n",
      "Val Loss: 0.03634\n",
      "---------\n",
      "Epoch 44/1000, Train Loss: 0.03478, lr: 0.001\n",
      "Val Loss: 0.03625\n",
      "---------\n",
      "Epoch 45/1000, Train Loss: 0.03471, lr: 0.001\n",
      "Val Loss: 0.03616\n",
      "---------\n",
      "Epoch 46/1000, Train Loss: 0.03462, lr: 0.001\n",
      "Val Loss: 0.03609\n",
      "---------\n",
      "Epoch 47/1000, Train Loss: 0.03460, lr: 0.001\n",
      "Val Loss: 0.03602\n",
      "---------\n",
      "Epoch 48/1000, Train Loss: 0.03452, lr: 0.001\n",
      "Val Loss: 0.03596\n",
      "---------\n",
      "Epoch 49/1000, Train Loss: 0.03447, lr: 0.001\n",
      "Val Loss: 0.03589\n",
      "---------\n",
      "Epoch 50/1000, Train Loss: 0.03441, lr: 0.001\n",
      "Val Loss: 0.03583\n",
      "---------\n",
      "Epoch 51/1000, Train Loss: 0.03435, lr: 0.001\n",
      "Val Loss: 0.03583\n",
      "---------\n",
      "Epoch 52/1000, Train Loss: 0.03430, lr: 0.001\n",
      "Val Loss: 0.03572\n",
      "---------\n",
      "Epoch 53/1000, Train Loss: 0.03422, lr: 0.001\n",
      "Val Loss: 0.03570\n",
      "---------\n",
      "Epoch 54/1000, Train Loss: 0.03417, lr: 0.001\n",
      "Val Loss: 0.03567\n",
      "---------\n",
      "Epoch 55/1000, Train Loss: 0.03412, lr: 0.001\n",
      "Val Loss: 0.03563\n",
      "---------\n",
      "Epoch 56/1000, Train Loss: 0.03406, lr: 0.001\n",
      "Val Loss: 0.03559\n",
      "---------\n",
      "Epoch 57/1000, Train Loss: 0.03401, lr: 0.001\n",
      "Val Loss: 0.03563\n",
      "---------\n",
      "Epoch 58/1000, Train Loss: 0.03396, lr: 0.001\n",
      "Val Loss: 0.03554\n",
      "---------\n",
      "Epoch 59/1000, Train Loss: 0.03394, lr: 0.001\n",
      "Val Loss: 0.03556\n",
      "---------\n",
      "Epoch 60/1000, Train Loss: 0.03392, lr: 0.001\n",
      "Val Loss: 0.03560\n",
      "---------\n",
      "Epoch 61/1000, Train Loss: 0.03391, lr: 0.001\n",
      "Val Loss: 0.03549\n",
      "---------\n",
      "Epoch 62/1000, Train Loss: 0.03387, lr: 0.001\n",
      "Val Loss: 0.03556\n",
      "---------\n",
      "Epoch 63/1000, Train Loss: 0.03386, lr: 0.001\n",
      "Val Loss: 0.03550\n",
      "---------\n",
      "Epoch 64/1000, Train Loss: 0.03381, lr: 0.001\n",
      "Val Loss: 0.03557\n",
      "---------\n",
      "Epoch 65/1000, Train Loss: 0.03378, lr: 0.001\n",
      "Val Loss: 0.03552\n",
      "---------\n",
      "Epoch 66/1000, Train Loss: 0.03376, lr: 0.001\n",
      "Val Loss: 0.03556\n",
      "---------\n",
      "Epoch 67/1000, Train Loss: 0.03374, lr: 0.001\n",
      "Val Loss: 0.03540\n",
      "---------\n",
      "Epoch 68/1000, Train Loss: 0.03371, lr: 0.001\n",
      "Val Loss: 0.03544\n",
      "---------\n",
      "Epoch 69/1000, Train Loss: 0.03376, lr: 0.001\n",
      "Val Loss: 0.03557\n",
      "---------\n",
      "Epoch 70/1000, Train Loss: 0.03371, lr: 0.001\n",
      "Val Loss: 0.03548\n",
      "---------\n",
      "Epoch 71/1000, Train Loss: 0.03370, lr: 0.001\n",
      "Val Loss: 0.03555\n",
      "---------\n",
      "Epoch 72/1000, Train Loss: 0.03365, lr: 0.001\n",
      "Val Loss: 0.03546\n",
      "---------\n",
      "Epoch 73/1000, Train Loss: 0.03356, lr: 0.001\n",
      "Val Loss: 0.03547\n",
      "---------\n",
      "Epoch 74/1000, Train Loss: 0.03350, lr: 0.001\n",
      "Val Loss: 0.03541\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 75/1000, Train Loss: 0.03295, lr: 0.0005\n",
      "Val Loss: 0.03491\n",
      "---------\n",
      "Epoch 76/1000, Train Loss: 0.03281, lr: 0.0005\n",
      "Val Loss: 0.03488\n",
      "---------\n",
      "Epoch 77/1000, Train Loss: 0.03275, lr: 0.0005\n",
      "Val Loss: 0.03485\n",
      "---------\n",
      "Epoch 78/1000, Train Loss: 0.03271, lr: 0.0005\n",
      "Val Loss: 0.03483\n",
      "---------\n",
      "Epoch 79/1000, Train Loss: 0.03267, lr: 0.0005\n",
      "Val Loss: 0.03484\n",
      "---------\n",
      "Epoch 80/1000, Train Loss: 0.03263, lr: 0.0005\n",
      "Val Loss: 0.03484\n",
      "---------\n",
      "Epoch 81/1000, Train Loss: 0.03260, lr: 0.0005\n",
      "Val Loss: 0.03486\n",
      "---------\n",
      "Epoch 82/1000, Train Loss: 0.03256, lr: 0.0005\n",
      "Val Loss: 0.03484\n",
      "---------\n",
      "Epoch 83/1000, Train Loss: 0.03254, lr: 0.0005\n",
      "Val Loss: 0.03483\n",
      "---------\n",
      "Epoch 84/1000, Train Loss: 0.03250, lr: 0.0005\n",
      "Val Loss: 0.03482\n",
      "---------\n",
      "Epoch 85/1000, Train Loss: 0.03248, lr: 0.0005\n",
      "Val Loss: 0.03483\n",
      "---------\n",
      "Epoch 86/1000, Train Loss: 0.03246, lr: 0.0005\n",
      "Val Loss: 0.03482\n",
      "---------\n",
      "Epoch 87/1000, Train Loss: 0.03243, lr: 0.0005\n",
      "Val Loss: 0.03482\n",
      "---------\n",
      "Epoch 88/1000, Train Loss: 0.03241, lr: 0.0005\n",
      "Val Loss: 0.03484\n",
      "---------\n",
      "Epoch 89/1000, Train Loss: 0.03240, lr: 0.0005\n",
      "Val Loss: 0.03484\n",
      "---------\n",
      "Epoch 90/1000, Train Loss: 0.03238, lr: 0.0005\n",
      "Val Loss: 0.03485\n",
      "---------\n",
      "Epoch 91/1000, Train Loss: 0.03236, lr: 0.0005\n",
      "Val Loss: 0.03482\n",
      "---------\n",
      "Epoch 92/1000, Train Loss: 0.03234, lr: 0.0005\n",
      "Val Loss: 0.03482\n",
      "---------\n",
      "Epoch 93/1000, Train Loss: 0.03232, lr: 0.0005\n",
      "Val Loss: 0.03482\n",
      "---------\n",
      "Epoch 94/1000, Train Loss: 0.03229, lr: 0.0005\n",
      "Val Loss: 0.03482\n",
      "---------\n",
      "Epoch 95/1000, Train Loss: 0.03227, lr: 0.0005\n",
      "Val Loss: 0.03481\n",
      "---------\n",
      "Epoch 96/1000, Train Loss: 0.03225, lr: 0.0005\n",
      "Val Loss: 0.03483\n",
      "---------\n",
      "Epoch 97/1000, Train Loss: 0.03223, lr: 0.0005\n",
      "Val Loss: 0.03482\n",
      "---------\n",
      "Epoch 98/1000, Train Loss: 0.03221, lr: 0.0005\n",
      "Val Loss: 0.03483\n",
      "---------\n",
      "Epoch 99/1000, Train Loss: 0.03218, lr: 0.0005\n",
      "Val Loss: 0.03482\n",
      "---------\n",
      "Epoch 100/1000, Train Loss: 0.03216, lr: 0.0005\n",
      "Val Loss: 0.03482\n",
      "---------\n",
      "Epoch 101/1000, Train Loss: 0.03214, lr: 0.0005\n",
      "Val Loss: 0.03483\n",
      "---------\n",
      "Epoch 102/1000, Train Loss: 0.03213, lr: 0.0005\n",
      "Val Loss: 0.03482\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 103/1000, Train Loss: 0.03173, lr: 0.00025\n",
      "Val Loss: 0.03463\n",
      "---------\n",
      "Epoch 104/1000, Train Loss: 0.03162, lr: 0.00025\n",
      "Val Loss: 0.03457\n",
      "---------\n",
      "Epoch 105/1000, Train Loss: 0.03158, lr: 0.00025\n",
      "Val Loss: 0.03453\n",
      "---------\n",
      "Epoch 106/1000, Train Loss: 0.03156, lr: 0.00025\n",
      "Val Loss: 0.03450\n",
      "---------\n",
      "Epoch 107/1000, Train Loss: 0.03153, lr: 0.00025\n",
      "Val Loss: 0.03449\n",
      "---------\n",
      "Epoch 108/1000, Train Loss: 0.03152, lr: 0.00025\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 109/1000, Train Loss: 0.03150, lr: 0.00025\n",
      "Val Loss: 0.03447\n",
      "---------\n",
      "Epoch 110/1000, Train Loss: 0.03149, lr: 0.00025\n",
      "Val Loss: 0.03448\n",
      "---------\n",
      "Epoch 111/1000, Train Loss: 0.03148, lr: 0.00025\n",
      "Val Loss: 0.03446\n",
      "---------\n",
      "Epoch 112/1000, Train Loss: 0.03146, lr: 0.00025\n",
      "Val Loss: 0.03445\n",
      "---------\n",
      "Epoch 113/1000, Train Loss: 0.03145, lr: 0.00025\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 114/1000, Train Loss: 0.03143, lr: 0.00025\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 115/1000, Train Loss: 0.03142, lr: 0.00025\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 116/1000, Train Loss: 0.03141, lr: 0.00025\n",
      "Val Loss: 0.03444\n",
      "---------\n",
      "Epoch 117/1000, Train Loss: 0.03140, lr: 0.00025\n",
      "Val Loss: 0.03443\n",
      "---------\n",
      "Epoch 118/1000, Train Loss: 0.03138, lr: 0.00025\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 119/1000, Train Loss: 0.03136, lr: 0.00025\n",
      "Val Loss: 0.03442\n",
      "---------\n",
      "Epoch 120/1000, Train Loss: 0.03135, lr: 0.00025\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 121/1000, Train Loss: 0.03134, lr: 0.00025\n",
      "Val Loss: 0.03441\n",
      "---------\n",
      "Epoch 122/1000, Train Loss: 0.03132, lr: 0.00025\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 123/1000, Train Loss: 0.03131, lr: 0.00025\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 124/1000, Train Loss: 0.03130, lr: 0.00025\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 125/1000, Train Loss: 0.03129, lr: 0.00025\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 126/1000, Train Loss: 0.03128, lr: 0.00025\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 127/1000, Train Loss: 0.03126, lr: 0.00025\n",
      "Val Loss: 0.03438\n",
      "---------\n",
      "Epoch 128/1000, Train Loss: 0.03126, lr: 0.00025\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 129/1000, Train Loss: 0.03125, lr: 0.00025\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 130/1000, Train Loss: 0.03123, lr: 0.00025\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 131/1000, Train Loss: 0.03122, lr: 0.00025\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 132/1000, Train Loss: 0.03121, lr: 0.00025\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "Epoch 133/1000, Train Loss: 0.03120, lr: 0.00025\n",
      "Val Loss: 0.03440\n",
      "---------\n",
      "Epoch 134/1000, Train Loss: 0.03119, lr: 0.00025\n",
      "Val Loss: 0.03439\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 135/1000, Train Loss: 0.03096, lr: 0.000125\n",
      "Val Loss: 0.03406\n",
      "---------\n",
      "Epoch 136/1000, Train Loss: 0.03094, lr: 0.000125\n",
      "Val Loss: 0.03405\n",
      "---------\n",
      "Epoch 137/1000, Train Loss: 0.03092, lr: 0.000125\n",
      "Val Loss: 0.03405\n",
      "---------\n",
      "Epoch 138/1000, Train Loss: 0.03091, lr: 0.000125\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "Epoch 139/1000, Train Loss: 0.03090, lr: 0.000125\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "Epoch 140/1000, Train Loss: 0.03089, lr: 0.000125\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "Epoch 141/1000, Train Loss: 0.03088, lr: 0.000125\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "Epoch 142/1000, Train Loss: 0.03087, lr: 0.000125\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "Epoch 143/1000, Train Loss: 0.03086, lr: 0.000125\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "Epoch 144/1000, Train Loss: 0.03086, lr: 0.000125\n",
      "Val Loss: 0.03403\n",
      "---------\n",
      "Epoch 145/1000, Train Loss: 0.03085, lr: 0.000125\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "Epoch 146/1000, Train Loss: 0.03084, lr: 0.000125\n",
      "Val Loss: 0.03403\n",
      "---------\n",
      "Epoch 147/1000, Train Loss: 0.03083, lr: 0.000125\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "Epoch 148/1000, Train Loss: 0.03083, lr: 0.000125\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "Epoch 149/1000, Train Loss: 0.03082, lr: 0.000125\n",
      "Val Loss: 0.03403\n",
      "---------\n",
      "Epoch 150/1000, Train Loss: 0.03081, lr: 0.000125\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "Epoch 151/1000, Train Loss: 0.03080, lr: 0.000125\n",
      "Val Loss: 0.03404\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 152/1000, Train Loss: 0.03070, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 153/1000, Train Loss: 0.03067, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 154/1000, Train Loss: 0.03066, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 155/1000, Train Loss: 0.03066, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 156/1000, Train Loss: 0.03065, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 157/1000, Train Loss: 0.03064, lr: 6.25e-05\n",
      "Val Loss: 0.03397\n",
      "---------\n",
      "Epoch 158/1000, Train Loss: 0.03064, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 159/1000, Train Loss: 0.03063, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 160/1000, Train Loss: 0.03063, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 161/1000, Train Loss: 0.03062, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 162/1000, Train Loss: 0.03062, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 163/1000, Train Loss: 0.03061, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 164/1000, Train Loss: 0.03061, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 165/1000, Train Loss: 0.03061, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 166/1000, Train Loss: 0.03060, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 167/1000, Train Loss: 0.03060, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 168/1000, Train Loss: 0.03059, lr: 6.25e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 169/1000, Train Loss: 0.03054, lr: 3.125e-05\n",
      "Val Loss: 0.03396\n",
      "---------\n",
      "Epoch 170/1000, Train Loss: 0.03053, lr: 3.125e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 171/1000, Train Loss: 0.03053, lr: 3.125e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 172/1000, Train Loss: 0.03053, lr: 3.125e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 173/1000, Train Loss: 0.03052, lr: 3.125e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 174/1000, Train Loss: 0.03052, lr: 3.125e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 175/1000, Train Loss: 0.03052, lr: 3.125e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 176/1000, Train Loss: 0.03051, lr: 3.125e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 177/1000, Train Loss: 0.03051, lr: 3.125e-05\n",
      "Val Loss: 0.03395\n",
      "---------\n",
      "Epoch 178/1000, Train Loss: 0.03051, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 179/1000, Train Loss: 0.03051, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 180/1000, Train Loss: 0.03050, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 181/1000, Train Loss: 0.03050, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 182/1000, Train Loss: 0.03050, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 183/1000, Train Loss: 0.03049, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 184/1000, Train Loss: 0.03049, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 185/1000, Train Loss: 0.03049, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 186/1000, Train Loss: 0.03049, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 187/1000, Train Loss: 0.03049, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 188/1000, Train Loss: 0.03048, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 189/1000, Train Loss: 0.03048, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 190/1000, Train Loss: 0.03048, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 191/1000, Train Loss: 0.03048, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 192/1000, Train Loss: 0.03047, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 193/1000, Train Loss: 0.03047, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 194/1000, Train Loss: 0.03047, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 195/1000, Train Loss: 0.03047, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 196/1000, Train Loss: 0.03047, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 197/1000, Train Loss: 0.03046, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 198/1000, Train Loss: 0.03046, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 199/1000, Train Loss: 0.03046, lr: 3.125e-05\n",
      "Val Loss: 0.03394\n",
      "---------\n",
      "Epoch 200/1000, Train Loss: 0.03046, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 201/1000, Train Loss: 0.03045, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 202/1000, Train Loss: 0.03045, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 203/1000, Train Loss: 0.03045, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 204/1000, Train Loss: 0.03045, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 205/1000, Train Loss: 0.03045, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 206/1000, Train Loss: 0.03044, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 207/1000, Train Loss: 0.03044, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 208/1000, Train Loss: 0.03044, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 209/1000, Train Loss: 0.03044, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 210/1000, Train Loss: 0.03044, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 211/1000, Train Loss: 0.03043, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 212/1000, Train Loss: 0.03043, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 213/1000, Train Loss: 0.03043, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 214/1000, Train Loss: 0.03043, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 215/1000, Train Loss: 0.03043, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 216/1000, Train Loss: 0.03042, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 217/1000, Train Loss: 0.03042, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 218/1000, Train Loss: 0.03042, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 219/1000, Train Loss: 0.03042, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 220/1000, Train Loss: 0.03042, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 221/1000, Train Loss: 0.03042, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 222/1000, Train Loss: 0.03041, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 223/1000, Train Loss: 0.03041, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 224/1000, Train Loss: 0.03041, lr: 3.125e-05\n",
      "Val Loss: 0.03393\n",
      "---------\n",
      "Epoch 225/1000, Train Loss: 0.03041, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 226/1000, Train Loss: 0.03041, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 227/1000, Train Loss: 0.03040, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 228/1000, Train Loss: 0.03040, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 229/1000, Train Loss: 0.03040, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 230/1000, Train Loss: 0.03040, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 231/1000, Train Loss: 0.03040, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 232/1000, Train Loss: 0.03039, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 233/1000, Train Loss: 0.03039, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 234/1000, Train Loss: 0.03039, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 235/1000, Train Loss: 0.03039, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 236/1000, Train Loss: 0.03039, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 237/1000, Train Loss: 0.03038, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 238/1000, Train Loss: 0.03038, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 239/1000, Train Loss: 0.03038, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 240/1000, Train Loss: 0.03038, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 241/1000, Train Loss: 0.03038, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 242/1000, Train Loss: 0.03038, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 243/1000, Train Loss: 0.03037, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 244/1000, Train Loss: 0.03037, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 245/1000, Train Loss: 0.03037, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 246/1000, Train Loss: 0.03037, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 247/1000, Train Loss: 0.03037, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 248/1000, Train Loss: 0.03037, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 249/1000, Train Loss: 0.03036, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 250/1000, Train Loss: 0.03036, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 251/1000, Train Loss: 0.03036, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 252/1000, Train Loss: 0.03036, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 253/1000, Train Loss: 0.03036, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 254/1000, Train Loss: 0.03035, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 255/1000, Train Loss: 0.03035, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 256/1000, Train Loss: 0.03035, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 257/1000, Train Loss: 0.03035, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 258/1000, Train Loss: 0.03035, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 259/1000, Train Loss: 0.03035, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 260/1000, Train Loss: 0.03034, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 261/1000, Train Loss: 0.03034, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 262/1000, Train Loss: 0.03034, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 263/1000, Train Loss: 0.03034, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 264/1000, Train Loss: 0.03034, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 265/1000, Train Loss: 0.03034, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 266/1000, Train Loss: 0.03033, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 267/1000, Train Loss: 0.03033, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 268/1000, Train Loss: 0.03033, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 269/1000, Train Loss: 0.03033, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 270/1000, Train Loss: 0.03033, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 271/1000, Train Loss: 0.03033, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 272/1000, Train Loss: 0.03032, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 273/1000, Train Loss: 0.03032, lr: 3.125e-05\n",
      "Val Loss: 0.03391\n",
      "---------\n",
      "Epoch 274/1000, Train Loss: 0.03032, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 275/1000, Train Loss: 0.03032, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 276/1000, Train Loss: 0.03032, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 277/1000, Train Loss: 0.03032, lr: 3.125e-05\n",
      "Val Loss: 0.03391\n",
      "---------\n",
      "Epoch 278/1000, Train Loss: 0.03031, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 279/1000, Train Loss: 0.03031, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "Epoch 280/1000, Train Loss: 0.03031, lr: 3.125e-05\n",
      "Val Loss: 0.03392\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 281/1000, Train Loss: 0.03029, lr: 1.5625e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 282/1000, Train Loss: 0.03029, lr: 1.5625e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 283/1000, Train Loss: 0.03029, lr: 1.5625e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 284/1000, Train Loss: 0.03029, lr: 1.5625e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 285/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 286/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 287/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 288/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 289/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 290/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 291/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 292/1000, Train Loss: 0.03028, lr: 1.5625e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 293/1000, Train Loss: 0.03027, lr: 1.5625e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 294/1000, Train Loss: 0.03027, lr: 1.5625e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 295/1000, Train Loss: 0.03027, lr: 1.5625e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 296/1000, Train Loss: 0.03027, lr: 1.5625e-05\n",
      "Val Loss: 0.03387\n",
      "---------\n",
      "Epoch 297/1000, Train Loss: 0.03027, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 298/1000, Train Loss: 0.03027, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 299/1000, Train Loss: 0.03027, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 300/1000, Train Loss: 0.03027, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 301/1000, Train Loss: 0.03027, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 302/1000, Train Loss: 0.03027, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 303/1000, Train Loss: 0.03026, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 304/1000, Train Loss: 0.03026, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 305/1000, Train Loss: 0.03026, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 306/1000, Train Loss: 0.03026, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 307/1000, Train Loss: 0.03026, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 308/1000, Train Loss: 0.03026, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 309/1000, Train Loss: 0.03026, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 310/1000, Train Loss: 0.03026, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 311/1000, Train Loss: 0.03026, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 312/1000, Train Loss: 0.03026, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 313/1000, Train Loss: 0.03025, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 314/1000, Train Loss: 0.03025, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 315/1000, Train Loss: 0.03025, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 316/1000, Train Loss: 0.03025, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 317/1000, Train Loss: 0.03025, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 318/1000, Train Loss: 0.03025, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 319/1000, Train Loss: 0.03025, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 320/1000, Train Loss: 0.03025, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 321/1000, Train Loss: 0.03025, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 322/1000, Train Loss: 0.03025, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 323/1000, Train Loss: 0.03024, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 324/1000, Train Loss: 0.03024, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 325/1000, Train Loss: 0.03024, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 326/1000, Train Loss: 0.03024, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 327/1000, Train Loss: 0.03024, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 328/1000, Train Loss: 0.03024, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 329/1000, Train Loss: 0.03024, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 330/1000, Train Loss: 0.03024, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 331/1000, Train Loss: 0.03024, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 332/1000, Train Loss: 0.03024, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 333/1000, Train Loss: 0.03024, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 334/1000, Train Loss: 0.03024, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 335/1000, Train Loss: 0.03023, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 336/1000, Train Loss: 0.03023, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 337/1000, Train Loss: 0.03023, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 338/1000, Train Loss: 0.03023, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 339/1000, Train Loss: 0.03023, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 340/1000, Train Loss: 0.03023, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "Epoch 341/1000, Train Loss: 0.03023, lr: 1.5625e-05\n",
      "Val Loss: 0.03386\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 342/1000, Train Loss: 0.03021, lr: 7.8125e-06\n",
      "Val Loss: 0.03384\n",
      "---------\n",
      "Epoch 343/1000, Train Loss: 0.03021, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 344/1000, Train Loss: 0.03020, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 345/1000, Train Loss: 0.03020, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 346/1000, Train Loss: 0.03020, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 347/1000, Train Loss: 0.03020, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 348/1000, Train Loss: 0.03020, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 349/1000, Train Loss: 0.03020, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 350/1000, Train Loss: 0.03020, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 351/1000, Train Loss: 0.03020, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 352/1000, Train Loss: 0.03020, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 353/1000, Train Loss: 0.03020, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 354/1000, Train Loss: 0.03020, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 355/1000, Train Loss: 0.03020, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 356/1000, Train Loss: 0.03020, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 357/1000, Train Loss: 0.03020, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 358/1000, Train Loss: 0.03020, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 359/1000, Train Loss: 0.03020, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 360/1000, Train Loss: 0.03020, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 361/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 362/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 363/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 364/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 365/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 366/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 367/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 368/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 369/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 370/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 371/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 372/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 373/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 374/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 375/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 376/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 377/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 378/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 379/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 380/1000, Train Loss: 0.03019, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 381/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 382/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 383/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 384/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 385/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 386/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 387/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 388/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 389/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 390/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 391/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 392/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 393/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 394/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 395/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 396/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 397/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 398/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 399/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 400/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 401/1000, Train Loss: 0.03018, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 402/1000, Train Loss: 0.03017, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 403/1000, Train Loss: 0.03017, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 404/1000, Train Loss: 0.03017, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 405/1000, Train Loss: 0.03017, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 406/1000, Train Loss: 0.03017, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 407/1000, Train Loss: 0.03017, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 408/1000, Train Loss: 0.03017, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 409/1000, Train Loss: 0.03017, lr: 7.8125e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 410/1000, Train Loss: 0.03016, lr: 3.90625e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 411/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03383\n",
      "---------\n",
      "Epoch 412/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 413/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 414/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 415/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 416/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 417/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 418/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 419/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 420/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 421/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 422/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 423/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 424/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 425/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 426/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 427/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 428/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 429/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 430/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 431/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 432/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 433/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 434/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 435/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 436/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 437/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 438/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 439/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 440/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 441/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 442/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 443/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 444/1000, Train Loss: 0.03015, lr: 3.90625e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 445/1000, Train Loss: 0.03014, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 446/1000, Train Loss: 0.03014, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 447/1000, Train Loss: 0.03014, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 448/1000, Train Loss: 0.03014, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 449/1000, Train Loss: 0.03014, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 450/1000, Train Loss: 0.03014, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 451/1000, Train Loss: 0.03014, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 452/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 453/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 454/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 455/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 456/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 457/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 458/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 459/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 460/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 461/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 462/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 463/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 464/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 465/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 466/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 467/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 468/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 469/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 470/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 471/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 472/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 473/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 474/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 475/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 476/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 477/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 478/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 479/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 480/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 481/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 482/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 483/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 484/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 485/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 486/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 487/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 488/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 489/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 490/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 491/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 492/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 493/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 494/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 495/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 496/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 497/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 498/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 499/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 500/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 501/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 502/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 503/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 504/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 505/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 506/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 507/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 508/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 509/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 510/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 511/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 512/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 513/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 514/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 515/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 516/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 517/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 518/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 519/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 520/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 521/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 522/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 523/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 524/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 525/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 526/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 527/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 528/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 529/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 530/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 531/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 532/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 533/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 534/1000, Train Loss: 0.03013, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 535/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 536/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 537/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 538/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 539/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 540/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 541/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 542/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 543/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 544/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 545/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 546/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 547/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 548/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 549/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 550/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 551/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 552/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 553/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 554/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 555/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 556/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 557/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 558/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 559/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 560/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 561/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 562/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 563/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 564/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 565/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 566/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 567/1000, Train Loss: 0.03012, lr: 1.953125e-06\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 568/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 569/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 570/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 571/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 572/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 573/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 574/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 575/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 576/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 577/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 578/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 579/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 580/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 581/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 582/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 583/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 584/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 585/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 586/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 587/1000, Train Loss: 0.03012, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 588/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 589/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 590/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 591/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 592/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 593/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 594/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 595/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 596/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 597/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 598/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 599/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 600/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 601/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 602/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 603/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 604/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 605/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 606/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 607/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 608/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 609/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 610/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 611/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 612/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 613/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 614/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 615/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 616/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 617/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 618/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 619/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 620/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 621/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 622/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 623/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 624/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 625/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 626/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 627/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 628/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 629/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 630/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 631/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 632/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 633/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 634/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 635/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 636/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 637/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 638/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 639/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 640/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 641/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 642/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 643/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 644/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 645/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 646/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 647/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 648/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 649/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 650/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 651/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 652/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 653/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 654/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 655/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 656/1000, Train Loss: 0.03011, lr: 9.765625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 657/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 658/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 659/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 660/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 661/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 662/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 663/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 664/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 665/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 666/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 667/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 668/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 669/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 670/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 671/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 672/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 673/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 674/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 675/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 676/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 677/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 678/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 679/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 680/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 681/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 682/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 683/1000, Train Loss: 0.03011, lr: 4.8828125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 684/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 685/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 686/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 687/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 688/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 689/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 690/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 691/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 692/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 693/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 694/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 695/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 696/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 697/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 698/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 699/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 700/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 701/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 702/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 703/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 704/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 705/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 706/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 707/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 708/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 709/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 710/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 711/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 712/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 713/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 714/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 715/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 716/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 717/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 718/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 719/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 720/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 721/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 722/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 723/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 724/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 725/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 726/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 727/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 728/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 729/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 730/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 731/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 732/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 733/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 734/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 735/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 736/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 737/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 738/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 739/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 740/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 741/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 742/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 743/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 744/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 745/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 746/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 747/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 748/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 749/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 750/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 751/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 752/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 753/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 754/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 755/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 756/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 757/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 758/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 759/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 760/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 761/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 762/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 763/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 764/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 765/1000, Train Loss: 0.03011, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 766/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 767/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 768/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 769/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 770/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 771/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 772/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 773/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 774/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 775/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 776/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 777/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 778/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 779/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 780/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 781/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 782/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 783/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 784/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 785/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 786/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 787/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 788/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 789/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 790/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 791/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 792/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 793/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 794/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 795/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 796/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 797/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 798/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 799/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 800/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 801/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 802/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 803/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 804/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 805/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 806/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 807/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 808/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 809/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 810/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 811/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 812/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 813/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 814/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 815/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 816/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 817/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 818/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 819/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 820/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 821/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 822/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 823/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 824/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 825/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 826/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 827/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 828/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 829/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 830/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 831/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 832/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 833/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 834/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 835/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 836/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 837/1000, Train Loss: 0.03010, lr: 2.44140625e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 838/1000, Train Loss: 0.03010, lr: 1.220703125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 839/1000, Train Loss: 0.03010, lr: 1.220703125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 840/1000, Train Loss: 0.03010, lr: 1.220703125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 841/1000, Train Loss: 0.03010, lr: 1.220703125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 842/1000, Train Loss: 0.03010, lr: 1.220703125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 843/1000, Train Loss: 0.03010, lr: 1.220703125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 844/1000, Train Loss: 0.03010, lr: 1.220703125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 845/1000, Train Loss: 0.03010, lr: 1.220703125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 846/1000, Train Loss: 0.03010, lr: 1.220703125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 847/1000, Train Loss: 0.03010, lr: 1.220703125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 848/1000, Train Loss: 0.03010, lr: 1.220703125e-07\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 849/1000, Train Loss: 0.03010, lr: 6.103515625e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 850/1000, Train Loss: 0.03010, lr: 6.103515625e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 851/1000, Train Loss: 0.03010, lr: 6.103515625e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 852/1000, Train Loss: 0.03010, lr: 6.103515625e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 853/1000, Train Loss: 0.03010, lr: 6.103515625e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 854/1000, Train Loss: 0.03010, lr: 6.103515625e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 855/1000, Train Loss: 0.03010, lr: 6.103515625e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 856/1000, Train Loss: 0.03010, lr: 6.103515625e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 857/1000, Train Loss: 0.03010, lr: 6.103515625e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 858/1000, Train Loss: 0.03010, lr: 6.103515625e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 859/1000, Train Loss: 0.03010, lr: 6.103515625e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "\n",
      "[Callback] Adjusting lr. Counter: 7\n",
      "\n",
      "Epoch 860/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 861/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 862/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 863/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 864/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 865/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 866/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 867/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 868/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 869/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 870/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 871/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 872/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 873/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 874/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 875/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 876/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 877/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 878/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 879/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 880/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 881/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 882/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 883/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 884/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 885/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 886/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 887/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 888/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 889/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 890/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 891/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 892/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 893/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 894/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 895/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 896/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 897/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 898/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 899/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 900/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 901/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 902/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 903/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 904/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 905/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 906/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 907/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 908/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 909/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 910/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 911/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 912/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 913/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 914/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 915/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 916/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 917/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 918/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 919/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 920/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 921/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 922/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 923/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 924/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 925/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 926/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 927/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 928/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 929/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 930/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 931/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 932/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 933/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 934/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 935/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 936/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 937/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 938/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 939/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 940/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 941/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 942/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 943/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 944/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 945/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 946/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 947/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 948/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 949/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 950/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 951/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 952/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 953/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 954/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 955/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 956/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 957/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 958/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 959/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 960/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 961/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 962/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 963/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 964/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 965/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 966/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 967/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 968/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 969/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 970/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 971/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 972/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 973/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 974/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 975/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 976/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 977/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 978/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 979/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 980/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 981/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 982/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 983/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 984/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 985/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 986/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 987/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 988/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 989/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 990/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 991/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 992/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 993/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 994/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 995/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 996/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 997/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 998/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 999/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "Epoch 1000/1000, Train Loss: 0.03010, lr: 3.0517578125e-08\n",
      "Val Loss: 0.03382\n",
      "---------\n",
      "4921.34747004509 [s]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "RMSE for t2m: 1.689350738117588; MAE for t2m: 1.2989080959996044;\n",
      "RMSE for sp: 1.5835371255735728; MAE for sp: 1.1944136339823406;\n",
      "RMSE for tcc: 0.3010536458295845; MAE for tcc: 0.19823911579742032;\n",
      "RMSE for u10: 1.3727817117723249; MAE for u10: 0.9997638908440438;\n",
      "RMSE for v10: 1.2206359523423986; MAE for v10: 0.9133650059309768;\n",
      "RMSE for tp: 0.2959898505246854; MAE for tp: 0.08294929358747383;\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "RMSE for t2m: 1.647429909980504; MAE for t2m: 1.2405465865273388;\n",
      "RMSE for sp: 1.4949778985611508; MAE for sp: 1.160108648390997;\n",
      "RMSE for tcc: 0.2782416456903059; MAE for tcc: 0.18568411958176267;\n",
      "RMSE for u10: 1.2002777667116233; MAE for u10: 0.8968121618715407;\n",
      "RMSE for v10: 1.1997953998398894; MAE for v10: 0.8871711424936707;\n",
      "RMSE for tp: 0.26092787869986167; MAE for tp: 0.07470302027111513;\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "RMSE for t2m: 1.7468158653296395; MAE for t2m: 1.3036458052420132;\n",
      "RMSE for sp: 1.6540115623017897; MAE for sp: 1.2153701479311343;\n",
      "RMSE for tcc: 0.2931623593948111; MAE for tcc: 0.19237479198348564;\n",
      "RMSE for u10: 1.2795099480080085; MAE for u10: 0.9510784174376113;\n",
      "RMSE for v10: 1.232310824362047; MAE for v10: 0.8880899847447452;\n",
      "RMSE for tp: 0.34133770467764857; MAE for tp: 0.09225589556185906;\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "RMSE for t2m: 1.5836994491726424; MAE for t2m: 1.2144866611995124;\n",
      "RMSE for sp: 1.553367971598503; MAE for sp: 1.1754966774269386;\n",
      "RMSE for tcc: 0.27813554218633857; MAE for tcc: 0.18407165698815456;\n",
      "RMSE for u10: 1.2372530311607937; MAE for u10: 0.9268925664996395;\n",
      "RMSE for v10: 1.192080709558646; MAE for v10: 0.8811741470650393;\n",
      "RMSE for tp: 0.28409631792927303; MAE for tp: 0.08057374349039302;\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "RMSE for t2m: 1.718832550843821; MAE for t2m: 1.3063615600086331;\n",
      "RMSE for sp: 1.588793245489733; MAE for sp: 1.2068794593811034;\n",
      "RMSE for tcc: 0.29155609628540896; MAE for tcc: 0.19752396083022694;\n",
      "RMSE for u10: 1.234291394502762; MAE for u10: 0.9318769542746025;\n",
      "RMSE for v10: 1.197474021383126; MAE for v10: 0.8805438100520409;\n",
      "RMSE for tp: 0.27362645227235655; MAE for tp: 0.07233816992512136;\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "RMSE for t2m: 1.677614960189677; MAE for t2m: 1.2671287080360392;\n",
      "RMSE for sp: 1.6096325372310396; MAE for sp: 1.2048665210865162;\n",
      "RMSE for tcc: 0.30515243773407064; MAE for tcc: 0.20848131868987052;\n",
      "RMSE for u10: 1.2898887130054095; MAE for u10: 0.9562919906664519;\n",
      "RMSE for v10: 1.2621208093820575; MAE for v10: 0.919148731982336;\n",
      "RMSE for tp: 0.26193522164493305; MAE for tp: 0.07728997988957337;\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "RMSE for t2m: 1.6894187152426696; MAE for t2m: 1.2707650570967879;\n",
      "RMSE for sp: 1.5368738120467949; MAE for sp: 1.1571172141746238;\n",
      "RMSE for tcc: 0.28310566663127607; MAE for tcc: 0.18865746509859355;\n",
      "RMSE for u10: 1.3312245009099835; MAE for u10: 0.967132957868417;\n",
      "RMSE for v10: 1.2349105733110466; MAE for v10: 0.9125451399513623;\n",
      "RMSE for tp: 0.34380851758660474; MAE for tp: 0.0949037225241157;\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "RMSE for t2m: 1.5164741223689804; MAE for t2m: 1.1410770425013979;\n",
      "RMSE for sp: 1.5326899484701422; MAE for sp: 1.1536430676778158;\n",
      "RMSE for tcc: 0.28123586885427265; MAE for tcc: 0.1835874561340371;\n",
      "RMSE for u10: 1.2802298585493026; MAE for u10: 0.9356656250185026;\n",
      "RMSE for v10: 1.2619893843881267; MAE for v10: 0.9058923062885946;\n",
      "RMSE for tp: 0.3187610553745829; MAE for tp: 0.08511933808011106;\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "RMSE for t2m: 1.7294853535274426; MAE for t2m: 1.30901707818784;\n",
      "RMSE for sp: 1.5907528094633576; MAE for sp: 1.196322198260272;\n",
      "RMSE for tcc: 0.29032770242770095; MAE for tcc: 0.19711941433557498;\n",
      "RMSE for u10: 1.3032766215077916; MAE for u10: 0.9750339971979464;\n",
      "RMSE for v10: 1.2474905010005901; MAE for v10: 0.9217376812211413;\n",
      "RMSE for tp: 0.32797445680842785; MAE for tp: 0.08575920657032136;\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "RMSE for t2m: 1.7057663167861843; MAE for t2m: 1.2690263413160032;\n",
      "RMSE for sp: 1.5144535910249604; MAE for sp: 1.1489851919668692;\n",
      "RMSE for tcc: 0.3028368694187912; MAE for tcc: 0.203580945692858;\n",
      "RMSE for u10: 1.2512973878956706; MAE for u10: 0.9442654412707133;\n",
      "RMSE for v10: 1.2255572311788718; MAE for v10: 0.9080879077395707;\n",
      "RMSE for tp: 0.2129937642899521; MAE for tp: 0.06639967004275343;\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "RMSE for t2m: 1.6773411197057329; MAE for t2m: 1.285470046363463;\n",
      "RMSE for sp: 1.585170212694086; MAE for sp: 1.210151336895978;\n",
      "RMSE for tcc: 0.2816062462896084; MAE for tcc: 0.182423042558491;\n",
      "RMSE for u10: 1.2603164606160018; MAE for u10: 0.95055892692694;\n",
      "RMSE for v10: 1.1762214524232537; MAE for v10: 0.8621381962836695;\n",
      "RMSE for tp: 0.28001388914371483; MAE for tp: 0.07763315725682933;\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "continue\n",
      "RMSE for t2m: 1.623715397116722; MAE for t2m: 1.2311357948588284;\n",
      "RMSE for sp: 1.538641351236577; MAE for sp: 1.140433826952425;\n",
      "RMSE for tcc: 0.28755188680582694; MAE for tcc: 0.19238783560502892;\n",
      "RMSE for u10: 1.2797309773570098; MAE for u10: 0.9422530556477111;\n",
      "RMSE for v10: 1.2233682980102494; MAE for v10: 0.8856470706710748;\n",
      "RMSE for tp: 0.28780050440581345; MAE for tp: 0.07824234574050828;\n"
     ]
    }
   ],
   "source": [
    "hpo.monthly_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo.write_plots_to_json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "engeneeringEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
